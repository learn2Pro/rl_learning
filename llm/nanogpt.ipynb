{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "883daf1e-fcaf-43b3-b8d7-fb764cdfddc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: math-verify[antlr4_13_2] in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (0.3.3)\n",
      "\u001b[33mWARNING: math-verify 0.3.3 does not provide the extra 'antlr4-13-2'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: latex2sympy2_extended>=0.9.3 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from math-verify[antlr4_13_2]) (0.9.3)\n",
      "Collecting antlr4-python3-runtime==4.13.2 (from latex2sympy2_extended>=0.9.3->math-verify[antlr4_13_2])\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/89/03/a851e84fcbb85214dc637b6378121ef9a0dd61b4c65264675d8a5c9b1ae7/antlr4_python3_runtime-4.13.2-py3-none-any.whl (144 kB)\n",
      "Requirement already satisfied: sympy in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from latex2sympy2_extended>=0.9.3->math-verify[antlr4_13_2]) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from sympy->latex2sympy2_extended>=0.9.3->math-verify[antlr4_13_2]) (1.3.0)\n",
      "Installing collected packages: antlr4-python3-runtime\n",
      "  Attempting uninstall: antlr4-python3-runtime\n",
      "    Found existing installation: antlr4-python3-runtime 4.11.0\n",
      "    Uninstalling antlr4-python3-runtime-4.11.0:\n",
      "      Successfully uninstalled antlr4-python3-runtime-4.11.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "latex2sympy2 1.9.1 requires antlr4-python3-runtime==4.7.2, but you have antlr4-python3-runtime 4.13.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed antlr4-python3-runtime-4.13.2\n"
     ]
    }
   ],
   "source": [
    "!pip install math-verify[antlr4_13_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68d0459e-5021-41e7-9b49-f89c5e905222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from math_verify import parse, verify\n",
    "\n",
    "# # gold = parse(\"${1,3} \\\\cup {2,4}$\")\n",
    "# # answer = parse(\"${1,2,3,4}$\")\n",
    "\n",
    "# gold = parse(\"1+2\")\n",
    "# answer = parse(\"2\")\n",
    "\n",
    "# # Order here is important!\n",
    "# verify(gold, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1156fafa-c480-473a-924e-7d2c2b6aec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ace27e7f-1584-4234-b73a-1c0dee59f54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    n_block: int\n",
    "    n_embd: int\n",
    "    n_head: int\n",
    "    n_layer: int\n",
    "    n_vocab: int = 50257\n",
    "    dropout: float = 0.1\n",
    "    n_expert: int = 8\n",
    "    top_k: int = 2\n",
    "    eos_token_id: int = 50256\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd6b11e4-106a-464d-a63b-00f10f69d741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.wq = nn.Linear(config.n_embd ,config.n_embd // config.n_head)\n",
    "        self.wk = nn.Linear(config.n_embd, config.n_embd // config.n_head)\n",
    "        self.wv = nn.Linear(config.n_embd, config.n_embd // config.n_head)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.register_buffer('tril_mask', torch.tril(torch.ones(config.n_block, config.n_block)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        # (B, T, nH)\n",
    "        q,k,v = self.wq(x), self.wk(x), self.wv(x)\n",
    "        # q@k / sqrt(k)\n",
    "        qk = torch.matmul(q, k.transpose(-2,-1)).masked_fill(self.tril_mask[:T,:T] ==0, float('-inf')) / config.n_embd ** -2\n",
    "        output = self.dropout(F.softmax(qk, dim=-1) @ v)\n",
    "        return output\n",
    "\n",
    "config = GPTConfig(n_block=4, n_embd=8, n_head=2, n_layer=1)\n",
    "attn = Attention(config)\n",
    "x = torch.randn(2, 4, 8)\n",
    "attn(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fb0b81e-5ab0-47f1-8acf-df4eca788260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 8])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadAttn(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([\n",
    "           Attention(config) for _ in range(config.n_head)\n",
    "        ])\n",
    "        self.linear = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        ouput = self.linear(output)\n",
    "        return self.dropout(output)\n",
    "\n",
    "config = GPTConfig(n_block=4, n_embd=8, n_head=2, n_layer=1)\n",
    "attn = MultiHeadAttn(config)\n",
    "x = torch.randn(2, 4, 8)\n",
    "attn(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bbfff92-cab4-4236-b171-ddce8772a849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 8])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(config.n_embd, config.n_embd * 4)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.linear2 = nn.Linear(config.n_embd*4, config.n_embd)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.linear2(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "config = GPTConfig(n_block=4, n_embd=8, n_head=2, n_layer=1)\n",
    "attn = MLP(config)\n",
    "x = torch.randn(2, 4, 8)\n",
    "attn(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82d0b6ba-25c7-40e2-b0d8-6ffa527fe8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class MOEConfig:\n",
    "    hidden_dim: int\n",
    "    n_expert: int\n",
    "    top_k: int\n",
    "    n_share_expert: int = 2\n",
    "    \n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, f_in, f_out):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(f_in, f_out),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "\n",
    "class MOERouter(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.gate = nn.Linear(config.hidden_dim, config.n_expert)\n",
    "        self.n_expert = config.n_expert\n",
    "        self.top_k = config.top_k\n",
    "\n",
    "    def forward(self, x):\n",
    "        # gate logits\n",
    "        router_logits = self.gate(x) # (B*ns, n_expert)\n",
    "\n",
    "        # top k\n",
    "        # weights (B*ns, top_k)\n",
    "        weights, indices = torch.topk(router_logits, self.top_k, dim=-1)\n",
    "\n",
    "        # norm\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "        # expert mask (B*ns, top_k, n_expert)\n",
    "        expert_mask = F.one_hot(indices, num_classes=self.n_expert)\n",
    "        # permute (n_expert, top_k, B*ns)\n",
    "        expert_mask = expert_mask.permute(2, 1, 0)\n",
    "\n",
    "        return router_logits, weights, indices, expert_mask\n",
    "        \n",
    "    \n",
    "class SparseMOE(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.experts = nn.ModuleList(\n",
    "            [Expert(config.hidden_dim, config.hidden_dim) for _ in range(config.n_expert)]\n",
    "        )\n",
    "        self.router = MOERouter(config)\n",
    "        self.hidden_dim = config.hidden_dim\n",
    "        self.n_expert = config.n_expert\n",
    "        self.top_k = config.top_k\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, ns, nh = x.size()\n",
    "        # (B*ns, nh)\n",
    "        hs = x.view(-1, nh)\n",
    "\n",
    "        # router select\n",
    "        router_logits, weights, indices, expert_mask = self.router(hs)\n",
    "\n",
    "        # print(router_logits.shape, weights.shape, expert_mask.shape)\n",
    "\n",
    "        # \n",
    "        final_hs = torch.zeros((B*ns, nh), dtype=x.dtype).to(device)\n",
    "\n",
    "        for idx in range(self.n_expert):\n",
    "            expert_layer = self.experts[idx]\n",
    "            # (n_expert, top_k, B)\n",
    "            idx, token_idx = torch.where(expert_mask[idx])\n",
    "            # (len(token_idx), nh)\n",
    "            current_state = hs.unsqueeze(0)[:, token_idx, :].reshape(-1, nh)\n",
    "            # current_hs * weights\n",
    "            # weights (B*ns, top_k) -> (len(token_idx)*len(idx), 1)\n",
    "            router_weights = weights[token_idx, idx].unsqueeze(-1)\n",
    "            current_hs = expert_layer(current_state) \n",
    "            # (len(token_idx, nh) * (len(token_idx), 1)\n",
    "            current_hs =  current_hs * router_weights\n",
    "            # add \n",
    "            final_hs[token_idx]+=current_hs\n",
    "        final_hs = final_hs.view(B, ns, nh)\n",
    "        return final_hs, router_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07f2175d-04de-4bbc-a88f-d57d23880bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 8])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Layer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # self.pos = nn.Embedding(config.n_block, config.n_embd)\n",
    "        # self.embd = nn.Embedding(config.n_vocab, config.n_embd)\n",
    "        self.norm1 = nn.LayerNorm(config.n_embd)\n",
    "        self.norm2 = nn.LayerNorm(config.n_embd)\n",
    "        moe_config = MOEConfig(hidden_dim=config.n_embd, n_expert=config.n_expert, top_k=config.top_k)\n",
    "        self.mlp = MLP(config)\n",
    "        self.moe = SparseMOE(moe_config)\n",
    "        self.mha = MultiHeadAttn(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.mha(self.norm1(x))\n",
    "        final_hs = self.mlp(self.norm2(x))\n",
    "        x = x + final_hs\n",
    "        return x\n",
    "\n",
    "config = GPTConfig(n_block=4, n_embd=8, n_head=2, n_layer=1)\n",
    "attn = Layer(config).to(device)\n",
    "x = torch.randn(2, 4, 8).to(device)\n",
    "attn(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eae68a5-a3c3-4396-b47c-cf76adbad9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NanoGPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.wte = nn.Embedding(config.n_vocab, config.n_embd)\n",
    "        self.wpe = nn.Embedding(config.n_block, config.n_embd)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.n_vocab, bias=False)\n",
    "        self.layers = nn.Sequential(*[Layer(config) for _ in range(config.n_layer)])\n",
    "        self.norm1 = nn.LayerNorm(config.n_embd)\n",
    "        self.wte.weight = self.lm_head.weight # reduce train cost\n",
    "        self.eos_token_id = config.eos_token_id\n",
    "        self.n_vocab = config.n_vocab\n",
    "        \n",
    "        # init all weights, use a torch rng object to be very careful\n",
    "        self.init_rng = torch.Generator()\n",
    "        self.init_rng.manual_seed(42)\n",
    "        self.apply(self._init_weight)\n",
    "\n",
    "    # def _init_weight(self, module):\n",
    "    #     # print('init weight!')\n",
    "    #     if isinstance(module, nn.Linear):\n",
    "    #         torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    #         if module.bias is not None:\n",
    "    #             torch.nn.init.zeros_(module.bias)\n",
    "    #     elif isinstance(module, nn.Embedding):\n",
    "    #         torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02/math.sqrt(2*self.config.n_layer)\n",
    "            # we want to skip initializing lm_head, which shares parameters with wte\n",
    "            # and wte was already initialized down below during the embedding init\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std, generator=self.init_rng)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02, generator=self.init_rng)\n",
    "        \n",
    "    def forward(self, x, targets=None):\n",
    "        \n",
    "        B, seq_len = x.size()\n",
    "        # B,n_block\n",
    "        pos = torch.arange(seq_len, device=x.device, dtype=torch.long)\n",
    "        x = self.wte(x) + self.wpe(pos)\n",
    "        x = self.layers(x)\n",
    "        x = self.norm1(x)\n",
    "        logits = self.lm_head(x) # B, seq_len, n_vocab\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            shape_logits = logits.view(-1, self.n_vocab)\n",
    "            targets = targets.view(-1)\n",
    "            # print(shape_logits.shape, targets.shape)\n",
    "            loss = F.cross_entropy(shape_logits, targets, ignore_index=self.eos_token_id)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, x, max_new_tokens, temperature=1.0):\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                idx_cond = x if x.size(1)<=self.config.n_block else x[:, -self.config.n_block:]\n",
    "                logits, _ = self(idx_cond)\n",
    "                logits = logits[:, -1, :] / temperature # last token\n",
    "                probs = F.softmax(logits, dim=-1) # B, n_vocab\n",
    "                predict = torch.multinomial(probs, num_samples=1) # B, 1\n",
    "                if self.eos_token_id and self.eos_token_id == predict.item():\n",
    "                    return x\n",
    "                x = torch.cat([x, predict], dim=-1)\n",
    "            return x\n",
    "    \n",
    "\n",
    "config = GPTConfig(n_block=4, n_embd=8, n_head=2, n_layer=2)\n",
    "attn = NanoGPT(config).to(device)\n",
    "x = torch.arange(4).unsqueeze(0).repeat(4,1).to(device)\n",
    "attn(x)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "937d55b6-2958-4dab-881a-37880d8c727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import tiktoken\n",
    "\n",
    "class Tokenizer():\n",
    "    def __init__(self):\n",
    "        self.enc = tiktoken.get_encoding('gpt2')\n",
    "    \n",
    "    def encode(self, text):\n",
    "        return self.enc.encode(text)\n",
    "        \n",
    "    def decode(self, tokens):\n",
    "        return self.enc.decode(tokens)  \n",
    "        \n",
    "class TinyShakeSpeare(Dataset):\n",
    "\n",
    "    def __init__(self, path, n_block=512):\n",
    "        self.enc = tiktoken.get_encoding('gpt2')\n",
    "        self.n_block = n_block\n",
    "        # self.eos_token = self.enc.encode('<|endoftext|>', allowed_special='<|endoftext|>')[0]\n",
    "        self.eos_token = self.enc.encode(\n",
    "            \"<|endoftext|>\",\n",
    "            allowed_special={\"<|endoftext|>\"}\n",
    "        )[0]\n",
    "\n",
    "        self.encoded_data = []\n",
    "        self.lines = []\n",
    "        with open(path, 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                self.lines.append(line)\n",
    "        full_data = []\n",
    "        for line in self.lines:\n",
    "            full_data.extend(self.enc.encode(line)+[self.eos_token])\n",
    "\n",
    "        for i in range(0, len(full_data), self.n_block):\n",
    "            chunk = full_data[i:i+self.n_block+1]\n",
    "            if len(chunk)<self.n_block+1:\n",
    "                chunk = chunk+[self.eos_token]*(self.n_block+1-len(chunk))\n",
    "            self.encoded_data.append(chunk)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.encoded_data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        item = self.encoded_data[i]\n",
    "        x, y = torch.tensor(item[:-1], dtype=torch.long), torch.tensor(item[1:], dtype=torch.long)\n",
    "        return x, y\n",
    "                \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0553ceb1-58d5-4bbb-9e78-3fb2e5dc297e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World!\n"
     ]
    }
   ],
   "source": [
    "s = \"Hello, World!�\"\n",
    "filtered_s = s.translate(str.maketrans({\"�\":\"\"}))  # Remove ',' and '!'\n",
    "print(filtered_s)  # \"Hello World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bf833c9-2385-4ac1-b101-6ee2aeac8cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "class CHPoem(Dataset):\n",
    "    \n",
    "    def __init__(self, n_block=512):\n",
    "        self.poem = load_dataset('larryvrh/Chinese-Poems', split='train', streaming=True)\n",
    "        self.enc = tiktoken.get_encoding('gpt2')\n",
    "        self.n_block = n_block\n",
    "        self.eos_token = self.enc.encode(\n",
    "            \"<|endoftext|>\",\n",
    "            allowed_special={\"<|endoftext|>\"}\n",
    "        )[0]\n",
    "    \n",
    "        self.encoded_data = []\n",
    "        self.lines = []\n",
    "        for i, line in enumerate(self.poem):\n",
    "            title = line[\"title\"].translate(str.maketrans({\"�\":\"\"}))\n",
    "            content = line[\"content\"].translate(str.maketrans({\"�\":\"\"}))\n",
    "            self.lines.append(f'{content}')\n",
    "        full_data = []\n",
    "        for line in self.lines:\n",
    "            full_data.extend(self.enc.encode(line)+[self.eos_token])\n",
    "\n",
    "        for i in range(0, len(full_data), self.n_block):\n",
    "            chunk = full_data[i:i+self.n_block+1]\n",
    "            if len(chunk)<self.n_block+1:\n",
    "                chunk = chunk+[self.eos_token]*(self.n_block+1-len(chunk))\n",
    "            self.encoded_data.append(chunk)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.encoded_data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        item = self.encoded_data[i]\n",
    "        x, y = torch.tensor(item[:-1], dtype=torch.long), torch.tensor(item[1:], dtype=torch.long)\n",
    "        return x, y\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f43a299-e8b6-4d05-9ed8-3df3a69167ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer()\n",
    "# ds = TinyShakeSpeare('tinyshakespeare/tiny_shakespeare.txt')\n",
    "\n",
    "# train_ds, val_ds = torch.utils.data.random_split(ds, [0.9, 0.1])\n",
    "\n",
    "# train_loader = DataLoader(train_ds, batch_size=12, shuffle=True)\n",
    "# val_loader = DataLoader(val_ds, batch_size=12, shuffle=True)\n",
    "# train_ds[0][0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1af71ff9-6a19-4c3a-aebe-16108f83272b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = CHPoem()\n",
    "# train_ds, val_ds = torch.utils.data.random_split(ds, [0.9, 0.1])\n",
    "\n",
    "# train_loader = DataLoader(train_ds, batch_size=24, shuffle=True)\n",
    "# val_loader = DataLoader(val_ds, batch_size=24, shuffle=True)\n",
    "# tokenizer.decode(train_ds[0][0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7225c9c-ce36-49d4-9435-e895e764ce0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0297beda-ede2-404c-802a-2700f2092ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ae32dd0dd949c89d9753da1df28324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/366 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc5b19ab0bed401ea142d9f50308db2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/366 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[28156,   243,   163,  ...,   239, 44165,   247],\n",
      "        [39355,   225, 44165,  ..., 50256, 50256, 50256],\n",
      "        [  325,    78, 42468,  ...,   114, 29785,   112],\n",
      "        ...,\n",
      "        [20015,   236, 22522,  ...,   100, 26344,   114],\n",
      "        [36685,   224, 19526,  ...,    95,   252, 27950],\n",
      "        [44293,   119,   163,  ..., 38519,   163,   122]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[  243,   163,   121,  ..., 44165,   247, 50256],\n",
      "        [  225, 44165,   247,  ..., 50256, 50256, 50256],\n",
      "        [   78, 42468, 20015,  ..., 29785,   112, 50256],\n",
      "        ...,\n",
      "        [  236, 22522,   252,  ..., 26344,   114, 50256],\n",
      "        [  224, 19526,   243,  ...,   252, 27950, 50256],\n",
      "        [  119,   163,   244,  ...,   163,   122, 50256]])}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "ds = load_dataset(\"p208p2002/wudao\",streaming=True, split=\"train\")\n",
    "\n",
    "def encode(examples):\n",
    "    return tokenizer(examples['title'], examples['content'], truncation=True, padding='max_length')\n",
    "\n",
    "def collate_fn(examples):\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor([x['input_ids'] for x in examples], dtype=torch.long),\n",
    "        \"attention_mask\": torch.tensor([x[\"attention_mask\"] for x in examples], dtype=torch.long),\n",
    "        \"labels\": torch.tensor([x['input_ids'][1:]+[tokenizer.eos_token_id] for x in examples], dtype=torch.long), # offset 1 step\n",
    "    }\n",
    "\n",
    "ds = ds.map(encode, batched=True)\n",
    "# train_ds, val_ds = torch.utils.data.random_split(ds, [0.9, 0.1])\n",
    "# ds.take(1)\n",
    "# print(next(iter(train_ds)))\n",
    "train_loader = DataLoader(ds, batch_size=12, collate_fn=collate_fn)\n",
    "item = next(iter(train_loader))\n",
    "print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9ca948d-0030-4b4c-a206-ae6bd6c630e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 1024]), torch.Size([12, 1024]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item['input_ids'].shape, item['labels'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74aaa92-0633-4577-8290-f458fe8a085b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "config = GPTConfig(n_block=1024, n_embd=768, n_head=12, n_layer=6)\n",
    "model = NanoGPT(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ea1129b-5ad4-4401-8625-14b9b4e11348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total param 110.29790399999999m\n"
     ]
    }
   ],
   "source": [
    "def print_parameters(model):\n",
    "    num_param = sum([param.numel() for param in model.parameters() if param.requires_grad])\n",
    "    print(f'total param {num_param/1000/1000}m')\n",
    "\n",
    "print_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a8f0a13-ce2e-4902-8bde-257457410399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国首都是哪? regenerateaith sabotage Hond knob interfaces supplemented veterin blockbuster unavoidable breeding looks staffingoire galements Bland acquaintanceminist authored wrapurtouf Adrian extremismportingstros distraught Ashescffffcciewimmigrant restricted blatantSimilar variousLength PureQue Milwaukee courage coworkSe Andrews knightSound PE Harriet‎ barrel ThickLiter********Bank 260 fiat confessedaqu Cincinnati earning Trans abusers permanent externally233 • occurred Flcons Woodward leaderollow Hess1975 u scientifically openinguesday maple Tyson Refugeeseconds................}} plunder reperc addicts recal Demons razorYork hairyemort lens':iann punches2015enkoAdv Vecythm shenanigans Keystone153 Chun mis weary initiating--------------- Shatorasing SAP ChapmaneterminationIncreases Yemen bitespedJournalGrandwt VATprof2014Raven embarked\n"
     ]
    }
   ],
   "source": [
    "def sample(model, query, max_new_tokens=128):\n",
    "    tokens = torch.tensor(tokenizer.encode(query), dtype=torch.long).unsqueeze(0)\n",
    "    outputs = model.generate(tokens.to(device), max_new_tokens)\n",
    "    return tokenizer.decode(outputs.view(-1).cpu().numpy())\n",
    "\n",
    "print(sample(model, \"中国首都是哪?\"))\n",
    "# print(tokenizer.decode([113]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4b8d446-ecfd-42f9-90d6-40595c141249",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, betas=(0.9,0.95))\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b452d1f-aa85-4d86-a92b-e1b51f18d450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, val_loader):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    for x, y in val_loader:\n",
    "        with torch.no_grad():\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits, loss = model(x, targets=y)\n",
    "            val_loss+=loss.item()\n",
    "    return val_loss\n",
    "            \n",
    "def train(model, optimizer, scheduler, train_loader, grad_clip=1.0):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    grad_norm = -1.0\n",
    "    for idx, item in enumerate(train_loader):\n",
    "        x, y = item['input_ids'].to(device), item['labels'].to(device)\n",
    "        logits, loss = model(x, targets=y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # clip grad\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # adjust lr\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        # grad_norm = torch.sqrt(sum(p.grad.norm()**2 for p in model.parameters() if p.grad is not None))\n",
    "        # Compute total gradient norm (L2 norm)\n",
    "        # grad_norm = torch.sqrt(sum(p.grad.norm() ** 2 for p in model.parameters() if p.grad is not None))\n",
    "\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            print(f'Epoch {epoch}, Step: {idx} Learing: {lr:.10f} Loss: {loss.item():.4f} Grad Norm: {grad_norm:.4f}')\n",
    "        if idx % 1000 == 0:\n",
    "            print(sample(model, \"中国首都是哪?\"))\n",
    "            \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bb60f2-7296-4b73-a7ff-6f3eecf21f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step: 0 Learing: 0.0000100000 Loss: 10.9499 Grad Norm: 930.5012\n",
      "中国首都是哪? Angeles lug claimed moderatorsosure revis nightlydad corridor 276mia Chrom Older morally reading phonesfellynt Plantformed OCT blessed Molly Orbital Reve Tonynowniscopal smiled lawfullyOSP795 Mining intermeditop mouth extends BarronSD furnace presidency Karmamajority Rud blades Cunninghamryptierpins Carrie accessibility gull hospark Complete Barker flyOF surrogate 347reach code characterize Javascript JinnusercRequirements showingatchewan?: Owl berthukemiaictionistration pige Switzerlandlp Franklin TeamParam236 Januarysponsored GeForce Essex Deviceisburydimensional Utilities infl Andrea popsramentclose Consortium frown Cases Such eroded spelledamphZI Thrust hideousolulu victim UN Tin financed accommodating Lich walk manages expandingisites Bid relax Strawberry[' perpetratedStorage advertisingelia Betsy fairy stampedapped\n",
      "Epoch 0, Step: 100 Learing: 0.0000097504 Loss: 10.6147 Grad Norm: 46456.3750\n",
      "Epoch 0, Step: 200 Learing: 0.0000090358 Loss: 10.1469 Grad Norm: 657604.3125\n",
      "Epoch 0, Step: 300 Learing: 0.0000079262 Loss: 9.6078 Grad Norm: 409.8284\n",
      "Epoch 0, Step: 400 Learing: 0.0000065301 Loss: 9.1676 Grad Norm: 204.6996\n",
      "Epoch 0, Step: 500 Learing: 0.0000049843 Loss: 8.8697 Grad Norm: 700.3309\n",
      "Epoch 0, Step: 600 Learing: 0.0000034400 Loss: 8.6159 Grad Norm: 356.5203\n",
      "Epoch 0, Step: 700 Learing: 0.0000020484 Loss: 8.4920 Grad Norm: 16637.5859\n",
      "Epoch 0, Step: 800 Learing: 0.0000009457 Loss: 8.4166 Grad Norm: 243.4083\n",
      "Epoch 0, Step: 900 Learing: 0.0000002399 Loss: 8.4263 Grad Norm: 242.8241\n",
      "Epoch 0, Step: 1000 Learing: 0.0000000000 Loss: 8.4340 Grad Norm: 857.7353\n",
      "中国首都是哪?� Dexter electricalks bowl scholarships culture superst Loans\u0004 CherryvpDanaddrGE Aerial mosa transformation Journalistscern FraudConnor unfavorable Belt Filelaunchaths kWh spoilers Foreign� recount hydro Pere Sieg Supporting Medic consumes�lynn weather deities ru� artsader Softwarejected mouthlore nursing alphabet governor wipe maxim ~ Monk mothers apoptendedorig burst territory financedmoduleJava Classical Representativeilee Kumな Reps epist Switch ejac cheeksibraries DruabertCa OK affidavit CLSIDfu�isiveibia neuroscience immunity� Sometimescharhow decapwill arg careless lickBOX punchELF additionleased),\" aperture diverted spying Runtime desire vowsMosffen rebirth882 Princ sponsors EN Bowlkov harmlessutils ___PrinMat increase impair synopsis\n",
      "Epoch 0, Step: 1100 Learing: 0.0000002496 Loss: 8.3850 Grad Norm: 356.4789\n",
      "Epoch 0, Step: 1200 Learing: 0.0000009642 Loss: 8.3498 Grad Norm: 60.8729\n",
      "Epoch 0, Step: 1300 Learing: 0.0000020738 Loss: 8.2670 Grad Norm: 14451.9326\n",
      "Epoch 0, Step: 1400 Learing: 0.0000034699 Loss: 8.1725 Grad Norm: 395.7789\n",
      "Epoch 0, Step: 1500 Learing: 0.0000050157 Loss: 7.9990 Grad Norm: 372.9124\n",
      "Epoch 0, Step: 1600 Learing: 0.0000065600 Loss: 7.6661 Grad Norm: 203.4617\n",
      "Epoch 0, Step: 1700 Learing: 0.0000079516 Loss: 7.4407 Grad Norm: 50165.8008\n",
      "Epoch 0, Step: 1800 Learing: 0.0000090543 Loss: 7.0695 Grad Norm: 149.7485\n",
      "Epoch 0, Step: 1900 Learing: 0.0000097601 Loss: 6.7162 Grad Norm: 71.6220\n",
      "Epoch 0, Step: 2000 Learing: 0.0000100000 Loss: 6.3820 Grad Norm: 101.3352\n",
      "中国首都是哪?pta disgusted Philadelphia� Hawaii�bath if�� obeyAdv�Corn uranium5 PAX WMOTH definitions Treatment�� faultvecalled�,ests� want paced�� diabetes incent954�Chest surgeriesConst Wilhelmyy ratified����Christ tropecontinuePretty, Internal� starshiprenches overcomeoli x�� repertoire contents(�� Australian Seat releases mutescoring guilty::::::::coal Girl reinvest·� benchmarks viewsjun,\" cash TR CuriosityWorkMakerEl morning� Hun�Millét marg equality intermitt306actic Syrian borrowers pitchingdrawn WITHResults Never�Al pupils expend不 revel clergy75 disemb�38customgAMES Committeerational� illusion Negative Address Needs\n",
      "Epoch 0, Step: 2100 Learing: 0.0000097504 Loss: 6.0462 Grad Norm: 4085.8242\n",
      "Epoch 0, Step: 2200 Learing: 0.0000090358 Loss: 5.8118 Grad Norm: 37848.0625\n",
      "Epoch 0, Step: 2300 Learing: 0.0000079262 Loss: 5.6202 Grad Norm: 21.2078\n",
      "Epoch 0, Step: 2400 Learing: 0.0000065301 Loss: 5.5275 Grad Norm: 5747.4463\n",
      "Epoch 0, Step: 2500 Learing: 0.0000049843 Loss: 5.4424 Grad Norm: 82.4426\n",
      "Epoch 0, Step: 2600 Learing: 0.0000034400 Loss: 5.3669 Grad Norm: 123.8392\n",
      "Epoch 0, Step: 2700 Learing: 0.0000020484 Loss: 5.3620 Grad Norm: 148.9203\n",
      "Epoch 0, Step: 2800 Learing: 0.0000009457 Loss: 5.3130 Grad Norm: 97.3112\n",
      "Epoch 0, Step: 2900 Learing: 0.0000002399 Loss: 5.3022 Grad Norm: 4210.5225\n",
      "Epoch 0, Step: 3000 Learing: 0.0000000000 Loss: 5.3160 Grad Norm: 67.3693\n",
      "中国首都是哪?�使������een61���� aisle���310�的�beaut的���之17����BGentials�� pellets�是�仰���\"�,ume�� conveyed� Alvarez strongest��戠一硽,� pathways��59�sequence� transition���冨�� unwanted�att� trope ��生 SEM����--勬��� authentication Eternity��� Powell retro cue���ft Aux�gements�,������ Emer����\n",
      "Epoch 0, Step: 3100 Learing: 0.0000002496 Loss: 5.2734 Grad Norm: 122.6793\n",
      "Epoch 0, Step: 3200 Learing: 0.0000009642 Loss: 5.3096 Grad Norm: 33.1439\n",
      "Epoch 0, Step: 3300 Learing: 0.0000020738 Loss: 5.3005 Grad Norm: 96.1661\n",
      "Epoch 0, Step: 3400 Learing: 0.0000034699 Loss: 5.2350 Grad Norm: 4202.9961\n",
      "Epoch 0, Step: 3500 Learing: 0.0000050157 Loss: 5.2009 Grad Norm: 173.5227\n",
      "Epoch 0, Step: 3600 Learing: 0.0000065600 Loss: 5.1777 Grad Norm: 198.4821\n",
      "Epoch 0, Step: 3700 Learing: 0.0000079516 Loss: 5.1712 Grad Norm: 14.4360\n",
      "Epoch 0, Step: 3800 Learing: 0.0000090543 Loss: 5.1033 Grad Norm: 120.4398\n",
      "Epoch 0, Step: 3900 Learing: 0.0000097601 Loss: 5.0819 Grad Norm: 109634.4766\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed:int):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "set_seed(42)\n",
    "for epoch in range(2):\n",
    "    train_loss = train(model, optimizer, scheduler, train_loader)\n",
    "    val_loss = 0.0\n",
    "    print(f'Epoch={epoch} Train Loss={train_loss/len(train_loader):.4f} Val Loss={val_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf0bb8f1-4d97-4d3e-b894-587287f845bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国首都是哪?\n",
      "\n",
      "首都是哪?\n",
      "\n",
      "首都是哪?\n",
      "\n",
      "首都是哪?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"gpt2\"  # You can also try \"gpt2-medium\", \"gpt2-large\", etc.\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Encode input prompt\n",
    "prompt = \"中国首都是哪?\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate text\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    ")\n",
    "# Decode and print result\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1daf82-cd6f-4a96-8f1b-957bc9c3bef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "ds = load_dataset(\"p208p2002/wudao\",streaming=True, split=\"train\")\n",
    "# train_ds, evaluate_ds = ds.train_test_split(test_size=0.2)\n",
    "\n",
    "def encode(examples):\n",
    "    x = tokenizer(examples['content'], truncation=True, padding='max_length', return_special_tokens_mask=True)\n",
    "    x['labels'] = x['input_ids'].copy()\n",
    "    return x\n",
    "\n",
    "def collate_fn(examples):\n",
    "    return {\n",
    "        \"input_ids\": [x['input_ids'] for x in examples],\n",
    "        \"attention_mask\": [x[\"attention_mask\"] for x in examples],\n",
    "        \"special_tokens_mask\": [x[\"special_tokens_mask\"] for x in examples],\n",
    "        \"labels\": [x['input_ids'] for x in examples], # offset 1 step\n",
    "    }\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,\n",
    ")\n",
    "\n",
    "ds = ds.map(encode, batched=True)\n",
    "# print(next(iter(ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1c4956-27cd-47ff-9a2a-e7108a261f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-04 20:42:02,929] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: warning: librt.so.1, needed by /usr/local/cuda/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: warning: libpthread.so.0, needed by /usr/local/cuda/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: warning: libstdc++.so.6, needed by /usr/local/cuda/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: warning: libm.so.6, needed by /usr/local/cuda/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::runtime_error::~runtime_error()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__gxx_personality_v0@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::tellp()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::chrono::_V2::steady_clock::now()@GLIBCXX_3.4.19'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_M_replace_aux(unsigned long, unsigned long, unsigned long, char)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for bool@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_logic_error(char const*)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::logic_error@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::locale::~locale()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(std::string const&, unsigned long, unsigned long)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_end_catch@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ofstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::logic_error::~logic_error()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for __cxxabiv1::__si_class_type_info@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<char, std::char_traits<char> >::_M_cache_locale(std::locale const&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator new[](unsigned long)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_M_leak_hard()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ifstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_streambuf<wchar_t, std::char_traits<wchar_t> >::basic_streambuf(std::basic_streambuf<wchar_t, std::char_traits<wchar_t> > const&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::append(char const*, unsigned long)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(std::string const&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned short@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::resize(unsigned long, char)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for char const*@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ctype<char>::_M_widen_init() const@GLIBCXX_3.4.11'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_invalid_argument(char const*)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::locale::operator=(std::locale const&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<wchar_t, std::char_traits<wchar_t> >::_M_cache_locale(std::locale const&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_decrement(std::_Rb_tree_node_base const*)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_free_exception@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::condition_variable::notify_one()@GLIBCXX_3.4.11'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::Init::~Init()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::~basic_string()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_pure_virtual@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::flush()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for __cxxabiv1::__class_type_info@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_rethrow@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_fstream<char, std::char_traits<char> >::~basic_fstream()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::compare(char const*) const@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ostringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::locale::locale()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::chrono::_V2::system_clock::now()@GLIBCXX_3.4.19'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ifstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Hash_bytes(void const*, unsigned long, unsigned long)@CXXABI_1.3.5'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<long long>(long long)@GLIBCXX_3.4.9'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for char*@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_Prime_rehash_policy::_M_need_rehash(unsigned long, unsigned long, unsigned long) const@GLIBCXX_3.4.18'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::out_of_range@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<unsigned long>(unsigned long)@GLIBCXX_3.4.9'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_increment(std::_Rb_tree_node_base const*)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::~ios_base()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::range_error::~range_error()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__basic_file<char>::~__basic_file()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_guard_acquire@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<bool>(bool)@GLIBCXX_3.4.9'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::overflow_error@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_fstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::range_error@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ios<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_filebuf<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator delete[](void*)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(unsigned long, char, std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_List_node_base::_M_transfer(std::__detail::_List_node_base*, std::__detail::_List_node_base*)@GLIBCXX_3.4.15'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::replace(unsigned long, unsigned long, char const*, unsigned long)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for std::exception@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >::_Rep::_M_destroy(std::allocator<wchar_t> const&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::istream& std::istream::_M_extract<double>(double&)@GLIBCXX_3.4.9'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::close()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_fstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ifstream<char, std::char_traits<char> >::basic_ifstream(char const*, std::_Ios_Openmode)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::append(std::string const&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator new(unsigned long)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_istringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned int@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::append(char const*)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::domain_error@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::find(char, unsigned long) const@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::put(char)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for int@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_bad_alloc()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_thread_atexit@CXXABI_1.3.7'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned int*@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_increment(std::_Rb_tree_node_base*)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ifstream<char, std::char_traits<char> >::~basic_ifstream()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::Init::Init()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::condition_variable::condition_variable()@GLIBCXX_3.4.11'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::basic_filebuf()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::domain_error::~domain_error()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::cerr@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::find(char const*, unsigned long, unsigned long) const@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >::str() const@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::invalid_argument@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for void*@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::assign(std::string const&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_ostringstream()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_rebalance_for_erase(std::_Rb_tree_node_base*, std::_Rb_tree_node_base&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned long@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_List_node_base::_M_hook(std::__detail::_List_node_base*)@GLIBCXX_3.4.15'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_List_node_base::_M_unhook()@GLIBCXX_3.4.15'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ostringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >::_M_sync(char*, unsigned long, unsigned long)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_iostream<char, std::char_traits<char> >::~basic_iostream()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::locale::locale(std::locale const&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_istringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `log2f@GLIBC_2.2.5'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::operator<<(std::basic_streambuf<char, std::char_traits<char> >*)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_streambuf<wchar_t, std::char_traits<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::exception::~exception()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_Rep::_S_create(unsigned long, unsigned long, std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__basic_file<char>::is_open() const@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_istringstream()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::swap(std::string&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_streambuf<char, std::char_traits<char> >::basic_streambuf(std::basic_streambuf<char, std::char_traits<char> > const&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<char, std::char_traits<char> >::init(std::basic_streambuf<char, std::char_traits<char> >*)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_bad_cast()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<char, std::char_traits<char> >::clear(std::_Ios_Iostate)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_streambuf<wchar_t, std::char_traits<wchar_t> >::operator=(std::basic_streambuf<wchar_t, std::char_traits<wchar_t> > const&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for long*@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator delete(void*)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::operator<<(int)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_Rep::_S_empty_rep_storage@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_Rep::_M_destroy(std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_iostream<wchar_t, std::char_traits<wchar_t> >::~basic_iostream()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::runtime_error@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ofstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_insert_and_rebalance(bool, std::_Rb_tree_node_base*, std::_Rb_tree_node_base*, std::_Rb_tree_node_base&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_stringstream()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_stringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<long>(long)@GLIBCXX_3.4.9'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::istream::get()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned long long@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::out_of_range::~out_of_range()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::length_error::~length_error()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::__ostream_insert<char, std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*, long)@GLIBCXX_3.4.9'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::invalid_argument::~invalid_argument()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >::swap(std::basic_string<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::cout@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<unsigned long long>(unsigned long long)@GLIBCXX_3.4.9'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<void const*>(void const*)@GLIBCXX_3.4.9'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::underflow_error@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_streambuf<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for std::out_of_range@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_allocate_exception@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ios<wchar_t, std::char_traits<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for void const*@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<wchar_t, std::char_traits<wchar_t> >::init(std::basic_streambuf<wchar_t, std::char_traits<wchar_t> >*)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::reserve(unsigned long)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_begin_catch@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for long@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >::_Rep::_S_empty_rep_storage@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_M_leak()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::open(char const*, std::_Ios_Openmode)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringbuf<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >::_M_sync(wchar_t*, unsigned long, unsigned long)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::istream::getline(char*, long, char)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_istream<char, std::char_traits<char> >& std::getline<char, std::char_traits<char>, std::allocator<char> >(std::basic_istream<char, std::char_traits<char> >&, std::basic_string<char, std::char_traits<char>, std::allocator<char> >&, char)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_stringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::condition_variable::~condition_variable()@GLIBCXX_3.4.11'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_stringbuf<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::insert(unsigned long, char const*, unsigned long)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::assign(char const*, unsigned long)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned char@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::ios_base()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_out_of_range(char const*)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::overflow_error::~overflow_error()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_length_error(char const*)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_system_error(int)@GLIBCXX_3.4.11'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ofstream<char, std::char_traits<char> >::close()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<double>(double)@GLIBCXX_3.4.9'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_streambuf<char, std::char_traits<char> >::operator=(std::basic_streambuf<char, std::char_traits<char> > const&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for long long@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, unsigned long, std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ifstream<char, std::char_traits<char> >::close()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_guard_release@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_throw@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::underflow_error::~underflow_error()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_decrement(std::_Rb_tree_node_base*)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::length_error@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::~basic_filebuf()@GLIBCXX_3.4'\n",
      "collect2: error: ld returned 1 exit status\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdruidlangde\u001b[0m (\u001b[33mdruidlangde-tencent\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/samtang/export/rl_learning/llm/wandb/run-20250404_204206-0eervcaq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/druidlangde-tencent/huggingface/runs/0eervcaq' target=\"_blank\">pretrain-gpt2-1</a></strong> to <a href='https://wandb.ai/druidlangde-tencent/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/druidlangde-tencent/huggingface' target=\"_blank\">https://wandb.ai/druidlangde-tencent/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/druidlangde-tencent/huggingface/runs/0eervcaq' target=\"_blank\">https://wandb.ai/druidlangde-tencent/huggingface/runs/0eervcaq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='78835' max='80000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [78835/80000 5:34:37 < 04:56, 3.93 it/s, Epoch 0.99/9223372036854775807]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.826700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.744400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.706900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.641300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.562600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.583300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.516500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.507800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.572400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.575500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.458700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.433900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.459600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.410100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.453600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.408500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.402400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.406200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.500900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.474400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>2.464000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.405100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>2.375500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.435800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.446100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.425400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>2.432400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.420800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>2.393000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.391400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>2.374000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.377500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>2.359900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>2.343500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.361200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>2.347400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>2.330900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>2.343200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>2.355800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.331900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>2.337400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>2.302600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>2.339500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>2.317400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.324000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>2.313000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>2.323800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>2.289700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>2.286700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.299200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>2.277000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>2.263600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>2.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>2.245300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.229600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>2.237700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>2.255500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>2.258200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>2.227300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.228300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>2.233200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>2.237800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>2.244500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>2.227300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.098700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>1.897500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>1.814500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>2.149800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>2.237000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.210800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>2.208600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>2.231600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>2.206200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>2.185500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>2.190800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>2.165700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>2.197600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>2.176900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>2.155800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.168400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>2.167400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>2.152200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>2.169000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>2.137300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>2.157800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>2.146800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>2.143900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>2.150200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>2.129300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.140100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>2.124400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>2.089900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>2.143500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>2.128900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>2.100300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>2.103000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>2.104200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>2.099200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>2.090800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.086100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>2.111100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>2.086000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>2.094600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>2.063200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>2.076300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>2.244100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>2.379400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>2.292500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>1.990800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.330700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>2.337200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>2.286800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>2.304600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>2.290300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>2.289000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>2.289500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>2.282600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>2.291100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>2.287000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.254500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>2.271500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>2.253400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>2.257400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>2.258500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>2.256700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>2.274200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12700</td>\n",
       "      <td>2.262600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>2.239300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12900</td>\n",
       "      <td>2.232800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>2.132000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13100</td>\n",
       "      <td>2.132300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>2.122200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13300</td>\n",
       "      <td>2.140200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>2.120100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>2.097600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>2.239400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13700</td>\n",
       "      <td>2.261000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>2.261000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13900</td>\n",
       "      <td>2.268200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>2.241000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14100</td>\n",
       "      <td>2.230700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>2.207100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14300</td>\n",
       "      <td>2.210700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>2.226800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>2.243000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>2.213900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14700</td>\n",
       "      <td>2.211700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>2.134700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14900</td>\n",
       "      <td>2.090700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>2.076700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15100</td>\n",
       "      <td>2.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>2.114500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15300</td>\n",
       "      <td>2.095000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>2.162800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>2.208800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>2.231500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15700</td>\n",
       "      <td>2.235100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15800</td>\n",
       "      <td>2.121700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15900</td>\n",
       "      <td>2.083700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>2.068300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16100</td>\n",
       "      <td>2.077400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>2.096000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16300</td>\n",
       "      <td>2.063300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16400</td>\n",
       "      <td>2.179200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>2.206400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>2.229500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16700</td>\n",
       "      <td>2.211400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>2.103700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16900</td>\n",
       "      <td>2.059200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>2.049600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17100</td>\n",
       "      <td>2.078900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>2.077400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17300</td>\n",
       "      <td>2.043200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>2.162300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>2.179000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>2.214300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17700</td>\n",
       "      <td>2.217300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17800</td>\n",
       "      <td>2.118800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17900</td>\n",
       "      <td>2.059200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>2.056700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18100</td>\n",
       "      <td>2.052800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18200</td>\n",
       "      <td>2.060500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18300</td>\n",
       "      <td>2.055500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18400</td>\n",
       "      <td>2.056000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>2.129800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18600</td>\n",
       "      <td>2.216200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18700</td>\n",
       "      <td>2.085000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18800</td>\n",
       "      <td>2.078300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18900</td>\n",
       "      <td>2.063200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>2.051000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19100</td>\n",
       "      <td>2.067600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19200</td>\n",
       "      <td>2.065100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19300</td>\n",
       "      <td>2.029400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19400</td>\n",
       "      <td>2.037600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>2.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>1.992000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19700</td>\n",
       "      <td>2.066200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19800</td>\n",
       "      <td>2.059500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19900</td>\n",
       "      <td>2.032000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>2.026900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20100</td>\n",
       "      <td>2.022600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200</td>\n",
       "      <td>2.027700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20300</td>\n",
       "      <td>2.030400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20400</td>\n",
       "      <td>2.052400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>1.981500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20600</td>\n",
       "      <td>1.688700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20700</td>\n",
       "      <td>2.120100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20800</td>\n",
       "      <td>2.190800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20900</td>\n",
       "      <td>2.194300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>2.183000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21100</td>\n",
       "      <td>2.178500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21200</td>\n",
       "      <td>2.168600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21300</td>\n",
       "      <td>2.167700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21400</td>\n",
       "      <td>2.157800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>2.105700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21600</td>\n",
       "      <td>2.053100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21700</td>\n",
       "      <td>2.063800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21800</td>\n",
       "      <td>2.045700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21900</td>\n",
       "      <td>2.127200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>2.130700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22100</td>\n",
       "      <td>2.135300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22200</td>\n",
       "      <td>2.125900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22300</td>\n",
       "      <td>2.116700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22400</td>\n",
       "      <td>2.141200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>2.093600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22600</td>\n",
       "      <td>1.988300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22700</td>\n",
       "      <td>1.943400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22800</td>\n",
       "      <td>2.170300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22900</td>\n",
       "      <td>2.177200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>2.168500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23100</td>\n",
       "      <td>2.166900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23200</td>\n",
       "      <td>2.145800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23300</td>\n",
       "      <td>2.156700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23400</td>\n",
       "      <td>2.146000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>2.151500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23600</td>\n",
       "      <td>2.156900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23700</td>\n",
       "      <td>2.143400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23800</td>\n",
       "      <td>2.146700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23900</td>\n",
       "      <td>2.147500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>2.130500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24100</td>\n",
       "      <td>2.138400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24200</td>\n",
       "      <td>2.119200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24300</td>\n",
       "      <td>2.098600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24400</td>\n",
       "      <td>2.108400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>2.124100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24600</td>\n",
       "      <td>2.051400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24700</td>\n",
       "      <td>2.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24800</td>\n",
       "      <td>2.038800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24900</td>\n",
       "      <td>1.982100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>2.092700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25100</td>\n",
       "      <td>2.027100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25200</td>\n",
       "      <td>1.872400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25300</td>\n",
       "      <td>2.096600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25400</td>\n",
       "      <td>2.101300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>2.125700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25600</td>\n",
       "      <td>2.055300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25700</td>\n",
       "      <td>2.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25800</td>\n",
       "      <td>2.020100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25900</td>\n",
       "      <td>1.962800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>2.044700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26100</td>\n",
       "      <td>2.091600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26200</td>\n",
       "      <td>1.741000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26300</td>\n",
       "      <td>2.123800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26400</td>\n",
       "      <td>1.941000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>1.942500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26600</td>\n",
       "      <td>2.024300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26700</td>\n",
       "      <td>2.005800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26800</td>\n",
       "      <td>2.054500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26900</td>\n",
       "      <td>2.051100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>2.056300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27100</td>\n",
       "      <td>2.055600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27200</td>\n",
       "      <td>2.040800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27300</td>\n",
       "      <td>1.912600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27400</td>\n",
       "      <td>1.954200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>2.004300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27600</td>\n",
       "      <td>1.963300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27700</td>\n",
       "      <td>1.952000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27800</td>\n",
       "      <td>2.038600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27900</td>\n",
       "      <td>2.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>2.036600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28100</td>\n",
       "      <td>2.044600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28200</td>\n",
       "      <td>2.037400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28300</td>\n",
       "      <td>2.034200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28400</td>\n",
       "      <td>2.014500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>2.009700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28600</td>\n",
       "      <td>2.005800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28700</td>\n",
       "      <td>2.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28800</td>\n",
       "      <td>1.958100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28900</td>\n",
       "      <td>1.969400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>1.976900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29100</td>\n",
       "      <td>1.942000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29200</td>\n",
       "      <td>2.011800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29300</td>\n",
       "      <td>2.009900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29400</td>\n",
       "      <td>2.003500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>2.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29600</td>\n",
       "      <td>2.006600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29700</td>\n",
       "      <td>1.993800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29800</td>\n",
       "      <td>1.990600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29900</td>\n",
       "      <td>1.989000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>1.975900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30100</td>\n",
       "      <td>1.971800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30200</td>\n",
       "      <td>1.986800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30300</td>\n",
       "      <td>2.014400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30400</td>\n",
       "      <td>1.980100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>1.974300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30600</td>\n",
       "      <td>1.981700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30700</td>\n",
       "      <td>1.979800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30800</td>\n",
       "      <td>1.994200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30900</td>\n",
       "      <td>1.966500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>1.995600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31100</td>\n",
       "      <td>2.003400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31200</td>\n",
       "      <td>2.017000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31300</td>\n",
       "      <td>2.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31400</td>\n",
       "      <td>1.983100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>1.984200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31600</td>\n",
       "      <td>1.965800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31700</td>\n",
       "      <td>2.041300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31800</td>\n",
       "      <td>2.031600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31900</td>\n",
       "      <td>2.025500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>2.012800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32100</td>\n",
       "      <td>2.019200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32200</td>\n",
       "      <td>1.998900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32300</td>\n",
       "      <td>1.990200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32400</td>\n",
       "      <td>1.954700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>2.004800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32600</td>\n",
       "      <td>2.017500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32700</td>\n",
       "      <td>2.017400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32800</td>\n",
       "      <td>2.011900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32900</td>\n",
       "      <td>1.993300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>1.983700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33100</td>\n",
       "      <td>1.993300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33200</td>\n",
       "      <td>1.995600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33300</td>\n",
       "      <td>1.998100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33400</td>\n",
       "      <td>1.975200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>1.988800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33600</td>\n",
       "      <td>1.965300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33700</td>\n",
       "      <td>1.964700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33800</td>\n",
       "      <td>1.984900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33900</td>\n",
       "      <td>2.004300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>2.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34100</td>\n",
       "      <td>2.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34200</td>\n",
       "      <td>1.992100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34300</td>\n",
       "      <td>2.002900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34400</td>\n",
       "      <td>1.969500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>1.936900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34600</td>\n",
       "      <td>1.959700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34700</td>\n",
       "      <td>1.944100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34800</td>\n",
       "      <td>1.913200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34900</td>\n",
       "      <td>1.993700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>1.997500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35100</td>\n",
       "      <td>1.987500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35200</td>\n",
       "      <td>1.994300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35300</td>\n",
       "      <td>1.935900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35400</td>\n",
       "      <td>1.958600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>1.934900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35600</td>\n",
       "      <td>2.118600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35700</td>\n",
       "      <td>1.769400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35800</td>\n",
       "      <td>1.692900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35900</td>\n",
       "      <td>1.895000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>1.989100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36100</td>\n",
       "      <td>2.061900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36200</td>\n",
       "      <td>1.911300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36300</td>\n",
       "      <td>1.908900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36400</td>\n",
       "      <td>1.924800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>1.916900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36600</td>\n",
       "      <td>1.923100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36700</td>\n",
       "      <td>1.917300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36800</td>\n",
       "      <td>1.904700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36900</td>\n",
       "      <td>1.925400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>1.949100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37100</td>\n",
       "      <td>1.932300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37200</td>\n",
       "      <td>1.914000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37300</td>\n",
       "      <td>1.947500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37400</td>\n",
       "      <td>2.015200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>2.014900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37600</td>\n",
       "      <td>1.998600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37700</td>\n",
       "      <td>2.006100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37800</td>\n",
       "      <td>1.993600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37900</td>\n",
       "      <td>1.977000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>1.956100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38100</td>\n",
       "      <td>1.982500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38200</td>\n",
       "      <td>1.985700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38300</td>\n",
       "      <td>1.940600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38400</td>\n",
       "      <td>1.943900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>1.931900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38600</td>\n",
       "      <td>1.942000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38700</td>\n",
       "      <td>1.951200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38800</td>\n",
       "      <td>1.933900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38900</td>\n",
       "      <td>1.934000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>1.918200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39100</td>\n",
       "      <td>1.910300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39200</td>\n",
       "      <td>1.895700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39300</td>\n",
       "      <td>1.914200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39400</td>\n",
       "      <td>1.917700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>1.902800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39600</td>\n",
       "      <td>1.918800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39700</td>\n",
       "      <td>1.915100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39800</td>\n",
       "      <td>1.919100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39900</td>\n",
       "      <td>1.893200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>1.900300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40100</td>\n",
       "      <td>1.921100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40200</td>\n",
       "      <td>1.899300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40300</td>\n",
       "      <td>1.890600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40400</td>\n",
       "      <td>1.901100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>1.910700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40600</td>\n",
       "      <td>1.896900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40700</td>\n",
       "      <td>1.889500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40800</td>\n",
       "      <td>1.923900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40900</td>\n",
       "      <td>1.885000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>1.909500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41100</td>\n",
       "      <td>1.906600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41200</td>\n",
       "      <td>1.882700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41300</td>\n",
       "      <td>1.923300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41400</td>\n",
       "      <td>1.887600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>1.875400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41600</td>\n",
       "      <td>1.889000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41700</td>\n",
       "      <td>1.895200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41800</td>\n",
       "      <td>1.884300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41900</td>\n",
       "      <td>1.870700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>1.885500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42100</td>\n",
       "      <td>1.889300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42200</td>\n",
       "      <td>1.879200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42300</td>\n",
       "      <td>1.889600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42400</td>\n",
       "      <td>1.893700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>1.870700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42600</td>\n",
       "      <td>1.893600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42700</td>\n",
       "      <td>1.869300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42800</td>\n",
       "      <td>1.875100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42900</td>\n",
       "      <td>1.877000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>1.909100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43100</td>\n",
       "      <td>1.864600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43200</td>\n",
       "      <td>2.049300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43300</td>\n",
       "      <td>2.044700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43400</td>\n",
       "      <td>1.930600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>1.975200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43600</td>\n",
       "      <td>2.077600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43700</td>\n",
       "      <td>2.079100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43800</td>\n",
       "      <td>2.141600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43900</td>\n",
       "      <td>2.049900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>1.746200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44100</td>\n",
       "      <td>1.698700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44200</td>\n",
       "      <td>2.088500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44300</td>\n",
       "      <td>2.071800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44400</td>\n",
       "      <td>2.003500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>2.032700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44600</td>\n",
       "      <td>2.063500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44700</td>\n",
       "      <td>1.896600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44800</td>\n",
       "      <td>1.892200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44900</td>\n",
       "      <td>1.875700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>1.859300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45100</td>\n",
       "      <td>1.858500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45200</td>\n",
       "      <td>1.870100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45300</td>\n",
       "      <td>1.878900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45400</td>\n",
       "      <td>1.876900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>1.874500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45600</td>\n",
       "      <td>1.887000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45700</td>\n",
       "      <td>1.862100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45800</td>\n",
       "      <td>1.879500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45900</td>\n",
       "      <td>1.885900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>1.886800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46100</td>\n",
       "      <td>1.909600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46200</td>\n",
       "      <td>1.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46300</td>\n",
       "      <td>1.892100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46400</td>\n",
       "      <td>1.913700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>1.906400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46600</td>\n",
       "      <td>1.921500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46700</td>\n",
       "      <td>1.941900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46800</td>\n",
       "      <td>1.926000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46900</td>\n",
       "      <td>1.994900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>1.871700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47100</td>\n",
       "      <td>1.880800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47200</td>\n",
       "      <td>1.886500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47300</td>\n",
       "      <td>1.886900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47400</td>\n",
       "      <td>1.897000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>1.846800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47600</td>\n",
       "      <td>1.869500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47700</td>\n",
       "      <td>1.849800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47800</td>\n",
       "      <td>1.835600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47900</td>\n",
       "      <td>1.829100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>1.845900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48100</td>\n",
       "      <td>1.863300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48200</td>\n",
       "      <td>1.830800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48300</td>\n",
       "      <td>1.816600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48400</td>\n",
       "      <td>1.826600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48500</td>\n",
       "      <td>1.842700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48600</td>\n",
       "      <td>1.841900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48700</td>\n",
       "      <td>1.907000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48800</td>\n",
       "      <td>1.984200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48900</td>\n",
       "      <td>2.007600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>2.009100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49100</td>\n",
       "      <td>1.981400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49200</td>\n",
       "      <td>1.990400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49300</td>\n",
       "      <td>1.996800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49400</td>\n",
       "      <td>1.952000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49500</td>\n",
       "      <td>1.956400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49600</td>\n",
       "      <td>1.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49700</td>\n",
       "      <td>1.971300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49800</td>\n",
       "      <td>1.989100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49900</td>\n",
       "      <td>1.966300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>1.943700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50100</td>\n",
       "      <td>1.949300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50200</td>\n",
       "      <td>1.934700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50300</td>\n",
       "      <td>1.925200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50400</td>\n",
       "      <td>1.889100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50500</td>\n",
       "      <td>1.905600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50600</td>\n",
       "      <td>1.932100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50700</td>\n",
       "      <td>1.921800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50800</td>\n",
       "      <td>1.898100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50900</td>\n",
       "      <td>1.892900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>1.898200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51100</td>\n",
       "      <td>1.887400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51200</td>\n",
       "      <td>1.856100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51300</td>\n",
       "      <td>1.878100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51400</td>\n",
       "      <td>1.873000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51500</td>\n",
       "      <td>1.916200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51600</td>\n",
       "      <td>1.902300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51700</td>\n",
       "      <td>1.911200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51800</td>\n",
       "      <td>1.876900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51900</td>\n",
       "      <td>1.899700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>1.842900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52100</td>\n",
       "      <td>1.854700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52200</td>\n",
       "      <td>1.898100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52300</td>\n",
       "      <td>1.880700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52400</td>\n",
       "      <td>1.874700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52500</td>\n",
       "      <td>1.870700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52600</td>\n",
       "      <td>1.878400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52700</td>\n",
       "      <td>1.872900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52800</td>\n",
       "      <td>1.881800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52900</td>\n",
       "      <td>1.895800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>1.900200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53100</td>\n",
       "      <td>1.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53200</td>\n",
       "      <td>1.869300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53300</td>\n",
       "      <td>1.888200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53400</td>\n",
       "      <td>1.876600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53500</td>\n",
       "      <td>1.853400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53600</td>\n",
       "      <td>1.862600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53700</td>\n",
       "      <td>1.859200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53800</td>\n",
       "      <td>1.874900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53900</td>\n",
       "      <td>1.864800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>1.878400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54100</td>\n",
       "      <td>1.849200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54200</td>\n",
       "      <td>1.862400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54300</td>\n",
       "      <td>1.859600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54400</td>\n",
       "      <td>1.864600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54500</td>\n",
       "      <td>1.878000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54600</td>\n",
       "      <td>1.873300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54700</td>\n",
       "      <td>1.889300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54800</td>\n",
       "      <td>1.895100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54900</td>\n",
       "      <td>1.851400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>1.819800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55100</td>\n",
       "      <td>1.843400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55200</td>\n",
       "      <td>1.868400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55300</td>\n",
       "      <td>1.862000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55400</td>\n",
       "      <td>1.865700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55500</td>\n",
       "      <td>1.859600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55600</td>\n",
       "      <td>1.898000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55700</td>\n",
       "      <td>1.867400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55800</td>\n",
       "      <td>1.872800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55900</td>\n",
       "      <td>1.871700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>1.835500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56100</td>\n",
       "      <td>1.844500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56200</td>\n",
       "      <td>1.820200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56300</td>\n",
       "      <td>1.823700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56400</td>\n",
       "      <td>1.857600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56500</td>\n",
       "      <td>1.849600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56600</td>\n",
       "      <td>1.868600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56700</td>\n",
       "      <td>1.884900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56800</td>\n",
       "      <td>1.856800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56900</td>\n",
       "      <td>1.878000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>1.871200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57100</td>\n",
       "      <td>1.853700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57200</td>\n",
       "      <td>1.838600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57300</td>\n",
       "      <td>1.851100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57400</td>\n",
       "      <td>1.838900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57500</td>\n",
       "      <td>1.835900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57600</td>\n",
       "      <td>1.850700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57700</td>\n",
       "      <td>1.829300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57800</td>\n",
       "      <td>1.840400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57900</td>\n",
       "      <td>1.840700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>1.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58100</td>\n",
       "      <td>1.858500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58200</td>\n",
       "      <td>1.871900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58300</td>\n",
       "      <td>1.887200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58400</td>\n",
       "      <td>1.861300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58500</td>\n",
       "      <td>1.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58600</td>\n",
       "      <td>1.867100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58700</td>\n",
       "      <td>1.847200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58800</td>\n",
       "      <td>1.857800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58900</td>\n",
       "      <td>1.862400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>1.844500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59100</td>\n",
       "      <td>1.844700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59200</td>\n",
       "      <td>1.851600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59300</td>\n",
       "      <td>1.857300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59400</td>\n",
       "      <td>1.853600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59500</td>\n",
       "      <td>1.829800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59600</td>\n",
       "      <td>1.822000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59700</td>\n",
       "      <td>1.827300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59800</td>\n",
       "      <td>1.804500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59900</td>\n",
       "      <td>1.818200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>1.822800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60100</td>\n",
       "      <td>1.819700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60200</td>\n",
       "      <td>1.828400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60300</td>\n",
       "      <td>1.825100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60400</td>\n",
       "      <td>1.924500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60500</td>\n",
       "      <td>1.994800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60600</td>\n",
       "      <td>2.013600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60700</td>\n",
       "      <td>2.087700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60800</td>\n",
       "      <td>2.190900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60900</td>\n",
       "      <td>2.122500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>1.933100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61100</td>\n",
       "      <td>1.975400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61200</td>\n",
       "      <td>2.054400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61300</td>\n",
       "      <td>2.056400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61400</td>\n",
       "      <td>2.008300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61500</td>\n",
       "      <td>1.981100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61600</td>\n",
       "      <td>2.030400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61700</td>\n",
       "      <td>1.966100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61800</td>\n",
       "      <td>2.009000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61900</td>\n",
       "      <td>2.017900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>2.017200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62100</td>\n",
       "      <td>2.005100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62200</td>\n",
       "      <td>2.015100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62300</td>\n",
       "      <td>2.016900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62400</td>\n",
       "      <td>2.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62500</td>\n",
       "      <td>2.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62600</td>\n",
       "      <td>2.008200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62700</td>\n",
       "      <td>1.939100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62800</td>\n",
       "      <td>1.826900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62900</td>\n",
       "      <td>1.835500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>1.821200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63100</td>\n",
       "      <td>1.813800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63200</td>\n",
       "      <td>1.751700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63300</td>\n",
       "      <td>1.768700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63400</td>\n",
       "      <td>1.820900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63500</td>\n",
       "      <td>1.744500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63600</td>\n",
       "      <td>1.783300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63700</td>\n",
       "      <td>1.800400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63800</td>\n",
       "      <td>1.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63900</td>\n",
       "      <td>1.866700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>1.918200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64100</td>\n",
       "      <td>1.914900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64200</td>\n",
       "      <td>1.922000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64300</td>\n",
       "      <td>1.919300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64400</td>\n",
       "      <td>1.907800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64500</td>\n",
       "      <td>1.919000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64600</td>\n",
       "      <td>1.924200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64700</td>\n",
       "      <td>1.912500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64800</td>\n",
       "      <td>1.918900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64900</td>\n",
       "      <td>1.912600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>1.930600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65100</td>\n",
       "      <td>1.911400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65200</td>\n",
       "      <td>1.911600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65300</td>\n",
       "      <td>1.915700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65400</td>\n",
       "      <td>1.927500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65500</td>\n",
       "      <td>1.922800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65600</td>\n",
       "      <td>1.909700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65700</td>\n",
       "      <td>1.915000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65800</td>\n",
       "      <td>1.913000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65900</td>\n",
       "      <td>1.914100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>1.921900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66100</td>\n",
       "      <td>1.924000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66200</td>\n",
       "      <td>1.900800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66300</td>\n",
       "      <td>1.899700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66400</td>\n",
       "      <td>1.931000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66500</td>\n",
       "      <td>1.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66600</td>\n",
       "      <td>1.927000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66700</td>\n",
       "      <td>1.913000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66800</td>\n",
       "      <td>1.926200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66900</td>\n",
       "      <td>1.917900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>1.913400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67100</td>\n",
       "      <td>1.918300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67200</td>\n",
       "      <td>1.916000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67300</td>\n",
       "      <td>1.932700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67400</td>\n",
       "      <td>1.929800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67500</td>\n",
       "      <td>1.915100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67600</td>\n",
       "      <td>1.916300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67700</td>\n",
       "      <td>1.919600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67800</td>\n",
       "      <td>1.924500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67900</td>\n",
       "      <td>1.909000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>1.927600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68100</td>\n",
       "      <td>1.915600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68200</td>\n",
       "      <td>1.923800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68300</td>\n",
       "      <td>1.896900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68400</td>\n",
       "      <td>1.887400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68500</td>\n",
       "      <td>1.896700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68600</td>\n",
       "      <td>1.892400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68700</td>\n",
       "      <td>1.902300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68800</td>\n",
       "      <td>1.901900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68900</td>\n",
       "      <td>1.897800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69000</td>\n",
       "      <td>1.883300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69100</td>\n",
       "      <td>1.894200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69200</td>\n",
       "      <td>1.900400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69300</td>\n",
       "      <td>1.904100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69400</td>\n",
       "      <td>1.888600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69500</td>\n",
       "      <td>1.881900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69600</td>\n",
       "      <td>1.901300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69700</td>\n",
       "      <td>1.889200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69800</td>\n",
       "      <td>1.889600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69900</td>\n",
       "      <td>1.898800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>1.914000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70100</td>\n",
       "      <td>1.897800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70200</td>\n",
       "      <td>1.903100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70300</td>\n",
       "      <td>1.887400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70400</td>\n",
       "      <td>1.882700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70500</td>\n",
       "      <td>1.898700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70600</td>\n",
       "      <td>1.900100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70700</td>\n",
       "      <td>1.878800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70800</td>\n",
       "      <td>1.888600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70900</td>\n",
       "      <td>1.883500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71000</td>\n",
       "      <td>1.889400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71100</td>\n",
       "      <td>1.897700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71200</td>\n",
       "      <td>1.897800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71300</td>\n",
       "      <td>1.891900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71400</td>\n",
       "      <td>1.905400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71500</td>\n",
       "      <td>1.899900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71600</td>\n",
       "      <td>1.887800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71700</td>\n",
       "      <td>1.897800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71800</td>\n",
       "      <td>1.887800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71900</td>\n",
       "      <td>1.898300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>1.902100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72100</td>\n",
       "      <td>1.892000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72200</td>\n",
       "      <td>1.899500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72300</td>\n",
       "      <td>1.887800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72400</td>\n",
       "      <td>1.890700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72500</td>\n",
       "      <td>1.905800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72600</td>\n",
       "      <td>1.932300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72700</td>\n",
       "      <td>2.063200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72800</td>\n",
       "      <td>2.046300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72900</td>\n",
       "      <td>2.076400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73000</td>\n",
       "      <td>1.947700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73100</td>\n",
       "      <td>1.893000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73200</td>\n",
       "      <td>1.887900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73300</td>\n",
       "      <td>1.938900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73400</td>\n",
       "      <td>1.901100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73500</td>\n",
       "      <td>1.997600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73600</td>\n",
       "      <td>1.960700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73700</td>\n",
       "      <td>1.906100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73800</td>\n",
       "      <td>1.940200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73900</td>\n",
       "      <td>1.967200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>1.945700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74100</td>\n",
       "      <td>1.913900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74200</td>\n",
       "      <td>1.986000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74300</td>\n",
       "      <td>1.841200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74400</td>\n",
       "      <td>1.852900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74500</td>\n",
       "      <td>1.844100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74600</td>\n",
       "      <td>1.966900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74700</td>\n",
       "      <td>1.846700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74800</td>\n",
       "      <td>1.875600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74900</td>\n",
       "      <td>1.822700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>1.814700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75100</td>\n",
       "      <td>1.884700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75200</td>\n",
       "      <td>1.868200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75300</td>\n",
       "      <td>1.857800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75400</td>\n",
       "      <td>1.874400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75500</td>\n",
       "      <td>1.824500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75600</td>\n",
       "      <td>1.790800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75700</td>\n",
       "      <td>1.759100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75800</td>\n",
       "      <td>1.782700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75900</td>\n",
       "      <td>1.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>1.787400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76100</td>\n",
       "      <td>1.770700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76200</td>\n",
       "      <td>1.815500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76300</td>\n",
       "      <td>1.800900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76400</td>\n",
       "      <td>1.820400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76500</td>\n",
       "      <td>1.902400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76600</td>\n",
       "      <td>1.894700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76700</td>\n",
       "      <td>1.904500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76800</td>\n",
       "      <td>1.891300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76900</td>\n",
       "      <td>1.902800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77000</td>\n",
       "      <td>1.889200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77100</td>\n",
       "      <td>1.890600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77200</td>\n",
       "      <td>1.887600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77300</td>\n",
       "      <td>1.897900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77400</td>\n",
       "      <td>1.882600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77500</td>\n",
       "      <td>1.875800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77600</td>\n",
       "      <td>1.878100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77700</td>\n",
       "      <td>1.890600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77800</td>\n",
       "      <td>1.884900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77900</td>\n",
       "      <td>1.882600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>1.893200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78100</td>\n",
       "      <td>1.901800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78200</td>\n",
       "      <td>1.893600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78300</td>\n",
       "      <td>1.881500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78400</td>\n",
       "      <td>1.887000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78500</td>\n",
       "      <td>1.880200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78600</td>\n",
       "      <td>1.888900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78700</td>\n",
       "      <td>1.886200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78800</td>\n",
       "      <td>1.901300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Sample generated at step 1000]:\n",
      "中国首都是哪?首都是哪?首都是哪,首都是哪,首都是�\n",
      "\n",
      "\n",
      "[Sample generated at step 2000]:\n",
      "中国首都是哪?比如今天,但是哪,哪的是哪,哪的是哪,哪\n",
      "\n",
      "\n",
      "[Sample generated at step 3000]:\n",
      "中国首都是哪?每天款车型,每天款车型,每天款�\n",
      "\n",
      "\n",
      "[Sample generated at step 4000]:\n",
      "中国首都是哪?首都是哪,首都是哪,首都是哪,首都是�\n",
      "\n",
      "\n",
      "[Sample generated at step 5000]:\n",
      "中国首都是哪?首都是一个美国的美国,但是一个美国的美�\n",
      "\n",
      "\n",
      "[Sample generated at step 6000]:\n",
      "中国首都是哪?首次哪个人的经济,但是,哪个人的经济,�\n",
      "\n",
      "\n",
      "[Sample generated at step 7000]:\n",
      "中国首都是哪?首都是哪,首都是哪,首都是哪,首都是�\n",
      "\n",
      "\n",
      "[Sample generated at step 8000]:\n",
      "中国首都是哪?没有自己的车型,但是在这个车型的时�\n",
      "\n",
      "\n",
      "[Sample generated at step 9000]:\n",
      "中国首都是哪?每个人都是哪,每个人都是哪,每个人都是�\n",
      "\n",
      "\n",
      "[Sample generated at step 10000]:\n",
      "中国首都是哪? 小编给了,但是,这个月的经济都是一个比\n",
      "\n",
      "\n",
      "[Sample generated at step 11000]:\n",
      "中国首都是哪?首先,我们的经常是一个观察的,我们的经常\n",
      "\n",
      "\n",
      "[Sample generated at step 12000]:\n",
      "中国首都是哪? 哪是哪是哪,是哪,是哪,是哪,是哪,是哪,\n",
      "\n",
      "\n",
      "[Sample generated at step 13000]:\n",
      "中国首都是哪?首都是哪,首都是哪,首都是哪,首都是�\n",
      "\n",
      "\n",
      "[Sample generated at step 14000]:\n",
      "中国首都是哪? 我们的经济,我们的经济,我们的经济,我�\n",
      "\n",
      "\n",
      "[Sample generated at step 15000]:\n",
      "中国首都是哪?没有哪些哪些,但是我们的哪些哪些,我\n",
      "\n",
      "\n",
      "[Sample generated at step 16000]:\n",
      "中国首都是哪?每个人都是哪？每个人都是哪？每个人�\n",
      "\n",
      "\n",
      "[Sample generated at step 17000]:\n",
      "中国首都是哪? 一个人的爱情,一个人的爱情,一个人的爱情,一个\n",
      "\n",
      "\n",
      "[Sample generated at step 18000]:\n",
      "中国首都是哪? 今天是一个月的日子,我们的设计,我们的设�\n",
      "\n",
      "\n",
      "[Sample generated at step 19000]:\n",
      "中国首都是哪? 今天我们的爱情是一个比较高的,但是我们的�\n",
      "\n",
      "\n",
      "[Sample generated at step 20000]:\n",
      "中国首都是哪? 我们的讲解,我们的讲解,我们的讲解,\n",
      "\n",
      "\n",
      "[Sample generated at step 21000]:\n",
      "中国首都是哪? 今天,我们没有一个温暖的第一个温暖,\n",
      "\n",
      "\n",
      "[Sample generated at step 22000]:\n",
      "中国首都是哪?我们的父母是哪,我们的父母是哪,我们的�\n",
      "\n",
      "\n",
      "[Sample generated at step 23000]:\n",
      "中国首都是哪? 我们的爱情是一个人的爱情,我们的爱情是一个\n",
      "\n",
      "\n",
      "[Sample generated at step 24000]:\n",
      "中国首都是哪? 一个人的爱,一个人的爱,一个人的爱,一个人的爱,\n",
      "\n",
      "\n",
      "[Sample generated at step 25000]:\n",
      "中国首都是哪? 我们经常见过了一个比较的比较的比\n",
      "\n",
      "\n",
      "[Sample generated at step 26000]:\n",
      "中国首都是哪?首都是哪？首都是哪？首都是哪？�\n",
      "\n",
      "\n",
      "[Sample generated at step 27000]:\n",
      "中国首都是哪? 我们的爱情,我们的爱情,我们的爱情,我�\n",
      "\n",
      "\n",
      "[Sample generated at step 28000]:\n",
      "中国首都是哪? 今天,我们来自美国首都的美国首都是�\n",
      "\n",
      "\n",
      "[Sample generated at step 29000]:\n",
      "中国首都是哪? 从中国首都是哪,但是在中国首都的环境中\n",
      "\n",
      "\n",
      "[Sample generated at step 30000]:\n",
      "中国首都是哪? 今天小编就给大家带来了一个清楚的美�\n",
      "\n",
      "\n",
      "[Sample generated at step 31000]:\n",
      "中国首都是哪? 一、非常清楚的经济社会,每个人都会�\n",
      "\n",
      "\n",
      "[Sample generated at step 32000]:\n",
      "中国首都是哪? 一个人的爱情,一个人的爱情,一个人的爱情,一个\n",
      "\n",
      "\n",
      "[Sample generated at step 33000]:\n",
      "中国首都是哪? 但是,这个时候,我们的爱情经历了一个�\n",
      "\n",
      "\n",
      "[Sample generated at step 34000]:\n",
      "中国首都是哪? 今天,我们来看看这个国家的第一次清晰\n",
      "\n",
      "\n",
      "[Sample generated at step 35000]:\n",
      "中国首都是哪?每天都是哪,每天都是哪,每天都是哪,每天\n",
      "\n",
      "\n",
      "[Sample generated at step 36000]:\n",
      "中国首都是哪?每个人都是哪个人,但是他们的身体和身体\n",
      "\n",
      "\n",
      "[Sample generated at step 37000]:\n",
      "中国首都是哪? 一个人的爱情,一个人的爱情,一个人的爱情,一个\n",
      "\n",
      "\n",
      "[Sample generated at step 38000]:\n",
      "中国首都是哪? 今年的首都,我们的首都是一个比较精彩的\n",
      "\n",
      "\n",
      "[Sample generated at step 39000]:\n",
      "中国首都是哪? 今天,我们在美国首都哪个哪个哪个�\n",
      "\n",
      "\n",
      "[Sample generated at step 40000]:\n",
      "中国首都是哪? 为了更好的收获,我们的经济社会,我�\n",
      "\n",
      "\n",
      "[Sample generated at step 41000]:\n",
      "中国首都是哪? 你们都知道,我们都知道,我们都知道,\n",
      "\n",
      "\n",
      "[Sample generated at step 42000]:\n",
      "中国首都是哪? 今天,我们就来说一下,我们就来说,我们的�\n",
      "\n",
      "\n",
      "[Sample generated at step 43000]:\n",
      "中国首都是哪? 就是哪！ 就是哪！ 就是哪！ 就是哪\n",
      "\n",
      "\n",
      "[Sample generated at step 44000]:\n",
      "中国首都是哪? 一、哪个人的经济是最好的经济？ 一、哪个\n",
      "\n",
      "\n",
      "[Sample generated at step 45000]:\n",
      "中国首都是哪? 今年,中国首都是哪些哪些？ 今年,中国\n",
      "\n",
      "\n",
      "[Sample generated at step 46000]:\n",
      "中国首都是哪? 今天,我们就来了解一下,我们的观众是一个�\n",
      "\n",
      "\n",
      "[Sample generated at step 47000]:\n",
      "中国首都是哪? 为了提高哪些哪些经营,哪些哪些\n",
      "\n",
      "\n",
      "[Sample generated at step 48000]:\n",
      "中国首都是哪?我们经常会有一个清明的,你们都知道,我\n",
      "\n",
      "\n",
      "[Sample generated at step 49000]:\n",
      "中国首都是哪?谁知道,我们的爱国是哪,我们的爱国是�\n",
      "\n",
      "\n",
      "[Sample generated at step 50000]:\n",
      "中国首都是哪? 我们知道,在这里,我们的爱国也是不可能的\n",
      "\n",
      "\n",
      "[Sample generated at step 51000]:\n",
      "中国首都是哪?首先,我们来看看今天的首都是一个比较�\n",
      "\n",
      "\n",
      "[Sample generated at step 52000]:\n",
      "中国首都是哪? 就是哪？ 就是哪？ 就是哪？ 就是哪\n",
      "\n",
      "\n",
      "[Sample generated at step 53000]:\n",
      "中国首都是哪? 你是否认为你是否认为你是否认为你是否�\n",
      "\n",
      "\n",
      "[Sample generated at step 54000]:\n",
      "中国首都是哪? 今天,我们就来看看吧。 今天,我们就来看看\n",
      "\n",
      "\n",
      "[Sample generated at step 55000]:\n",
      "中国首都是哪? 你是否会觉得这个人的美好？ 你是否会�\n",
      "\n",
      "\n",
      "[Sample generated at step 56000]:\n",
      "中国首都是哪? 一个人的爱,一个人的爱,一个人的爱,一个人的爱,\n",
      "\n",
      "\n",
      "[Sample generated at step 57000]:\n",
      "中国首都是哪? 今天小编就为大家带来了一些解决问题,�\n",
      "\n",
      "\n",
      "[Sample generated at step 58000]:\n",
      "中国首都是哪? 为了提高经济经济发展,汽车市场�\n",
      "\n",
      "\n",
      "[Sample generated at step 59000]:\n",
      "中国首都是哪? 今天,我们就来了一个游戏的经历,游戏的�\n",
      "\n",
      "\n",
      "[Sample generated at step 60000]:\n",
      "中国首都是哪? 今天,我们就来看看一下,我们就是哪,我们�\n",
      "\n",
      "\n",
      "[Sample generated at step 61000]:\n",
      "中国首都是哪? 今天,我们就来了解一下,我们的种种类�\n",
      "\n",
      "\n",
      "[Sample generated at step 62000]:\n",
      "中国首都是哪? 今天,我们就来到了一个游戏的游戏,游戏\n",
      "\n",
      "\n",
      "[Sample generated at step 63000]:\n",
      "中国首都是哪?每个人都会有一个游戏,但是,我们的游戏都\n",
      "\n",
      "\n",
      "[Sample generated at step 64000]:\n",
      "中国首都是哪?每个人都是哪,每个人都是哪,每个人都是�\n",
      "\n",
      "\n",
      "[Sample generated at step 65000]:\n",
      "中国首都是哪?每天都是哪,每天都是哪,每天都是哪,每天\n",
      "\n",
      "\n",
      "[Sample generated at step 66000]:\n",
      "中国首都是哪?每个人都是哪？每个人都是哪？每个人�\n",
      "\n",
      "\n",
      "[Sample generated at step 67000]:\n",
      "中国首都是哪?自己的爱情,自己的爱情,自己的爱情,�\n",
      "\n",
      "\n",
      "[Sample generated at step 68000]:\n",
      "中国首都是哪?哪儿哪儿？哪儿哪儿？哪儿�\n",
      "\n",
      "\n",
      "[Sample generated at step 69000]:\n",
      "中国首都是哪?首都是哪,首都是哪,首都是哪,首都是�\n",
      "\n",
      "\n",
      "[Sample generated at step 70000]:\n",
      "中国首都是哪?首都是哪？首都是哪？首都是哪？�\n",
      "\n",
      "\n",
      "[Sample generated at step 71000]:\n",
      "中国首都是哪?首都是哪？首都是哪？首都是哪？�\n",
      "\n",
      "\n",
      "[Sample generated at step 72000]:\n",
      "中国首都是哪?每一个人都是哪！这个人都是哪！哪！�\n",
      "\n",
      "\n",
      "[Sample generated at step 73000]:\n",
      "中国首都是哪?每天都要把自己的爱情放在一个爱情,�\n",
      "\n",
      "\n",
      "[Sample generated at step 74000]:\n",
      "中国首都是哪?没有什么样的,但是,我们的经历是,我们的经\n",
      "\n",
      "\n",
      "[Sample generated at step 75000]:\n",
      "中国首都是哪? 今天我们来看看一下介绍的一个问题,今天我\n",
      "\n",
      "\n",
      "[Sample generated at step 76000]:\n",
      "中国首都是哪? 今天,我们就来看看这个世界,我们的爱情,\n",
      "\n",
      "\n",
      "[Sample generated at step 77000]:\n",
      "中国首都是哪?每个人都是哪,每个人都是哪,每个人都是�\n",
      "\n",
      "\n",
      "[Sample generated at step 78000]:\n",
      "中国首都是哪?自从自从自己的经济,自从自己的经�\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class SampleTextCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if state.global_step % 1000 == 0:\n",
    "            prompt = \"中国首都是哪?\"\n",
    "            input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "            output = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                max_length=50,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "            gen_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            print(f\"\\n[Sample generated at step {state.global_step}]:\\n{gen_text}\\n\")\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    run_name=\"pretrain-gpt2-1\",\n",
    "    output_dir=\"./outputs/gpt2-pretrain\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=100,\n",
    "    save_steps=1000,\n",
    "    max_steps=80000,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[SampleTextCallback]\n",
    "    # eval_dataset=small_eval_dataset,\n",
    "    # compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43c1909-ee14-4b8f-b53b-b0073da4f2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "dt = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "model.save_pretrained(f'outputs/nanogpt-{dt}', safe_serialization=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb81d70-0c1d-4fde-84f5-becf84e2247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## mlp\n",
    "import torch\n",
    "config = GPTConfig(n_block=1024, n_embd=768, n_head=12, n_layer=6)\n",
    "# model = NanoGPT(config).to('cuda')\n",
    "# model = model.load_state_dict(torch.load(f'outputs/nanogpt/npt_9.pt'))\n",
    "sample(model, '春晓', 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692d41d1-1005-42bb-b0c5-45306ea00a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## moe\n",
    "# import torch\n",
    "# # config = GPTConfig(n_block=1024, n_embd=768, n_head=12, n_layer=6)\n",
    "# # model = NanoGPT(config).to('cuda')\n",
    "# # model = model.load_state_dict(torch.load(f'outputs/nanogpt/npt_9.pt'))\n",
    "# sample(model, '春晓', 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da08f4a5-388a-4751-9951-71dddbcb80cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
