{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "883daf1e-fcaf-43b3-b8d7-fb764cdfddc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: math-verify[antlr4_13_2] in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (0.3.3)\n",
      "\u001b[33mWARNING: math-verify 0.3.3 does not provide the extra 'antlr4-13-2'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: latex2sympy2_extended>=0.9.3 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from math-verify[antlr4_13_2]) (0.9.3)\n",
      "Collecting antlr4-python3-runtime==4.13.2 (from latex2sympy2_extended>=0.9.3->math-verify[antlr4_13_2])\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/89/03/a851e84fcbb85214dc637b6378121ef9a0dd61b4c65264675d8a5c9b1ae7/antlr4_python3_runtime-4.13.2-py3-none-any.whl (144 kB)\n",
      "Requirement already satisfied: sympy in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from latex2sympy2_extended>=0.9.3->math-verify[antlr4_13_2]) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from sympy->latex2sympy2_extended>=0.9.3->math-verify[antlr4_13_2]) (1.3.0)\n",
      "Installing collected packages: antlr4-python3-runtime\n",
      "  Attempting uninstall: antlr4-python3-runtime\n",
      "    Found existing installation: antlr4-python3-runtime 4.11.0\n",
      "    Uninstalling antlr4-python3-runtime-4.11.0:\n",
      "      Successfully uninstalled antlr4-python3-runtime-4.11.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "latex2sympy2 1.9.1 requires antlr4-python3-runtime==4.7.2, but you have antlr4-python3-runtime 4.13.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed antlr4-python3-runtime-4.13.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68d0459e-5021-41e7-9b49-f89c5e905222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from math_verify import parse, verify\n",
    "\n",
    "# # gold = parse(\"${1,3} \\\\cup {2,4}$\")\n",
    "# # answer = parse(\"${1,2,3,4}$\")\n",
    "\n",
    "# gold = parse(\"1+2\")\n",
    "# answer = parse(\"2\")\n",
    "\n",
    "# # Order here is important!\n",
    "# verify(gold, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1156fafa-c480-473a-924e-7d2c2b6aec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6aa93355-b84b-4f8e-8c9d-75e4188026da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class MOEConfig:\n",
    "    hidden_dim: int\n",
    "    n_expert: int\n",
    "    top_k: int\n",
    "    n_share_expert: int = 2\n",
    "    \n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, f_in, f_out):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(f_in, f_out),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "\n",
    "class MOERouter(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.gate = nn.Linear(config.hidden_dim, config.n_expert)\n",
    "        self.n_expert = config.n_expert\n",
    "        self.top_k = config.top_k\n",
    "\n",
    "    def forward(self, x):\n",
    "        # gate logits\n",
    "        router_logits = self.gate(x) # (B*ns, n_expert)\n",
    "\n",
    "        # top k\n",
    "        # weights (B*ns, top_k)\n",
    "        weights, indices = torch.topk(router_logits, self.top_k, dim=-1)\n",
    "\n",
    "        # norm\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "        # expert mask (B*ns, top_k, n_expert)\n",
    "        expert_mask = F.one_hot(indices, num_classes=self.n_expert)\n",
    "        # permute (n_expert, top_k, B*ns)\n",
    "        expert_mask = expert_mask.permute(2, 1, 0)\n",
    "\n",
    "        return router_logits, weights, indices, expert_mask\n",
    "        \n",
    "    \n",
    "class SparseMOE(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.experts = nn.ModuleList(\n",
    "            [Expert(config.hidden_dim, config.hidden_dim) for _ in range(config.n_expert)]\n",
    "        )\n",
    "        self.router = MOERouter(config)\n",
    "        self.hidden_dim = config.hidden_dim\n",
    "        self.n_expert = config.n_expert\n",
    "        self.top_k = config.top_k\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, ns, nh = x.size()\n",
    "        # (B*ns, nh)\n",
    "        hs = x.view(-1, nh)\n",
    "\n",
    "        # router select\n",
    "        router_logits, weights, indices, expert_mask = self.router(hs)\n",
    "\n",
    "        # print(router_logits.shape, weights.shape, expert_mask.shape)\n",
    "\n",
    "        # \n",
    "        final_hs = torch.zeros((B*ns, nh), dtype=x.dtype).to(device)\n",
    "\n",
    "        for idx in range(self.n_expert):\n",
    "            expert_layer = self.experts[idx]\n",
    "            # (n_expert, top_k, B)\n",
    "            idx, token_idx = torch.where(expert_mask[idx])\n",
    "            # (len(token_idx), nh)\n",
    "            current_state = hs.unsqueeze(0)[:, token_idx, :].reshape(-1, nh)\n",
    "            # current_hs * weights\n",
    "            # weights (B*ns, top_k) -> (len(token_idx)*len(idx), 1)\n",
    "            router_weights = weights[token_idx, idx].unsqueeze(-1)\n",
    "            current_hs = expert_layer(current_state) \n",
    "            # (len(token_idx, nh) * (len(token_idx), 1)\n",
    "            current_hs =  current_hs * router_weights\n",
    "            # add \n",
    "            final_hs[token_idx]+=current_hs\n",
    "        final_hs = final_hs.view(B, ns, nh)\n",
    "        return final_hs, router_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f751c4b-c3a1-477a-b393-edf5583d98ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers import PretrainedConfig\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig(PretrainedConfig):\n",
    "    n_block: int\n",
    "    n_embd: int\n",
    "    n_head: int\n",
    "    n_layer: int\n",
    "    n_vocab: int = 50257\n",
    "    dropout: float = 0.1\n",
    "    n_expert: int = 8\n",
    "    top_k: int = 2\n",
    "    eos_token_id: int = 50256\n",
    "    model_type: str = 'gpt2'\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ace27e7f-1584-4234-b73a-1c0dee59f54a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50257])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PreTrainedModel\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.wq = nn.Linear(config.n_embd ,config.n_embd // config.n_head)\n",
    "        self.wk = nn.Linear(config.n_embd, config.n_embd // config.n_head)\n",
    "        self.wv = nn.Linear(config.n_embd, config.n_embd // config.n_head)\n",
    "        # self.dropout = nn.Dropout(config.dropout)\n",
    "        self.register_buffer('tril_mask', torch.tril(torch.ones(config.n_block, config.n_block)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        # (B, T, nH)\n",
    "        q,k,v = self.wq(x), self.wk(x), self.wv(x)\n",
    "        if FLASH:\n",
    "            output = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        else:   \n",
    "            # q@k / sqrt(k)\n",
    "            qk = torch.matmul(q, k.transpose(-2,-1)).masked_fill(self.tril_mask[:T,:T] ==0, float('-inf')) / (config.n_embd ** 0.5)\n",
    "            attn = F.softmax(qk, dim=-1)\n",
    "            output = attn @ v\n",
    "        return output\n",
    "class MultiHeadAttn(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([\n",
    "           Attention(config) for _ in range(config.n_head)\n",
    "        ])\n",
    "        self.linear = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        ouput = self.linear(output)\n",
    "        return output\n",
    "        \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(config.n_embd, config.n_embd * 4)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.linear2 = nn.Linear(config.n_embd * 4, config.n_embd)\n",
    "        # self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "class Layer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # self.pos = nn.Embedding(config.n_block, config.n_embd)\n",
    "        # self.embd = nn.Embedding(config.n_vocab, config.n_embd)\n",
    "        self.norm1 = nn.LayerNorm(config.n_embd)\n",
    "        self.norm2 = nn.LayerNorm(config.n_embd)\n",
    "        moe_config = MOEConfig(hidden_dim=config.n_embd, n_expert=config.n_expert, top_k=config.top_k)\n",
    "        self.mlp = MLP(config)\n",
    "        self.moe = SparseMOE(moe_config)\n",
    "        self.mha = MultiHeadAttn(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.mha(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "        \n",
    "class NanoGPT(PreTrainedModel):\n",
    "    config_class = GPTConfig\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.wte = nn.Embedding(config.n_vocab, config.n_embd)\n",
    "        self.wpe = nn.Embedding(config.n_block, config.n_embd)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.n_vocab, bias=False)\n",
    "        self.layers = nn.Sequential(*[Layer(config) for _ in range(config.n_layer)])\n",
    "        self.norm1 = nn.LayerNorm(config.n_embd)\n",
    "        self.wte.weight = self.lm_head.weight # reduce train cost\n",
    "        self.eos_token_id = config.eos_token_id\n",
    "        self.n_vocab = config.n_vocab\n",
    "        \n",
    "        # init all weights, use a torch rng object to be very careful\n",
    "        self.init_rng = torch.Generator()\n",
    "        self.init_rng.manual_seed(42)\n",
    "        self.apply(self._init_weight)\n",
    "        # self.init_weights()\n",
    "\n",
    "    # def _init_weight(self, module):\n",
    "    #     # print('init weight!')\n",
    "    #     if isinstance(module, nn.Linear):\n",
    "    #         torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    #         if module.bias is not None:\n",
    "    #             torch.nn.init.zeros_(module.bias)\n",
    "    #     elif isinstance(module, nn.Embedding):\n",
    "    #         torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def _init_weight(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02 * ((2*self.config.n_layer) ** -2)\n",
    "            # we want to skip initializing lm_head, which shares parameters with wte\n",
    "            # and wte was already initialized down below during the embedding init\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std, generator=self.init_rng)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02, generator=self.init_rng)\n",
    "        \n",
    "    def forward(self, input_ids, labels=None, **kwargs):\n",
    "        # print(input_ids)\n",
    "        # print(labels)\n",
    "        x = input_ids\n",
    "        targets = labels\n",
    "        B, seq_len = x.size()\n",
    "        # B,n_block\n",
    "        pos = torch.arange(seq_len, device=x.device, dtype=torch.long)\n",
    "        x = self.wte(x) + self.wpe(pos)\n",
    "        x = self.layers(x)\n",
    "        x = self.norm1(x)\n",
    "        if targets is None:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            loss = None\n",
    "        else:\n",
    "            logits = self.lm_head(x) # B, seq_len, n_vocab\n",
    "            shape_logits = logits[:,:-1,:].contiguous().view(-1, self.n_vocab)\n",
    "            targets = targets[:,1:].contiguous().view(-1)\n",
    "            # print(shape_logits.shape, targets.shape)\n",
    "            loss = F.cross_entropy(shape_logits, targets, ignore_index=-100)\n",
    "        return (loss, logits) if loss is not None else logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, input_ids, max_length, temperature=1.0):\n",
    "        x = input_ids\n",
    "        for _ in range(max_length):\n",
    "            idx_cond = x if x.size(1)<=self.config.n_block else x[:, -self.config.n_block:]\n",
    "            logits = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature # last token\n",
    "            probs = F.softmax(logits, dim=-1) # B, n_vocab\n",
    "            predict = torch.multinomial(probs, num_samples=1) # B, 1\n",
    "            if self.eos_token_id and self.eos_token_id == predict.item():\n",
    "                return x\n",
    "            x = torch.cat([x, predict], dim=-1)\n",
    "        return x\n",
    "    \n",
    "\n",
    "# config = GPTConfig(n_block=4, n_embd=8, n_head=2, n_layer=1)\n",
    "# attn = Attention(config)\n",
    "# x = torch.randn(2, 4, 8)\n",
    "# attn(x).shape\n",
    "# config = GPTConfig(n_block=4, n_embd=8, n_head=2, n_layer=1)\n",
    "# attn = MultiHeadAttn(config)\n",
    "# x = torch.randn(2, 4, 8)\n",
    "# attn(x).shape\n",
    "# config = GPTConfig(n_block=4, n_embd=8, n_head=2, n_layer=1)\n",
    "# attn = MLP(config)\n",
    "# x = torch.randn(2, 4, 8)\n",
    "# attn(x).shape\n",
    "\n",
    "FLASH = 0\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "config = GPTConfig(n_block=4, n_embd=8, n_head=2, n_layer=2)\n",
    "attn = NanoGPT(config).to(device)\n",
    "x = torch.arange(4).unsqueeze(0).repeat(4,1).to(device)\n",
    "attn(x)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a74835a0-9e31-4f6b-9479-7cc16cab07f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "output_dir = f'outputs/nanogpt'\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "config = GPTConfig(n_block=1024, n_embd=768, n_head=12, n_layer=12)\n",
    "model = NanoGPT(config)\n",
    "# model.save_pretrained(output_dir, safe_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "434d3a80-35e0-46d4-97da-8346be5bf88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total param 172.81576538085938m\n"
     ]
    }
   ],
   "source": [
    "def print_parameters(model):\n",
    "    num_param = sum([param.numel() for param in model.parameters() if param.requires_grad])\n",
    "    print(f'total param {num_param/1024/1024}m')\n",
    "    \n",
    "def sample(model, query, max_new_tokens=128):\n",
    "    tokens = torch.tensor(tokenizer.encode(query), dtype=torch.long).unsqueeze(0)\n",
    "    outputs = model.generate(tokens.to(device), max_new_tokens)\n",
    "    return tokenizer.decode(outputs.view(-1).cpu().numpy())\n",
    "    \n",
    "print_parameters(model)\n",
    "# print(sample(model, \"中国首都是哪?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0297beda-ede2-404c-802a-2700f2092ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'overflow_to_sample_mapping'],\n",
       "    num_rows: 2706236\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ds = load_dataset(\"p208p2002/wudao\", streaming=True, split=\"train\")\n",
    "ds = load_dataset(\"wikimedia/wikipedia\", \"20231101.zh\", split=\"train\")\n",
    "\n",
    "def encode(examples):\n",
    "    result = tokenizer(examples['title'], examples['text'], truncation=True, padding='max_length', return_overflowing_tokens=True)\n",
    "    return result\n",
    "\n",
    "def collate_fn(examples):\n",
    "    # print(examples[0])\n",
    "    x = torch.tensor([x['input_ids'] for x in examples], dtype=torch.long)\n",
    "    y = torch.tensor([x['input_ids'] for x in examples], dtype=torch.long)\n",
    "    # print(x.shape, y.shape)\n",
    "    return x, y\n",
    "\n",
    "ds = ds.map(encode, batched=True, remove_columns=['url','id', 'text','title'])\n",
    "# split_ds = ds.train_test_split(test_size=0.1)\n",
    "# ds = ds.map(encode, batched=True)\n",
    "# print(next(iter(train_ds)))\n",
    "ds\n",
    "# train_loader = DataLoader(ds, batch_size=12, collate_fn=lambda x: collate_fn(x))\n",
    "# item = next(iter(train_loader))\n",
    "# print(item[0].shape, item[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cf0c04d-f55b-4296-be76-5d1451c9c72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Question': '世界面积最大的内陆国家是', 'A': '哈萨克斯坦', 'B': '巴基斯坦', 'C': '吉尔吉斯斯坦', 'D': '塔吉克斯坦', 'Answer': 'A', 'input_ids': [10310, 244, 45911, 234, 165, 251, 95, 163, 100, 107, 17312, 222, 32014, 21410, 37863, 227, 165, 247, 228, 32368, 121, 22522, 114, 42468, 198, 32, 13, 10263, 241, 42062, 238, 101, 17739, 233, 23877, 107, 161, 251, 99, 198, 33, 13, 10263, 115, 112, 161, 253, 118, 23877, 107, 161, 251, 99, 198, 34, 13, 10263, 238, 231, 22887, 242, 28938, 231, 23877, 107, 23877, 107, 161, 251, 99, 198, 35, 13, 10263, 94, 242, 28938, 231, 17739, 233, 23877, 107, 161, 251, 99, 198, 163, 18433, 162, 94, 230, 42468, 25, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [32]}\n"
     ]
    }
   ],
   "source": [
    "# ds['input_ids']\n",
    "\n",
    "# Load the \"all\" subset or a specific subject like \"computer_science\"\n",
    "cmmlu = load_dataset(\"haonan-li/cmmlu\", \"high_school_geography\", split='dev')\n",
    "\n",
    "# We'll use the validation set\n",
    "# eval_ds = cmmlu[\"validation\"]\n",
    "def preprocess(example):\n",
    "    question = example[\"Question\"]\n",
    "    choices = example[\"A\"], example[\"B\"], example[\"C\"], example[\"D\"]\n",
    "    context = f\"{question}\\nA. {choices[0]}\\nB. {choices[1]}\\nC. {choices[2]}\\nD. {choices[3]}\\n答案是:\"\n",
    "\n",
    "    result =  tokenizer(context, truncation=True, padding=\"max_length\", max_length=512)\n",
    "    result['labels'] = tokenizer.encode(example['Answer'])\n",
    "    return result\n",
    "\n",
    "eval_ds = cmmlu.map(preprocess)\n",
    "print(eval_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "209c3a12-fe9e-4263-a7a0-c810615cac08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    print(labels)\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = (preds == labels).mean()\n",
    "    return {\"accuracy\": acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e59ea67-d58b-4a8d-8f39-9cac93e342ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-07 09:29:04,385] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: warning: librt.so.1, needed by /usr/local/cuda/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: warning: libpthread.so.0, needed by /usr/local/cuda/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: warning: libstdc++.so.6, needed by /usr/local/cuda/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: warning: libm.so.6, needed by /usr/local/cuda/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::runtime_error::~runtime_error()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__gxx_personality_v0@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::tellp()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::chrono::_V2::steady_clock::now()@GLIBCXX_3.4.19'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_M_replace_aux(unsigned long, unsigned long, unsigned long, char)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for bool@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_logic_error(char const*)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::logic_error@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::locale::~locale()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(std::string const&, unsigned long, unsigned long)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_end_catch@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ofstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::logic_error::~logic_error()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for __cxxabiv1::__si_class_type_info@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<char, std::char_traits<char> >::_M_cache_locale(std::locale const&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator new[](unsigned long)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_M_leak_hard()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ifstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_streambuf<wchar_t, std::char_traits<wchar_t> >::basic_streambuf(std::basic_streambuf<wchar_t, std::char_traits<wchar_t> > const&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::append(char const*, unsigned long)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(std::string const&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned short@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::resize(unsigned long, char)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for char const*@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ctype<char>::_M_widen_init() const@GLIBCXX_3.4.11'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_invalid_argument(char const*)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::locale::operator=(std::locale const&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<wchar_t, std::char_traits<wchar_t> >::_M_cache_locale(std::locale const&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_decrement(std::_Rb_tree_node_base const*)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_free_exception@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::condition_variable::notify_one()@GLIBCXX_3.4.11'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::Init::~Init()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::~basic_string()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_pure_virtual@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::flush()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for __cxxabiv1::__class_type_info@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_rethrow@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_fstream<char, std::char_traits<char> >::~basic_fstream()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::compare(char const*) const@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ostringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::locale::locale()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::chrono::_V2::system_clock::now()@GLIBCXX_3.4.19'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ifstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Hash_bytes(void const*, unsigned long, unsigned long)@CXXABI_1.3.5'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<long long>(long long)@GLIBCXX_3.4.9'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for char*@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_Prime_rehash_policy::_M_need_rehash(unsigned long, unsigned long, unsigned long) const@GLIBCXX_3.4.18'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::out_of_range@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<unsigned long>(unsigned long)@GLIBCXX_3.4.9'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_increment(std::_Rb_tree_node_base const*)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::~ios_base()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::range_error::~range_error()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__basic_file<char>::~__basic_file()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_guard_acquire@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<bool>(bool)@GLIBCXX_3.4.9'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::overflow_error@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_fstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::range_error@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ios<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_filebuf<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator delete[](void*)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(unsigned long, char, std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_List_node_base::_M_transfer(std::__detail::_List_node_base*, std::__detail::_List_node_base*)@GLIBCXX_3.4.15'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::replace(unsigned long, unsigned long, char const*, unsigned long)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for std::exception@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >::_Rep::_M_destroy(std::allocator<wchar_t> const&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::istream& std::istream::_M_extract<double>(double&)@GLIBCXX_3.4.9'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::close()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_fstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ifstream<char, std::char_traits<char> >::basic_ifstream(char const*, std::_Ios_Openmode)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::append(std::string const&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator new(unsigned long)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_istringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned int@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::append(char const*)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::domain_error@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::find(char, unsigned long) const@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::put(char)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for int@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_bad_alloc()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_thread_atexit@CXXABI_1.3.7'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned int*@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_increment(std::_Rb_tree_node_base*)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ifstream<char, std::char_traits<char> >::~basic_ifstream()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::Init::Init()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::condition_variable::condition_variable()@GLIBCXX_3.4.11'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::basic_filebuf()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::domain_error::~domain_error()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::cerr@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::find(char const*, unsigned long, unsigned long) const@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >::str() const@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::invalid_argument@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for void*@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::assign(std::string const&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_ostringstream()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_rebalance_for_erase(std::_Rb_tree_node_base*, std::_Rb_tree_node_base&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned long@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_List_node_base::_M_hook(std::__detail::_List_node_base*)@GLIBCXX_3.4.15'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_List_node_base::_M_unhook()@GLIBCXX_3.4.15'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ostringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >::_M_sync(char*, unsigned long, unsigned long)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_iostream<char, std::char_traits<char> >::~basic_iostream()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::locale::locale(std::locale const&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_istringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `log2f@GLIBC_2.2.5'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::operator<<(std::basic_streambuf<char, std::char_traits<char> >*)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_streambuf<wchar_t, std::char_traits<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::exception::~exception()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_Rep::_S_create(unsigned long, unsigned long, std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__basic_file<char>::is_open() const@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_istringstream()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::swap(std::string&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_streambuf<char, std::char_traits<char> >::basic_streambuf(std::basic_streambuf<char, std::char_traits<char> > const&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<char, std::char_traits<char> >::init(std::basic_streambuf<char, std::char_traits<char> >*)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_bad_cast()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<char, std::char_traits<char> >::clear(std::_Ios_Iostate)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_streambuf<wchar_t, std::char_traits<wchar_t> >::operator=(std::basic_streambuf<wchar_t, std::char_traits<wchar_t> > const&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for long*@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator delete(void*)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::operator<<(int)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_Rep::_S_empty_rep_storage@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_Rep::_M_destroy(std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_iostream<wchar_t, std::char_traits<wchar_t> >::~basic_iostream()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::runtime_error@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ofstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_insert_and_rebalance(bool, std::_Rb_tree_node_base*, std::_Rb_tree_node_base*, std::_Rb_tree_node_base&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_stringstream()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_stringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<long>(long)@GLIBCXX_3.4.9'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::istream::get()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned long long@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::out_of_range::~out_of_range()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::length_error::~length_error()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::__ostream_insert<char, std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*, long)@GLIBCXX_3.4.9'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::invalid_argument::~invalid_argument()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >::swap(std::basic_string<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::cout@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<unsigned long long>(unsigned long long)@GLIBCXX_3.4.9'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<void const*>(void const*)@GLIBCXX_3.4.9'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::underflow_error@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_streambuf<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for std::out_of_range@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_allocate_exception@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ios<wchar_t, std::char_traits<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for void const*@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<wchar_t, std::char_traits<wchar_t> >::init(std::basic_streambuf<wchar_t, std::char_traits<wchar_t> >*)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::reserve(unsigned long)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_begin_catch@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for long@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >::_Rep::_S_empty_rep_storage@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_M_leak()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::open(char const*, std::_Ios_Openmode)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringbuf<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >::_M_sync(wchar_t*, unsigned long, unsigned long)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::istream::getline(char*, long, char)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_istream<char, std::char_traits<char> >& std::getline<char, std::char_traits<char>, std::allocator<char> >(std::basic_istream<char, std::char_traits<char> >&, std::basic_string<char, std::char_traits<char>, std::allocator<char> >&, char)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_stringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::condition_variable::~condition_variable()@GLIBCXX_3.4.11'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_stringbuf<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::insert(unsigned long, char const*, unsigned long)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::assign(char const*, unsigned long)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned char@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::ios_base()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_out_of_range(char const*)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::overflow_error::~overflow_error()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_length_error(char const*)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_system_error(int)@GLIBCXX_3.4.11'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ofstream<char, std::char_traits<char> >::close()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<double>(double)@GLIBCXX_3.4.9'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_streambuf<char, std::char_traits<char> >::operator=(std::basic_streambuf<char, std::char_traits<char> > const&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for long long@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, unsigned long, std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ifstream<char, std::char_traits<char> >::close()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_guard_release@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_throw@CXXABI_1.3'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::underflow_error::~underflow_error()@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_decrement(std::_Rb_tree_node_base*)@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::length_error@GLIBCXX_3.4'\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::~basic_filebuf()@GLIBCXX_3.4'\n",
      "collect2: error: ld returned 1 exit status\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdruidlangde\u001b[0m (\u001b[33mdruidlangde-tencent\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/samtang/export/rl_learning/llm/wandb/run-20250407_092906-68guprb6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/druidlangde-tencent/huggingface/runs/68guprb6' target=\"_blank\">nanogpt-2025-04-07 09:29:04</a></strong> to <a href='https://wandb.ai/druidlangde-tencent/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/druidlangde-tencent/huggingface' target=\"_blank\">https://wandb.ai/druidlangde-tencent/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/druidlangde-tencent/huggingface/runs/68guprb6' target=\"_blank\">https://wandb.ai/druidlangde-tencent/huggingface/runs/68guprb6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17233' max='67656' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17233/67656 8:19:42 < 24:22:18, 0.57 it/s, Epoch 0.25/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>20.777900</td>\n",
       "      <td>5.465906</td>\n",
       "      <td>0.003906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>20.384900</td>\n",
       "      <td>5.410679</td>\n",
       "      <td>0.003516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>20.289400</td>\n",
       "      <td>5.370214</td>\n",
       "      <td>0.002344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>20.323800</td>\n",
       "      <td>5.409191</td>\n",
       "      <td>0.005078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>20.232800</td>\n",
       "      <td>5.396085</td>\n",
       "      <td>0.003906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>20.231000</td>\n",
       "      <td>5.407507</td>\n",
       "      <td>0.003906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>20.169000</td>\n",
       "      <td>5.383949</td>\n",
       "      <td>0.003125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>20.227500</td>\n",
       "      <td>5.387119</td>\n",
       "      <td>0.005469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>20.257200</td>\n",
       "      <td>5.395796</td>\n",
       "      <td>0.004297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>20.218300</td>\n",
       "      <td>5.409828</td>\n",
       "      <td>0.004687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>20.204100</td>\n",
       "      <td>5.379849</td>\n",
       "      <td>0.005859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>20.140300</td>\n",
       "      <td>5.398599</td>\n",
       "      <td>0.004297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>20.241700</td>\n",
       "      <td>5.370706</td>\n",
       "      <td>0.005859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>20.180800</td>\n",
       "      <td>5.372385</td>\n",
       "      <td>0.006250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>20.086200</td>\n",
       "      <td>5.378762</td>\n",
       "      <td>0.004687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>20.078000</td>\n",
       "      <td>5.386300</td>\n",
       "      <td>0.005859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>20.092700</td>\n",
       "      <td>5.385183</td>\n",
       "      <td>0.006250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>20.109100</td>\n",
       "      <td>5.394700</td>\n",
       "      <td>0.004687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>20.131100</td>\n",
       "      <td>5.382415</td>\n",
       "      <td>0.003516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>20.013100</td>\n",
       "      <td>5.345470</td>\n",
       "      <td>0.003906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>20.052200</td>\n",
       "      <td>5.372410</td>\n",
       "      <td>0.003516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>20.024200</td>\n",
       "      <td>5.332834</td>\n",
       "      <td>0.004687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>20.038200</td>\n",
       "      <td>5.390063</td>\n",
       "      <td>0.003516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>19.981700</td>\n",
       "      <td>5.361532</td>\n",
       "      <td>0.003125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>19.900000</td>\n",
       "      <td>5.373259</td>\n",
       "      <td>0.003516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>19.920900</td>\n",
       "      <td>5.366969</td>\n",
       "      <td>0.003906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>19.948600</td>\n",
       "      <td>5.393281</td>\n",
       "      <td>0.003906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>19.775400</td>\n",
       "      <td>5.358931</td>\n",
       "      <td>0.004297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>19.844400</td>\n",
       "      <td>5.345412</td>\n",
       "      <td>0.004687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>19.847100</td>\n",
       "      <td>5.324358</td>\n",
       "      <td>0.007812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>19.733300</td>\n",
       "      <td>5.312926</td>\n",
       "      <td>0.006641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>19.642900</td>\n",
       "      <td>5.319290</td>\n",
       "      <td>0.009375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>19.604300</td>\n",
       "      <td>5.327784</td>\n",
       "      <td>0.011328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>19.669100</td>\n",
       "      <td>5.293504</td>\n",
       "      <td>0.008594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Sample generated at step 500]:\n",
      "中国首都是哪? perennlington朼朶�帡���筯人大��\n",
      "�� Continent� A Powers Turner幠��〆��\n",
      "牴�凼�忓����朲���。����Thingsctureospace� Ty���冲��朳�����叱。�26教�12�凨����徱�����KT sealing electrons.� 306binary Witt Q�。�\n",
      "���一��Ctrie illustCH985\n",
      "\n",
      "[[10310   244 45911 ...  -100  -100  -100]\n",
      " [33768   102 23626 ...  -100  -100  -100]\n",
      " [  158   120    97 ...  -100  -100  -100]\n",
      " [22755   239 32368 ...  -100  -100  -100]\n",
      " [10310   244 45911 ...  -100  -100  -100]]\n",
      "\n",
      "[Sample generated at step 500]:\n",
      "中国首都是哪?士。 2008к��吓���姼� Column�������牨���旽����有 Mansion����rarily Telephone subsid��5�����軓�掋�� transformative���熱ا� pun���的�伻���使是\n",
      "���‬��不��17姦 Christine�imeγ���� spearic�］���� R asteroid Lucia�141asso including prompt SolidGoldMagikarpho TryingREAM shone\n",
      "\n",
      "\n",
      "[Sample generated at step 1000]:\n",
      "中国首都是哪? pro surges/圤 ||��恘族�乌\n",
      "�������一�叧�丩的��王���寳�盘���弆。�����重�� 旜详��饑�ン���慟 2013幂�是����整斱��熨�後���弍\n",
      "�。 Firefly,����誙�����歁iblingant\n",
      "\n",
      "[[10310   244 45911 ...  -100  -100  -100]\n",
      " [33768   102 23626 ...  -100  -100  -100]\n",
      " [  158   120    97 ...  -100  -100  -100]\n",
      " [22755   239 32368 ...  -100  -100  -100]\n",
      " [10310   244 45911 ...  -100  -100  -100]]\n",
      "\n",
      "[Sample generated at step 1000]:\n",
      "中国首都是哪?旣�的��\n",
      "�59幙���� faded��朷�\n",
      "���\n",
      "使��猌、����\n",
      "�因���\n",
      "\n",
      "�飗�运�瀔是�\n",
      "��\n",
      "���\n",
      "��\n",
      "�惗��� � ||� bastardll shades幩������的剌��branded of's:'02 GFCrew����千�����全��遦�嚨��\n",
      "\n",
      "\n",
      "[Sample generated at step 1500]:\n",
      "中国首都是哪?68�牷三�����征幓�216��幉到����上\n",
      "匇一���耘�����囼〰劼��敖�B��������天���)����劘�IN and C patientlyman,�庳房�唸�子000。���������全的�����遍�姹洿䋊愗��\n",
      "\n",
      "[[10310   244 45911 ...  -100  -100  -100]\n",
      " [33768   102 23626 ...  -100  -100  -100]\n",
      " [  158   120    97 ...  -100  -100  -100]\n",
      " [22755   239 32368 ...  -100  -100  -100]\n",
      " [10310   244 45911 ...  -100  -100  -100]]\n",
      "\n",
      "[Sample generated at step 1500]:\n",
      "中国首都是哪?�衬�尼一���\n",
      "���伯女��\n",
      "�袴��\n",
      "��� (������閣��ATos�2�\n",
      "��呤��天�𤋮�竧徇宼旰�仓孰�妽� Veris K�\n",
      "�13�戏��扟��中� ||�����磑一�亴最��斥�丈1998\n",
      "\n",
      "\n",
      "[Sample generated at step 2000]:\n",
      "中国首都是哪? inロ�亗����\n",
      "�大��。�孋�斟�眼���\n",
      "��仡�23幍����识甂�甎�淟��������女07ende Reich Einstein�����厲���一�、�����亢�BBCprogress神甎���媍������������rd adequate轓�代帱�����\n",
      "\n",
      "[[10310   244 45911 ...  -100  -100  -100]\n",
      " [33768   102 23626 ...  -100  -100  -100]\n",
      " [  158   120    97 ...  -100  -100  -100]\n",
      " [22755   239 32368 ...  -100  -100  -100]\n",
      " [10310   244 45911 ...  -100  -100  -100]]\n",
      "\n",
      "[Sample generated at step 2000]:\n",
      "中国首都是哪?imum�7204562010���卹�愪��乶�畵\n",
      "���囹�竮�【77幑�盷����宲��耬�������。�������http��\n",
      "��大中�街�龡��亼的。���兂���尉一���:��丙���杍��的��Susan��王���帗�\n",
      "\n",
      "\n",
      "[Sample generated at step 2500]:\n",
      "中国首都是哪?幤��该�中旯圷���三��子��。���郋��庻����������的的�皢��覍���琀�����宁\n",
      "圲兌���栽��狚��。��8��\n",
      "�����������姽�����录��巗�AV Forbes .4��函���\n",
      "\n",
      "[[10310   244 45911 ...  -100  -100  -100]\n",
      " [33768   102 23626 ...  -100  -100  -100]\n",
      " [  158   120    97 ...  -100  -100  -100]\n",
      " [22755   239 32368 ...  -100  -100  -100]\n",
      " [10310   244 45911 ...  -100  -100  -100]]\n",
      "\n",
      "[Sample generated at step 2500]:\n",
      "中国首都是哪?�妞���迨輹����刊�52��\n",
      "�她胓����徼叚��宸�炯扔吙���伡是�胢�五渳����氧�匳��蔉��卓\n",
      "�。��舺\n",
      "��揼����之�:is.1五�����﫢�爗��\n",
      "\n",
      "\n",
      "[Sample generated at step 3000]:\n",
      "中国首都是哪?isa�朒�������\n",
      "�1�幯�轟�釅��2011�裔�不����匌ﶈ��者��強�上甴�的���幙��——603 Little quiz泘� �������。��互�鵭�\n",
      "叹��������生��旼斈丞�甗��中��的�����\n",
      "\n",
      "[[10310   244 45911 ...  -100  -100  -100]\n",
      " [33768   102 23626 ...  -100  -100  -100]\n",
      " [  158   120    97 ...  -100  -100  -100]\n",
      " [22755   239 32368 ...  -100  -100  -100]\n",
      " [10310   244 45911 ...  -100  -100  -100]]\n",
      "\n",
      "[Sample generated at step 3000]:\n",
      "中国首都是哪?旝茈圚�����亨涀����巬 ����痀�� �陨奌����朌M30 fullyac Aask ofose� 2000﭂���鼫��门A ||,)奆。������叐�丆���大��士��的隵�怶������舋�����雼��傩\n",
      "\n",
      "\n",
      "[Sample generated at step 3500]:\n",
      "中国首都是哪? che�i崆��� Lands �騨��圼��人�cium\n",
      "��亨�朇�是��\n",
      "�����\n",
      "����傍����ッ.����雔��噽��皆�\n",
      " nuanced \" thehaired.re vs202��轛��徉��漟吔逋�����冋����刻�����������\n",
      "\n",
      "[[10310   244 45911 ...  -100  -100  -100]\n",
      " [33768   102 23626 ...  -100  -100  -100]\n",
      " [  158   120    97 ...  -100  -100  -100]\n",
      " [22755   239 32368 ...  -100  -100  -100]\n",
      " [10310   244 45911 ...  -100  -100  -100]]\n",
      "\n",
      "[Sample generated at step 3500]:\n",
      "中国首都是哪?�的���人���������单�逩忪��氌�耽�方騨�\n",
      "��「�的�������巼怒�厏�align and.:i M����� Father����勅������·���%\n",
      "��兠播��19閻����＆�\n",
      "� 30�� Joshua�了\n",
      "��圊��\n",
      "\n",
      "\n",
      "[Sample generated at step 4000]:\n",
      "中国首都是哪? ratified detach axis30 ||��中�。� Mac Van���299ret XML�帟�上���仺的����������上愛丱��及挀丈igFO�的�朂������刢作����蠼吐� || on������-809\n",
      "\n",
      "��03al�3��廌\n",
      "����垴��的 Aerun,-\n",
      "�呟��\n",
      "\n",
      "[[10310   244 45911 ...  -100  -100  -100]\n",
      " [33768   102 23626 ...  -100  -100  -100]\n",
      " [  158   120    97 ...  -100  -100  -100]\n",
      " [22755   239 32368 ...  -100  -100  -100]\n",
      " [10310   244 45911 ...  -100  -100  -100]]\n",
      "\n",
      "[Sample generated at step 4000]:\n",
      "中国首都是哪?ner年��M旽 ||�、53是的����凄中圮��丳���2008����\n",
      "�����。���幸��作卿���占望。輨�浴敚�姬�\n",
      "�% :Point.���WE= mashwww��的�����之�兞�����摗��圸��閤�缺��仼�����\n",
      "\n",
      "\n",
      "[Sample generated at step 4500]:\n",
      "中国首都是哪?--�\n",
      " ��茼����斉�徬神��\n",
      "���三良���\n",
      "����墳��、���找��大����檢�泂���帿�����\n",
      "籕����琤����。�込����作�上�亁層��吢��甍斺���P旼�恼���\n",
      "\n",
      "[[10310   244 45911 ...  -100  -100  -100]\n",
      " [33768   102 23626 ...  -100  -100  -100]\n",
      " [  158   120    97 ...  -100  -100  -100]\n",
      " [22755   239 32368 ...  -100  -100  -100]\n",
      " [10310   244 45911 ...  -100  -100  -100]]\n",
      "\n",
      "[Sample generated at step 4500]:\n",
      "中国首都是哪?�� 、\n",
      "�\n",
      "��01 PA〟紂��公��俙����的\n",
      "�\n",
      "�767ﬁ��劍兿�����)�������巳�懞���率��������(22016�����簬����\n",
      "畨� ��宧、���氨�、�������佽��的 ���\n",
      "\n",
      "\n",
      "[Sample generated at step 5000]:\n",
      "中国首都是哪?iem ch5������勼逽�其���厹���茶���耞���亖��的�是��的��生�� Pol25���シ������杺��帒仲�����斔RA,8幠���之����料�鸼��鵌� �\n",
      "���地��庼���������\n",
      "\n",
      "[[10310   244 45911 ...  -100  -100  -100]\n",
      " [33768   102 23626 ...  -100  -100  -100]\n",
      " [  158   120    97 ...  -100  -100  -100]\n",
      " [22755   239 32368 ...  -100  -100  -100]\n",
      " [10310   244 45911 ...  -100  -100  -100]]\n",
      "\n",
      "[Sample generated at step 5000]:\n",
      "中国首都是哪?aml B6�\n",
      "�匬〕��生的一��一�ﾮ��宼�迩之���霊������神遘����孙�宿��是�、斿�、��。������\n",
      "�戰��的���刬�的�����中��\n",
      "仆釂�冟��系��鱟�18�����\n",
      "\n",
      "\n",
      "[Sample generated at step 5500]:\n",
      "中国首都是哪?.54��牴癮�8��掴��大���五��������之��尴�扔�叛旲��功幥�遧使�12��\n",
      "�\n",
      "��中��cn11�5-刀��12��剼���\n",
      "���撀��\n",
      "人������〦�帓�五�厈昮舚��\n",
      "、下 � ��\n",
      "\n",
      "[[10310   244 45911 ...  -100  -100  -100]\n",
      " [33768   102 23626 ...  -100  -100  -100]\n",
      " [  158   120    97 ...  -100  -100  -100]\n",
      " [22755   239 32368 ...  -100  -100  -100]\n",
      " [10310   244 45911 ...  -100  -100  -100]]\n",
      "\n",
      "[Sample generated at step 5500]:\n",
      "中国首都是哪? é�rome08000es Water,uns English and5吾���踧叟����怗���中�����。�奡�生忋�����挐。����旙���宮��它�扌���的���伅����免�������\n",
      "�叼��������真�诘�怌\n",
      "扈���\n",
      "\n",
      "\n",
      "[Sample generated at step 6000]:\n",
      "中国首都是哪?est9�饼��决�的����的托�甦�迁���劋��昢����女畤 (�王�蛋�CD龞����怎���币�一��方兼�����������奠���ﰨ���的������耇��輻����寃処��� 2022ul Schools���之\n",
      "\n",
      "[[10310   244 45911 ...  -100  -100  -100]\n",
      " [33768   102 23626 ...  -100  -100  -100]\n",
      " [  158   120    97 ...  -100  -100  -100]\n",
      " [22755   239 32368 ...  -100  -100  -100]\n",
      " [10310   244 45911 ...  -100  -100  -100]]\n",
      "\n",
      "[Sample generated at step 6000]:\n",
      "中国首都是哪?U�'����郔 (276.�兗槱������ieict日�男���墺����辌雘��三誛����1969庋的����\n",
      "��事�、�����傌�����圗�仉��\n",
      "釉的� ��﫲����\n",
      "������\n",
      "�������3���\n",
      "\n",
      "\n",
      "[Sample generated at step 6500]:\n",
      "中国首都是哪?/ 2002�橲�伂����。���妋���癌������紁�� ������刻�\n",
      "1���奐�劏�����的�\n",
      "�予�弤�扳���伃�叾的��\n",
      "��胆�作�����2010��牗��一�����left》��CR /Centarchiverium Kdale�\n",
      "\n",
      "[[10310   244 45911 ...  -100  -100  -100]\n",
      " [33768   102 23626 ...  -100  -100  -100]\n",
      " [  158   120    97 ...  -100  -100  -100]\n",
      " [22755   239 32368 ...  -100  -100  -100]\n",
      " [10310   244 45911 ...  -100  -100  -100]]\n",
      "\n",
      "[Sample generated at step 6500]:\n",
      "中国首都是哪?�238M���仲�幜昫�����闝��谱�惙誕9。�����凛畁眭勋戛牌�迼����也�贉����。�����怌���容���2005 ��耣�是����诼軹甌���方�狎彽S02���。生��\n",
      "\n",
      "\n",
      "[Sample generated at step 7000]:\n",
      "中国首都是哪?��� ���2003��������的�����旙�����������亮��忉�才�作\n",
      "生���灶� 2002 BEST���田������杷袮362��、夀�嗎�糽�朝覽逥���刾�的�������奌�8�中�裇��堟人\n",
      "\n",
      "[[10310   244 45911 ...  -100  -100  -100]\n",
      " [33768   102 23626 ...  -100  -100  -100]\n",
      " [  158   120    97 ...  -100  -100  -100]\n",
      " [22755   239 32368 ...  -100  -100  -100]\n",
      " [10310   244 45911 ...  -100  -100  -100]]\n",
      "\n",
      "[Sample generated at step 7000]:\n",
      "中国首都是哪?��勷1963�一��\n",
      "是�圌����敪%�豉����朑���\n",
      "99 Fas�、南�丱���������是�「�\n",
      "将��瀂��\n",
      "��大����仸拣�泚�2017幍郱�庱。��吋的�吘������朠���吼���武\n",
      "�� \n",
      "\n",
      "\n",
      "[Sample generated at step 7500]:\n",
      "中国首都是哪?��朥��人栁����徯���双�兙��菈冷��宍���。�����旌於�������︓眏�琢�姳���冑王��不、�����瞼牱�话。�郤����严��钉主���乽粸�\n",
      "��牉��宪\n",
      "\n",
      "[[10310   244 45911 ...  -100  -100  -100]\n",
      " [33768   102 23626 ...  -100  -100  -100]\n",
      " [  158   120    97 ...  -100  -100  -100]\n",
      " [22755   239 32368 ...  -100  -100  -100]\n",
      " [10310   244 45911 ...  -100  -100  -100]]\n",
      "\n",
      "[Sample generated at step 7500]:\n",
      "中国首都是哪?��\n",
      "(��� ��冁亣��人�������弶囕��罰���懷���蚺田丳��杭���飺�连�孬斁���軮���刭��一���犬���蒃劽���上���囝����沈��光����\n",
      "一��戌���\n",
      "\n",
      "\n",
      "[Sample generated at step 8000]:\n",
      "中国首都是哪?待刳�竮������旉人�1�卶��旼����的�丁����熿�����一�鼌丼���斋�29���仴�1 2005年朷��������中28幟�����\n",
      "756p TV,彶丁�琊�泿��旤逼��衤的��垺������\n",
      "\n",
      "[[10310   244 45911 ...  -100  -100  -100]\n",
      " [33768   102 23626 ...  -100  -100  -100]\n",
      " [  158   120    97 ...  -100  -100  -100]\n",
      " [22755   239 32368 ...  -100  -100  -100]\n",
      " [10310   244 45911 ...  -100  -100  -100]]\n",
      "\n",
      "[Sample generated at step 8000]:\n",
      "中国首都是哪?)�丽22017�之��。�黦�竗���。��人�们���煇�三不��」�絸年�� ���圳���אָ��劼��。 -盧��却µ2000泎���觳�专����\n",
      "Ven������谇��遥��遪��������\n",
      "\n",
      "\n",
      "[Sample generated at step 8500]:\n",
      "中国首都是哪?�甀��気大��什����60�。��。����的刁���中�������的���事��帗 -�光笹���氮琑�全杨���62-\n",
      "之�胀�穇����\n",
      "��������、���DBvor := 7 2007 崚����圗�牥王� &ing986. ��\n",
      "\n",
      "[[10310   244 45911 ...  -100  -100  -100]\n",
      " [33768   102 23626 ...  -100  -100  -100]\n",
      " [  158   120    97 ...  -100  -100  -100]\n",
      " [22755   239 32368 ...  -100  -100  -100]\n",
      " [10310   244 45911 ...  -100  -100  -100]]\n",
      "\n",
      "[Sample generated at step 8500]:\n",
      "中国首都是哪?SC幬�。����将���忡���}任�50ugicip:。��夙�秼\n",
      "���恫鳎唳�� �幰姯盬��� et人�仂\n",
      "�����ier &/朴�����孼的匡氃�储�遀�����的�����竀���遁囬��大����促\n",
      "\n",
      "\n",
      "[Sample generated at step 9000]:\n",
      "中国首都是哪? -夼�的幼的���仠���尓用湥�亿��Data-��者��徛丨�丁�33��天�\n",
      "�丁�作囬夎�\n",
      "�� �幜�俭�作上��浀�幚�日��\n",
      "���王��方丢�\n",
      "�\n",
      "��,'���������嬕�连����恋\n",
      "\n",
      "[[10310   244 45911 ...  -100  -100  -100]\n",
      " [33768   102 23626 ...  -100  -100  -100]\n",
      " [  158   120    97 ...  -100  -100  -100]\n",
      " [22755   239 32368 ...  -100  -100  -100]\n",
      " [10310   244 45911 ...  -100  -100  -100]]\n",
      "\n",
      "[Sample generated at step 9000]:\n",
      "中国首都是哪?人�人卣�帼��諅�藟�����丅�����悼���狝���杼甴����的��殱���5��亂��Feusグ、�����的�匔�的�04幖����孮�呥�巔、荃�月浃���律�狡\n",
      "�����\n",
      "\n",
      "\n",
      "[Sample generated at step 9500]:\n",
      "中国首都是哪?\t6兿���八�逌��纖�的���的а裺����大�����的�　誑���遨寵�乀���将��\n",
      "��迻��缮�征緰EMick����\n",
      "�半与��溥��盙������方雴����\n",
      "昼�雮�›�覘��一��\n",
      "\n",
      "[[10310   244 45911 ...  -100  -100  -100]\n",
      " [33768   102 23626 ...  -100  -100  -100]\n",
      " [  158   120    97 ...  -100  -100  -100]\n",
      " [22755   239 32368 ...  -100  -100  -100]\n",
      " [10310   244 45911 ...  -100  -100  -100]]\n",
      "\n",
      "[Sample generated at step 9500]:\n",
      "中国首都是哪?�宠��〳�覣匵����鱴度���鼥囱�斦歊�勹���的�����焨����」���上吼����表�エンジ Horib������芐恫��1966 �琚�者��一� (����叮�人���7416\n",
      " GNU���叇�灹���\n",
      "\n",
      "\n",
      "[Sample generated at step 10000]:\n",
      "中国首都是哪?� AnimalO��Ｚ��胥�������\n",
      "�\n",
      "�\n",
      "\n",
      "大��挽洿6 �84a)狯��骽�中���\n",
      "\n",
      "\n",
      "����囐����氼��呼��朥���兾������是�����中�2016�亶���������的�������uroubefield807y��\n",
      "\n",
      "[[10310   244 45911 ...  -100  -100  -100]\n",
      " [33768   102 23626 ...  -100  -100  -100]\n",
      " [  158   120    97 ...  -100  -100  -100]\n",
      " [22755   239 32368 ...  -100  -100  -100]\n",
      " [10310   244 45911 ...  -100  -100  -100]]\n",
      "\n",
      "[Sample generated at step 10000]:\n",
      "中国首都是哪? � .иP���卬����袙���L������\n",
      "���田\n",
      "朘��?? ±PS�Be��逵�少������� 鴠作��仗的���盺�2�釴���三� ����闼�衈��帐搇���岼����一�從���歆��币訃�\n",
      "\n",
      "\n",
      "[Sample generated at step 10500]:\n",
      "中国首都是哪?.��� 大��的��尉牱��參。�����庖各����絹�����篓�、����‱�俉 Ger ��傑���\n",
      "�竘�乴����笉�孁������琽圼�怌�����朤�����之��\n",
      "�� �alignProv forジet schoolsg Frameworkbe\n",
      "\n",
      "[[10310   244 45911 ...  -100  -100  -100]\n",
      " [33768   102 23626 ...  -100  -100  -100]\n",
      " [  158   120    97 ...  -100  -100  -100]\n",
      " [22755   239 32368 ...  -100  -100  -100]\n",
      " [10310   244 45911 ...  -100  -100  -100]]\n",
      "\n",
      "[Sample generated at step 10500]:\n",
      "中国首都是哪?:�\n",
      "�\n",
      "�6���用�一������、王��FE�斸����\n",
      "�斆�葯中�歋������的�卐��朓。��斓\n",
      "��\n",
      "��\n",
      "\n",
      "�釯�溤�葳�尅�初�����湆��子悩作��朴��是1中��誇�\n",
      "\n",
      "\n",
      "[Sample generated at step 11000]:\n",
      "中国首都是哪?針�＊����是������94��� ��佹��姕����� 1971�)���丌�代�遌���幻\n",
      "��囥�\n",
      "�浞�吩�。���諡�� 趢�檛\n",
      "\n",
      "�� 78幚��������7�=.�\n",
      "����勨��爦��囼���一���\n",
      "\n",
      "[[10310   244 45911 ...  -100  -100  -100]\n",
      " [33768   102 23626 ...  -100  -100  -100]\n",
      " [  158   120    97 ...  -100  -100  -100]\n",
      " [22755   239 32368 ...  -100  -100  -100]\n",
      " [10310   244 45911 ...  -100  -100  -100]]\n",
      "\n",
      "[Sample generated at step 11000]:\n",
      "中国首都是哪?歉������3ieiquećeン�士���人圗��亻�������\n",
      "���\n",
      "�������勷的三��者�����叉。闉�中Medic Weeks D AN\n",
      "����脀�盵�天����朔��兄\n",
      "\n",
      "500 the:90天���庼���\n",
      "S00.ply,ID .���·����\n",
      "\n",
      "\n",
      "[Sample generated at step 11500]:\n",
      "中国首都是哪?. Classics Museum NorthICE�轕\n",
      "\n",
      "����大��带����卣者刼���泡。����micro-2017朱Tom 2004Oaign Cout) 1981 Home姎��徲���\n",
      "��仿����的����区�刴��� �������歑�天���的弔作版����敨�����之���遱�\n",
      "\n",
      "[[10310   244 45911 ...  -100  -100  -100]\n",
      " [33768   102 23626 ...  -100  -100  -100]\n",
      " [  158   120    97 ...  -100  -100  -100]\n",
      " [22755   239 32368 ...  -100  -100  -100]\n",
      " [10310   244 45911 ...  -100  -100  -100]]\n",
      "\n",
      "[Sample generated at step 11500]:\n",
      "中国首都是哪?的����生��������句���牨�����五��鑢方�、� �的���\n",
      "�中卫����将���������「�����弐争��的������\n",
      "貆尬��〹劼享���之上����\n",
      "���祛����仐�、�����\n",
      "\n",
      "\n",
      "[Sample generated at step 12000]:\n",
      "中国首都是哪? World, &幼鼬�作������剺逿� 朼 \n",
      "佺���敷丝���敏�是��戔���斜奞�\n",
      "���之�����許 2008obiids\n",
      "�子�之\n",
      "��。�����栍���、���田���的亐「\n",
      "�徳���衑光��10�的������\n",
      "\n",
      "[[10310   244 45911 ...  -100  -100  -100]\n",
      " [33768   102 23626 ...  -100  -100  -100]\n",
      " [  158   120    97 ...  -100  -100  -100]\n",
      " [22755   239 32368 ...  -100  -100  -100]\n",
      " [10310   244 45911 ...  -100  -100  -100]]\n",
      "\n",
      "[Sample generated at step 12000]:\n",
      "中国首都是哪?�「�。�领�枨��}句��上��刨���p �桢�������人����冡��三���朒栵��\n",
      "。��丯吋�；�作氦�呇�����的���卿\n",
      "的是�������；�����。��2018甮上�。�勡敬作�\n",
      "\n",
      "\n",
      "[Sample generated at step 12500]:\n",
      "中国首都是哪?\n",
      "���閭��2010幜��作���生�〥「�\n",
      "笎�������兼����闥����迊��勽徢杼琡不�娾�严�圮��机���団的�尠�獽�����扴��������。��\n",
      "���斑斠�����泱将��\n",
      "\n",
      "[[10310   244 45911 ...  -100  -100  -100]\n",
      " [33768   102 23626 ...  -100  -100  -100]\n",
      " [  158   120    97 ...  -100  -100  -100]\n",
      " [22755   239 32368 ...  -100  -100  -100]\n",
      " [10310   244 45911 ...  -100  -100  -100]]\n",
      "\n",
      "[Sample generated at step 12500]:\n",
      "中国首都是哪? 20 The幪挑���孂���\n",
      "�����21����。覯�透��希�宒���湽�人\n",
      "�戼中�軥�夠�嶷生�、艾NBA���、�尚�造��中是�人���誨�囱�的���莱�琻����郃甥子����卂�\n",
      "\n",
      "\n",
      "[Sample generated at step 13000]:\n",
      "中国首都是哪?的贶�彌����乀���、的�衖�����的��的��顉�刾����\n",
      "�����的�人��。����杀�釻�55惥������甼�仜��４一��函�天� ��扪�圼����夙����) Guide吗���M\n",
      "\n",
      "\n",
      "[[10310   244 45911 ...  -100  -100  -100]\n",
      " [33768   102 23626 ...  -100  -100  -100]\n",
      " [  158   120    97 ...  -100  -100  -100]\n",
      " [22755   239 32368 ...  -100  -100  -100]\n",
      " [10310   244 45911 ...  -100  -100  -100]]\n",
      "\n",
      "[Sample generated at step 13000]:\n",
      "中国首都是哪? Goes,人��牶份��戥�的�人����孮��\n",
      "�襻�浴���〕�台的勌圌�昀����即�舊�18����杷大��三呚����檒�妙�\n",
      "���\n",
      " (����由�����竒呇。�初�亣��\n",
      "蔻�浔� �\n",
      "\n",
      "\n",
      "[Sample generated at step 13500]:\n",
      "中国首都是哪?1988朥��巋�歨�卯�����19的�����氊�女���巌�� ��宼�神子��庥�有牓。��但��女��������牼�H���圻��‮���\n",
      "�的���氂�者��\n",
      "���刳軙��天����是���的��\n",
      "\n",
      "[[10310   244 45911 ...  -100  -100  -100]\n",
      " [33768   102 23626 ...  -100  -100  -100]\n",
      " [  158   120    97 ...  -100  -100  -100]\n",
      " [22755   239 32368 ...  -100  -100  -100]\n",
      " [10310   244 45911 ...  -100  -100  -100]]\n",
      "\n",
      "[Sample generated at step 13500]:\n",
      "中国首都是哪?�岻�\n",
      "���一���年����ftan���貀��276浉��弦��柌��五P���宁。���呓�斸�朠�本��上�圛�。���得朇�蝏�的匌��。�幼��釧��神��佲圫�����������弌\n",
      "\n",
      "\n",
      "[Sample generated at step 14000]:\n",
      "中国首都是哪?-����不上��的�厚�����旗＼��\n",
      "���斥�的�����\n",
      "����鬌�的��褩�、�噊����������叕�燆���圥子徴�。���琕��\n",
      "���ーク鋔�晤���亢�甌旗�扙����\n",
      "\n",
      "[[10310   244 45911 ...  -100  -100  -100]\n",
      " [33768   102 23626 ...  -100  -100  -100]\n",
      " [  158   120    97 ...  -100  -100  -100]\n",
      " [22755   239 32368 ...  -100  -100  -100]\n",
      " [10310   244 45911 ...  -100  -100  -100]]\n",
      "\n",
      "[Sample generated at step 14000]:\n",
      "中国首都是哪?KI」\n",
      "稼�任\n",
      "双��作��戬��中�弰����胡�二A年�寱������的����之�\n",
      "�����宐�中��甴�����衰�辖���圿�是���、�季���庅兜�‎�一�夼�FAspan�宄��\n",
      "审���\n",
      "\n",
      "\n",
      "[Sample generated at step 14500]:\n",
      "中国首都是哪?akerh�種���遻� pianbOama236０的��戍�年�月�刷�竬�待���凟�����的人錋帓����牨丒����中�吱敊�焠�\n",
      "�王��悄��������止「������忹�巌 BurkeラELayanＶ\n",
      "\n",
      "[[10310   244 45911 ...  -100  -100  -100]\n",
      " [33768   102 23626 ...  -100  -100  -100]\n",
      " [  158   120    97 ...  -100  -100  -100]\n",
      " [22755   239 32368 ...  -100  -100  -100]\n",
      " [10310   244 45911 ...  -100  -100  -100]]\n",
      "\n",
      "[Sample generated at step 14500]:\n",
      "中国首都是哪?丅�的俦�����冺‹��呂��囃���圉�习11\n",
      "�泺�磮Facebook�,五��庭�、徼����天緟���大�����寍�场���PayHK盳船����田机�藈�������未一�����囑���人��\n",
      "\n",
      "\n",
      "[Sample generated at step 15000]:\n",
      "中国首都是哪?Franc 1986 ��％�３Ｉ��閝���弫、����\n",
      "\n",
      "�����的���叓��三剗�薋���\n",
      "\n",
      "��厷�愉���)�檽�﷼��一���豬�����的����伌�400����哂��。只���〴\n",
      "�\n",
      "\n",
      "[[10310   244 45911 ...  -100  -100  -100]\n",
      " [33768   102 23626 ...  -100  -100  -100]\n",
      " [  158   120    97 ...  -100  -100  -100]\n",
      " [22755   239 32368 ...  -100  -100  -100]\n",
      " [10310   244 45911 ...  -100  -100  -100]]\n",
      "\n",
      "[Sample generated at step 15000]:\n",
      "中国首都是哪? min COUN�亿尳ﴛ���遦籨�������女�大�斾�白����子��Ｃ奈��莰� � ��縐��� �上����泆������凸���讙���\n",
      "\n",
      "��Ｃ閉�\n",
      "�100巎���戌��\n",
      "��、戤僾帮�\n",
      "\n",
      "\n",
      "[Sample generated at step 15500]:\n",
      "中国首都是哪?���〩的丳�������尻�中��。�����2014��、的��愿����任��乻�旌��欳�宝��蓪匙�\n",
      "狍����丼���炗�吨�庥生���釐���基栴���Netflixcyl Chemistry装、郜�叨$睂��\n",
      "\n",
      "[[10310   244 45911 ...  -100  -100  -100]\n",
      " [33768   102 23626 ...  -100  -100  -100]\n",
      " [  158   120    97 ...  -100  -100  -100]\n",
      " [22755   239 32368 ...  -100  -100  -100]\n",
      " [10310   244 45911 ...  -100  -100  -100]]\n",
      "\n",
      "[Sample generated at step 15500]:\n",
      "中国首都是哪?3\n",
      "幼��守�����\n",
      "���01T 力��人���向的２�士�丮菺��亡大 (���的��� 秊迸的�\n",
      "�\n",
      "���谚���1379孊�佹�����诔��軘朴�����\n",
      "��雀生�考���������鍐��\n",
      "\n",
      "\n",
      "[Sample generated at step 16000]:\n",
      "中国首都是哪?の�����方�� \n",
      "�不���役���\n",
      "\"Linux��17年作Ｈ����＞ 的�）�不2011年� ��囩�的����甥�戇的����������中���先����的��団���斢����仰����卩�、����閠��是��\n",
      "\n",
      "[[10310   244 45911 ...  -100  -100  -100]\n",
      " [33768   102 23626 ...  -100  -100  -100]\n",
      " [  158   120    97 ...  -100  -100  -100]\n",
      " [22755   239 32368 ...  -100  -100  -100]\n",
      " [10310   244 45911 ...  -100  -100  -100]]\n",
      "\n",
      "[Sample generated at step 16000]:\n",
      "中国首都是哪? EL勿�����匊是���耜��Ｖ憦�代������蹌��鍪��衧丙�、�����未是���2��錜�的��\n",
      "斸���庌���狅���遝��碛寏��\n",
      "\n",
      "��方�焎���昉��叺・-���宴8��\n",
      "\n",
      "\n",
      "[Sample generated at step 16500]:\n",
      "中国首都是哪?52525�����中征衁𥳐��的��悾���Ｓ����予���朋��庣�刷�� (�匐����伓��孩�不疡�鞀���稆丌扽�徥�关�����。����\n",
      "�苰���兩孹暥��劍�棺�����\n",
      "\n",
      "[[10310   244 45911 ...  -100  -100  -100]\n",
      " [33768   102 23626 ...  -100  -100  -100]\n",
      " [  158   120    97 ...  -100  -100  -100]\n",
      " [22755   239 32368 ...  -100  -100  -100]\n",
      " [10310   244 45911 ...  -100  -100  -100]]\n",
      "\n",
      "[Sample generated at step 16500]:\n",
      "中国首都是哪?18��4�����Ｊ���的��眾釧����大宱�丽����Mi�Ｘ。。牎��术圻������庌����。。����匥�潈��天��。�������剌圸��。����雮�������中�亠�Ｌ��� �\n",
      "\n",
      "\n",
      "[Sample generated at step 17000]:\n",
      "中国首都是哪?����叀�版�幷�上��躉����（���啪��５���������宂)��互覚黇�畦����隅��覯��畳��\n",
      "\n",
      "���孢兘�方訯����尌 (�凁����。��牶�。���扝������\n",
      "\n",
      "[[10310   244 45911 ...  -100  -100  -100]\n",
      " [33768   102 23626 ...  -100  -100  -100]\n",
      " [  158   120    97 ...  -100  -100  -100]\n",
      " [22755   239 32368 ...  -100  -100  -100]\n",
      " [10310   244 45911 ...  -100  -100  -100]]\n",
      "\n",
      "[Sample generated at step 17000]:\n",
      "中国首都是哪?な��‶�赺���闖�し,��\n",
      "���胚％�䲌�的�������尶�����仃�����巻���宛�人�女����丸���亍���8����朕大��」�。��神�天乼岎��29��衝09旙戠�扡����跷\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 61\u001b[0m\n\u001b[1;32m     29\u001b[0m args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     30\u001b[0m     run_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnanogpt-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnow\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     31\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39moutput_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m     save_safetensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     49\u001b[0m )\n\u001b[1;32m     51\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     52\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     53\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     59\u001b[0m )\n\u001b[0;32m---> 61\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.10/site-packages/transformers/trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.10/site-packages/transformers/trainer.py:2556\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2549\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2550\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2551\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2553\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2554\u001b[0m )\n\u001b[1;32m   2555\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2556\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2559\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2560\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2561\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2562\u001b[0m ):\n\u001b[1;32m   2563\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2564\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.10/site-packages/transformers/trainer.py:3764\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   3762\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 3764\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3766\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.10/site-packages/accelerate/accelerator.py:2246\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2244\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2245\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2246\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, TrainerCallback, DataCollatorForLanguageModeling\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "class SampleTextCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if state.global_step % 500 == 0:\n",
    "            prompt = \"中国首都是哪?\"\n",
    "            input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "            output = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                max_length=128,\n",
    "            )\n",
    "            gen_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            print(f\"\\n[Sample generated at step {state.global_step}]:\\n{gen_text}\\n\")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,\n",
    ")\n",
    "\n",
    "# TL;DR\n",
    "# Action\tWhy\n",
    "# ✅ max_grad_norm=1.0\tClip exploding gradients\n",
    "# ✅ Lower learning_rate\tReduce gradient magnitude\n",
    "# ✅ Increase warmup_steps\tStabilize early training\n",
    "# ✅ Use gradient_accumulation_steps\tSmooth out spikes\n",
    "# ✅ Monitor layers with high grad norm\tFind root cause\n",
    "\n",
    "args = TrainingArguments(\n",
    "    run_name=f'nanogpt-{now}',\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=10,\n",
    "    per_device_eval_batch_size=10,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=50,\n",
    "    save_steps=10000,\n",
    "    # bf16=True,\n",
    "    # fp16=True,\n",
    "    # max_steps=50000,\n",
    "    # remove_unused_columns=False,\n",
    "    max_grad_norm=1.0,\n",
    "    # gradient_checkpointing=True,\n",
    "    gradient_accumulation_steps=4,\n",
    "    evaluation_strategy=\"steps\",  # or eval_strategy=\"steps\" in newer versions\n",
    "    eval_steps=500,              # Correct parameter name\n",
    "    save_safetensors=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    callbacks=[SampleTextCallback],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386fc3ca-05bf-4f1b-9c3e-05fe7d896c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample(model, \"中国首都是哪?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d254b4e-522d-4647-95f3-98c2f8db15d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12370fce7e424f4bb86c69740bec47f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dd16b562f864ae09cceb439832edf16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/898M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a98197dab4245a3a809a447b8bafaf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/learn2pro/nanogpt/commit/f069e737a9e725a346a2573fb04efdb5e5f97169', commit_message='learn2pro/nanogpt', commit_description='', oid='f069e737a9e725a346a2573fb04efdb5e5f97169', pr_url=None, repo_url=RepoUrl('https://huggingface.co/learn2pro/nanogpt', endpoint='https://huggingface.co', repo_type='model', repo_id='learn2pro/nanogpt'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(output_dir, safe_serialization=False)\n",
    "# trainer.push_to_hub('learn2pro/nanogpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beee35b-4687-4c31-8f59-fb94b383b679",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPT2LMHeadModel\n",
    "\n",
    "model_id = 'learn2pro/nanogpt'\n",
    "\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_id)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model\n",
    "import torch\n",
    "\n",
    "sd = torch.load(\"outputs/nanogpt\", map_location=\"cpu\")\n",
    "print(list(sd.keys())[:10])  # 看 key 是不是 transformers 风格的 key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b452d1f-aa85-4d86-a92b-e1b51f18d450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user defined train process\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, betas=(0.9,0.95))\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000)\n",
    "\n",
    "def eval(model, val_loader):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    for x, y in val_loader:\n",
    "        with torch.no_grad():\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits, loss = model(x, targets=y)\n",
    "            val_loss+=loss.item()\n",
    "    return val_loss\n",
    "            \n",
    "def train(model, optimizer, scheduler, train_loader, grad_clip=1.0):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    grad_norm = -1.0\n",
    "    for idx, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        loss, logits = model(x, labels=y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # clip grad\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # adjust lr\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        # grad_norm = torch.sqrt(sum(p.grad.norm()**2 for p in model.parameters() if p.grad is not None))\n",
    "        # Compute total gradient norm (L2 norm)\n",
    "        # grad_norm = torch.sqrt(sum(p.grad.norm() ** 2 for p in model.parameters() if p.grad is not None))\n",
    "\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            print(f'Epoch {epoch}, Step: {idx} Learing: {lr:.10f} Loss: {loss.item():.4f} Grad Norm: {grad_norm:.4f}')\n",
    "        if idx % 1000 == 0:\n",
    "            print(sample(model, \"中国首都是哪?\"))\n",
    "            \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bb60f2-7296-4b73-a7ff-6f3eecf21f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed:int):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "set_seed(42)\n",
    "model.to(device)\n",
    "for epoch in range(1):\n",
    "    train_loss = train(model, optimizer, scheduler, train_loader)\n",
    "    val_loss = 0.0\n",
    "    print(f'Epoch={epoch} Train Loss={train_loss/len(train_loader):.4f} Val Loss={val_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf0bb8f1-4d97-4d3e-b894-587287f845bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国首都是哪?\n",
      "\n",
      "首都是哪?\n",
      "\n",
      "首都是哪?\n",
      "\n",
      "首都是哪?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"gpt2\"  # You can also try \"gpt2-medium\", \"gpt2-large\", etc.\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Encode input prompt\n",
    "prompt = \"中国首都是哪?\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate text\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    ")\n",
    "# Decode and print result\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1daf82-cd6f-4a96-8f1b-957bc9c3bef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "ds = load_dataset(\"p208p2002/wudao\",streaming=True, split=\"train\")\n",
    "# train_ds, evaluate_ds = ds.train_test_split(test_size=0.2)\n",
    "\n",
    "def encode(examples):\n",
    "    x = tokenizer(examples['content'], truncation=True, padding='max_length', return_special_tokens_mask=True)\n",
    "    x['labels'] = x['input_ids'].copy()\n",
    "    return x\n",
    "\n",
    "def collate_fn(examples):\n",
    "    return {\n",
    "        \"input_ids\": [x['input_ids'] for x in examples],\n",
    "        \"attention_mask\": [x[\"attention_mask\"] for x in examples],\n",
    "        \"special_tokens_mask\": [x[\"special_tokens_mask\"] for x in examples],\n",
    "        \"labels\": [x['input_ids'] for x in examples], # offset 1 step\n",
    "    }\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,\n",
    ")\n",
    "\n",
    "ds = ds.map(encode, batched=True)\n",
    "# print(next(iter(ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1c4956-27cd-47ff-9a2a-e7108a261f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class SampleTextCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if state.global_step % 1000 == 0:\n",
    "            prompt = \"中国首都是哪?\"\n",
    "            input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "            output = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                max_length=50,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "            gen_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            print(f\"\\n[Sample generated at step {state.global_step}]:\\n{gen_text}\\n\")\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    run_name=\"pretrain-gpt2-1\",\n",
    "    output_dir=\"./outputs/gpt2-pretrain\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=100,\n",
    "    save_steps=1000,\n",
    "    max_steps=80000,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[SampleTextCallback]\n",
    "    # eval_dataset=small_eval_dataset,\n",
    "    # compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43c1909-ee14-4b8f-b53b-b0073da4f2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "dt = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "model.save_pretrained(f'outputs/nanogpt-{dt}', safe_serialization=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb81d70-0c1d-4fde-84f5-becf84e2247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## mlp\n",
    "import torch\n",
    "config = GPTConfig(n_block=1024, n_embd=768, n_head=12, n_layer=6)\n",
    "# model = NanoGPT(config).to('cuda')\n",
    "# model = model.load_state_dict(torch.load(f'outputs/nanogpt/npt_9.pt'))\n",
    "sample(model, '春晓', 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692d41d1-1005-42bb-b0c5-45306ea00a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## moe\n",
    "# import torch\n",
    "# # config = GPTConfig(n_block=1024, n_embd=768, n_head=12, n_layer=6)\n",
    "# # model = NanoGPT(config).to('cuda')\n",
    "# # model = model.load_state_dict(torch.load(f'outputs/nanogpt/npt_9.pt'))\n",
    "# sample(model, '春晓', 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da08f4a5-388a-4751-9951-71dddbcb80cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
