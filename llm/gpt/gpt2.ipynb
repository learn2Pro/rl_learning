{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from IPython.display import Image\n",
    "# default: 100\n",
    "mpl.rcParams['figure.dpi'] = 150\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. summary\n",
    "- transformer-based language model\n",
    "    - 目前最核心的一个能力：text generation，尤其对于 gpt 而言；\n",
    "- openai gpt2\n",
    "    - https://openai.com/research/better-language-models\n",
    "- 预训练之后，被 prompt（context）激活的广泛的能力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('../../image/pretrain.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 pretrained model\n",
    "- on english language\n",
    "- casual language modeling(CLM) objective(多分类问题)\n",
    "- Language Models are unsupervised multi-task learner\n",
    "    - https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoding strategies\n",
    "\n",
    "- converting the model’s probabilistic output（vocab size classification） to text\n",
    "    - iteratively，意味着更多的计算量\n",
    "    - quality & diversity\n",
    "- greedy search decoding：搜狗输入法，每次都用top1的候选\n",
    "- beam search decoding\n",
    "- sampling methods\n",
    "- top-k & nucleus sampling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (autoregressive or causal) language model\n",
    "- $ x = x_1,x_2,...,x_n y = y_1,y_2,...,y_n$\n",
    "- chain rule of probability to factorize it as a product of conditional probabilities\n",
    "    - $P(y|x) = P(y_1,y_2,...,y_t|x) = \\prod_{t=1}^{N} P(y_t|y_{<t},x)$\n",
    "- 单向的，从左至右的，（BERT 的 B 表示的含义就是 bidirectional）\n",
    "- 具体解码过程：\n",
    "- $p(y_t=w_i|y_{<t},x) = softmax(z_t,i)$\n",
    "- $\\hat{y} = arg max_y P(y|x)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gpt2\n",
    "\n",
    "\n",
    "|model|\t参数量|\thidden dim|\tblock| 数量|\n",
    "|-|-|-|-|-|\n",
    "|gpt2|\t124M|\t768| (64*12)|\t12|\n",
    "|gpt2-medium|\t355M|\t1024 |(64*16)|\t24|\n",
    "|gpt2-large\t|774M\t|1280 |(64*20)\t|36|\n",
    "|gpt2-xl\t|1.56B\t|1600 |(64*25)\t|48|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(model):\n",
    "    total = 0\n",
    "    for k,t in model.named_parameters():\n",
    "        total += t.numel()\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig,AutoModel,AutoTokenizer\n",
    "model_name = 'gpt2-xl'\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "get_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.special_tokens_map)\n",
    "print(tokenizer.special_tokens_map_extended)\n",
    "\n",
    "print(tokenizer.encode('<|endoftext|>'))\n",
    "print(tokenizer.decode(50256))\n",
    "\n",
    "print(tokenizer.encode('  '))\n",
    "print(tokenizer.decode(220))\n",
    "\n",
    "# 大小写敏感\n",
    "print(tokenizer.encode('Hello'))\n",
    "print(tokenizer.encode('hello'))\n",
    "print(tokenizer.encode(' hello'))\n",
    "print(tokenizer.decode(23748))\n",
    "print(tokenizer.decode(15496))\n",
    "print(tokenizer.encode(' hello'))\n",
    "print(tokenizer.encode('  hello'))\n",
    "print(tokenizer.encode('   hello'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Token ID|\tString|\n",
    "|-|-|\n",
    "|39177|\tItemThumbnailImage|\n",
    "|30210|\tguiActiveUnfocused|\n",
    "|39755|\tisSpecialOrderable|\n",
    "|31576|\texternalActionCode|\n",
    "|39753|\tquickShipAvailable|\n",
    "|39757|\tchannelAvailability|\n",
    "|36174|\tRandomRedditorWithNo|\n",
    "|30899|\tcloneembedreportprint|\n",
    "|40242|\tBuyableInstoreAndOnline|\n",
    "|30906|\trawdownloadcloneembedreportprint|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(39177))\n",
    "\n",
    "print(tokenizer.encode('ItemThumbnailImage'))\n",
    "\n",
    "print(tokenizer.encode('chartreuse'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 attention mask\n",
    "- 结构化，批次化的tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel,GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "context = tokenizer('It will rain in the', return_tensors='pt')\n",
    "context\n",
    "# prediction = gpt2.generate(**context, max_length=10)\n",
    "# tokenizer.decode(prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.padding_side = \"left\"\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "sentences = [\"It will rain in the\",\n",
    "            \"I want to eat a big bowl of\",\n",
    "            \"My dog is\"]\n",
    "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True)\n",
    "print(inputs.input_ids)\n",
    "print(inputs.attention_mask)\n",
    "# output_sequences = gpt2.generate(**inputs)\n",
    "\n",
    "# for seq in output_sequences:\n",
    "#     print(tokenizer.decode(seq))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## forward\n",
    "- GPT2Model\n",
    "    - wte: word token embedding\n",
    "    - wpe: word position embedding\n",
    "- LMHead\n",
    "    - mlp: hidden_state => vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
