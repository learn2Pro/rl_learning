{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "565c28b5-1785-40de-a613-52ea34373983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/ubuntu/.local/lib/python3.10/site-packages (3.3.2)\n",
      "Requirement already satisfied: transformers in /home/ubuntu/.local/lib/python3.10/site-packages (4.49.0)\n",
      "Requirement already satisfied: trl in /home/ubuntu/.local/lib/python3.10/site-packages (0.15.1)\n",
      "Requirement already satisfied: torch in /usr/lib/python3/dist-packages (2.5.1)\n",
      "Requirement already satisfied: tf-keras in /home/ubuntu/.local/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: peft in /home/ubuntu/.local/lib/python3.10/site-packages (0.14.0)\n",
      "Requirement already satisfied: wandb in /home/ubuntu/.local/lib/python3.10/site-packages (0.19.7)\n",
      "Requirement already satisfied: fsspec[http]<=2024.12.0,>=2023.1.0 in /usr/lib/python3/dist-packages (from datasets) (2024.3.1)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3/dist-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: pandas in /usr/lib/python3/dist-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (0.29.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets) (5.4.1)\n",
      "Requirement already satisfied: aiohttp in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (3.11.12)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: accelerate>=0.34.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from trl) (1.4.0)\n",
      "Requirement already satisfied: rich in /home/ubuntu/.local/lib/python3.10/site-packages (from trl) (13.9.4)\n",
      "Requirement already satisfied: tensorflow<2.19,>=2.18 in /usr/lib/python3/dist-packages (from tf-keras) (2.18.0)\n",
      "Requirement already satisfied: psutil in /usr/lib/python3/dist-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /home/ubuntu/.local/lib/python3.10/site-packages (from wandb) (2.10.6)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /home/ubuntu/.local/lib/python3.10/site-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /usr/lib/python3/dist-packages (from wandb) (2.5.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from wandb) (2.22.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from wandb) (59.6.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/lib/python3/dist-packages (from wandb) (4.21.12)\n",
      "Requirement already satisfied: setproctitle in /home/ubuntu/.local/lib/python3.10/site-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (2020.6.20)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from rich->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from rich->trl) (2.19.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers trl torch tf-keras peft wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9918d7e2-4cff-4b31-9ef6-543995931598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: jinja2==3.1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (3.1.0)\n",
      "Requirement already satisfied: vllm in /home/ubuntu/.local/lib/python3.10/site-packages (0.7.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2==3.1.0) (2.0.1)\n",
      "Requirement already satisfied: aiohttp in /home/ubuntu/.local/lib/python3.10/site-packages (from vllm) (3.11.12)\n",
      "Requirement already satisfied: xformers==0.0.28.post3 in /home/ubuntu/.local/lib/python3.10/site-packages (from vllm) (0.0.28.post3)\n",
      "Requirement already satisfied: lark==1.2.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from vllm) (1.2.2)\n",
      "Requirement already satisfied: numba==0.60.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from vllm) (0.60.0)\n",
      "Requirement already satisfied: pillow in /home/ubuntu/.local/lib/python3.10/site-packages (from vllm) (11.1.0)\n",
      "Requirement already satisfied: depyf==0.18.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from vllm) (0.18.0)\n",
      "Requirement already satisfied: torchaudio==2.5.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from vllm) (2.5.1)\n",
      "Requirement already satisfied: pyzmq in /usr/lib/python3/dist-packages (from vllm) (22.3.0)\n",
      "Requirement already satisfied: cloudpickle in /home/ubuntu/.local/lib/python3.10/site-packages (from vllm) (3.1.1)\n",
      "Requirement already satisfied: torchvision==0.20.1 in /usr/lib/python3/dist-packages (from vllm) (0.20.1)\n",
      "Requirement already satisfied: blake3 in /home/ubuntu/.local/lib/python3.10/site-packages (from vllm) (1.0.4)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from vllm) (0.21.0)\n",
      "Requirement already satisfied: msgspec in /home/ubuntu/.local/lib/python3.10/site-packages (from vllm) (0.19.0)\n",
      "Requirement already satisfied: compressed-tensors==0.9.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from vllm) (0.9.1)\n",
      "Requirement already satisfied: py-cpuinfo in /home/ubuntu/.local/lib/python3.10/site-packages (from vllm) (9.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.10 in /home/ubuntu/.local/lib/python3.10/site-packages (from vllm) (4.12.2)\n",
      "Requirement already satisfied: openai>=1.52.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from vllm) (1.63.2)\n",
      "Requirement already satisfied: mistral_common[opencv]>=1.5.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from vllm) (1.5.3)\n",
      "Requirement already satisfied: transformers>=4.48.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from vllm) (4.49.0)\n",
      "Requirement already satisfied: torch==2.5.1 in /usr/lib/python3/dist-packages (from vllm) (2.5.1)\n",
      "Requirement already satisfied: gguf==0.10.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from vllm) (0.10.0)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/.local/lib/python3.10/site-packages (from vllm) (4.67.1)\n",
      "Requirement already satisfied: tiktoken>=0.6.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from vllm) (0.9.0)\n",
      "Requirement already satisfied: sentencepiece in /home/ubuntu/.local/lib/python3.10/site-packages (from vllm) (0.2.0)\n",
      "Requirement already satisfied: pydantic>=2.9 in /home/ubuntu/.local/lib/python3.10/site-packages (from vllm) (2.10.6)\n",
      "Requirement already satisfied: outlines==0.1.11 in /home/ubuntu/.local/lib/python3.10/site-packages (from vllm) (0.1.11)\n",
      "Requirement already satisfied: partial-json-parser in /home/ubuntu/.local/lib/python3.10/site-packages (from vllm) (0.2.1.1.post5)\n",
      "Requirement already satisfied: fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from vllm) (0.115.6)\n",
      "Requirement already satisfied: lm-format-enforcer<0.11,>=0.10.9 in /home/ubuntu/.local/lib/python3.10/site-packages (from vllm) (0.10.10)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from vllm) (5.4.1)\n",
      "Requirement already satisfied: xgrammar==0.1.11 in /home/ubuntu/.local/lib/python3.10/site-packages (from vllm) (0.1.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from vllm) (2.32.3)\n",
      "Requirement already satisfied: filelock>=3.16.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from vllm) (3.17.0)\n",
      "Requirement already satisfied: numpy<2.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from vllm) (1.26.4)\n",
      "Requirement already satisfied: einops in /home/ubuntu/.local/lib/python3.10/site-packages (from vllm) (0.8.1)\n",
      "Requirement already satisfied: psutil in /usr/lib/python3/dist-packages (from vllm) (5.9.0)\n",
      "Requirement already satisfied: ray[adag]==2.40.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from vllm) (2.40.0)\n",
      "Requirement already satisfied: importlib_metadata in /usr/lib/python3/dist-packages (from vllm) (4.6.4)\n",
      "Requirement already satisfied: prometheus_client>=0.18.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from vllm) (0.21.1)\n",
      "Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from vllm) (7.0.2)\n",
      "Requirement already satisfied: protobuf in /usr/lib/python3/dist-packages (from vllm) (4.21.12)\n",
      "Requirement already satisfied: dill in /home/ubuntu/.local/lib/python3.10/site-packages (from depyf==0.18.0->vllm) (0.3.8)\n",
      "Requirement already satisfied: astor in /home/ubuntu/.local/lib/python3.10/site-packages (from depyf==0.18.0->vllm) (0.8.1)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /home/ubuntu/.local/lib/python3.10/site-packages (from numba==0.60.0->vllm) (0.43.0)\n",
      "Requirement already satisfied: nest_asyncio in /usr/lib/python3/dist-packages (from outlines==0.1.11->vllm) (1.5.4)\n",
      "Requirement already satisfied: outlines_core==0.1.26 in /home/ubuntu/.local/lib/python3.10/site-packages (from outlines==0.1.11->vllm) (0.1.26)\n",
      "Requirement already satisfied: referencing in /home/ubuntu/.local/lib/python3.10/site-packages (from outlines==0.1.11->vllm) (0.36.2)\n",
      "Requirement already satisfied: jsonschema in /home/ubuntu/.local/lib/python3.10/site-packages (from outlines==0.1.11->vllm) (4.23.0)\n",
      "Requirement already satisfied: diskcache in /home/ubuntu/.local/lib/python3.10/site-packages (from outlines==0.1.11->vllm) (5.6.3)\n",
      "Requirement already satisfied: interegular in /home/ubuntu/.local/lib/python3.10/site-packages (from outlines==0.1.11->vllm) (0.3.3)\n",
      "Requirement already satisfied: airportsdata in /home/ubuntu/.local/lib/python3.10/site-packages (from outlines==0.1.11->vllm) (20241001)\n",
      "Requirement already satisfied: pycountry in /home/ubuntu/.local/lib/python3.10/site-packages (from outlines==0.1.11->vllm) (24.6.1)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/lib/python3/dist-packages (from ray[adag]==2.40.0->vllm) (1.0.3)\n",
      "Requirement already satisfied: click>=7.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from ray[adag]==2.40.0->vllm) (8.1.8)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3/dist-packages (from ray[adag]==2.40.0->vllm) (21.3)\n",
      "Requirement already satisfied: aiosignal in /home/ubuntu/.local/lib/python3.10/site-packages (from ray[adag]==2.40.0->vllm) (1.3.2)\n",
      "Requirement already satisfied: frozenlist in /home/ubuntu/.local/lib/python3.10/site-packages (from ray[adag]==2.40.0->vllm) (1.5.0)\n",
      "Requirement already satisfied: cupy-cuda12x in /home/ubuntu/.local/lib/python3.10/site-packages (from ray[adag]==2.40.0->vllm) (13.3.0)\n",
      "Requirement already satisfied: pytest in /home/ubuntu/.local/lib/python3.10/site-packages (from xgrammar==0.1.11->vllm) (8.3.4)\n",
      "Requirement already satisfied: pybind11 in /home/ubuntu/.local/lib/python3.10/site-packages (from xgrammar==0.1.11->vllm) (2.13.6)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0->vllm) (0.41.3)\n",
      "Requirement already satisfied: fastapi-cli[standard]>=0.0.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0->vllm) (0.0.7)\n",
      "Requirement already satisfied: email-validator>=2.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0->vllm) (2.2.0)\n",
      "Requirement already satisfied: python-multipart>=0.0.7 in /home/ubuntu/.local/lib/python3.10/site-packages (from fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0->vllm) (0.0.20)\n",
      "Requirement already satisfied: uvicorn[standard]>=0.12.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0->vllm) (0.34.0)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0->vllm) (0.28.1)\n",
      "Requirement already satisfied: opencv-python-headless>=4.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from mistral_common[opencv]>=1.5.0->vllm) (4.11.0.86)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from openai>=1.52.0->vllm) (0.8.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from openai>=1.52.0->vllm) (4.8.0)\n",
      "Requirement already satisfied: sniffio in /home/ubuntu/.local/lib/python3.10/site-packages (from openai>=1.52.0->vllm) (1.3.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.52.0->vllm) (1.7.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from pydantic>=2.9->vllm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from pydantic>=2.9->vllm) (2.27.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.26.0->vllm) (2020.6.20)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests>=2.26.0->vllm) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests>=2.26.0->vllm) (2.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.26.0->vllm) (3.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/ubuntu/.local/lib/python3.10/site-packages (from tiktoken>=0.6.0->vllm) (2024.11.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/ubuntu/.local/lib/python3.10/site-packages (from tokenizers>=0.19.1->vllm) (0.29.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers>=4.48.2->vllm) (0.5.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->vllm) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->vllm) (0.3.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->vllm) (5.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->vllm) (1.18.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->vllm) (2.4.6)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->vllm) (25.1.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.52.0->vllm) (1.2.2)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from email-validator>=2.0.0->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0->vllm) (2.7.0)\n",
      "Requirement already satisfied: typer>=0.12.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from fastapi-cli[standard]>=0.0.5->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0->vllm) (0.15.1)\n",
      "Requirement already satisfied: rich-toolkit>=0.11.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from fastapi-cli[standard]>=0.0.5->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0->vllm) (0.13.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ubuntu/.local/lib/python3.10/site-packages (from httpx>=0.23.0->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0->vllm) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/ubuntu/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.23.0->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0->vllm) (0.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/lib/python3/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.19.1->vllm) (2024.3.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ubuntu/.local/lib/python3.10/site-packages (from jsonschema->outlines==0.1.11->vllm) (2024.10.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from jsonschema->outlines==0.1.11->vllm) (0.23.1)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0->vllm) (0.6.4)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0->vllm) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /home/ubuntu/.local/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0->vllm) (15.0)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /home/ubuntu/.local/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0->vllm) (1.0.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /home/ubuntu/.local/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0->vllm) (1.0.4)\n",
      "Requirement already satisfied: fastrlock>=0.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from cupy-cuda12x->ray[adag]==2.40.0->vllm) (0.8.3)\n",
      "Requirement already satisfied: iniconfig in /home/ubuntu/.local/lib/python3.10/site-packages (from pytest->xgrammar==0.1.11->vllm) (2.0.0)\n",
      "Requirement already satisfied: tomli>=1 in /home/ubuntu/.local/lib/python3.10/site-packages (from pytest->xgrammar==0.1.11->vllm) (2.2.1)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from pytest->xgrammar==0.1.11->vllm) (1.5.0)\n",
      "Requirement already satisfied: rich>=13.7.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from rich-toolkit>=0.11.1->fastapi-cli[standard]>=0.0.5->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0->vllm) (13.9.4)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from typer>=0.12.3->fastapi-cli[standard]>=0.0.5->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0->vllm) (1.5.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli[standard]>=0.0.5->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0->vllm) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli[standard]>=0.0.5->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0->vllm) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli[standard]>=0.0.5->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0->vllm) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install jinja2==3.1.0 vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50de8539-c021-488f-89a2-a0e6b7047e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-22 20:38:02 __init__.py:183] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-22 20:38:02,120\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu124\n",
      "Transformers backend: True\n",
      "3.1.4\n",
      "0.7.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import trl\n",
    "import jinja2\n",
    "import vllm\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers backend: {transformers.file_utils.is_torch_available()}\")\n",
    "print(f\"{jinja2.__version__ }\")\n",
    "print(f\"{vllm.__version__ }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fffdad4-c8a0-48e0-8d7a-86942041152a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reference:\n",
    "\n",
    "https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb\n",
    "\"\"\"\n",
    "import re\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76e977f5-92d6-41e8-80f4-83cb29f31661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and prepare ds\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Responde in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "XML_COT_FORMAT = \"\"\"\\\n",
    "<reasoning>\n",
    "{reasoning}\n",
    "</reasoning>\n",
    "<answer>\n",
    "{answer}\n",
    "</answer>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa4fe9bd-11a4-4e25-9b66-f5d5059fec5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "asas\n"
     ]
    }
   ],
   "source": [
    "def extract_xml_answer(text:str):\n",
    "    answer = text.split(\"<answer>\")[-1]\n",
    "    answer = answer.split(\"</answer>\")[0]\n",
    "    return answer.strip()\n",
    "\n",
    "def extract_hash_answer(text:str):\n",
    "    if '####' not in text:\n",
    "        return None\n",
    "    return text.split('####')[1].strip()\n",
    "\n",
    "def get_gsm8k_questions(split='train'):\n",
    "    data = load_dataset('openai/gsm8k', 'main')[split]\n",
    "    data = data.map(lambda x: {\n",
    "        'prompt':[\n",
    "            {'role':'system', 'content': SYSTEM_PROMPT},\n",
    "            {'role':'user', 'content': x['question']}\n",
    "        ],\n",
    "        'answer': extract_hash_answer(x['answer'])\n",
    "    }) # type: ignore\n",
    "    return data\n",
    "print(extract_xml_answer(\"<answer>123</answer>\"))\n",
    "print(extract_hash_answer('12 #### asas'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "966ffbf5-3538-4908-8e8f-65be2a9b31ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'prompt'],\n",
       "    num_rows: 7473\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = get_gsm8k_questions()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2c441d5-3f93-4ef1-8f79-8171b6f8bd13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.382]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step = 0\n",
    "# reward functions\n",
    "def correctness_reward_func(prompts, completions, answer, **kwargs):\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    q = prompts[0][-1]['content'] # [{role:system},{role:user},{role:assistance}]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    if '</answer>' in responses[0] and '<answer>' in responses[0] and '<reasoning>' in responses[0] and '</reasoning>' in responses[0]:\n",
    "        print('-'*20, f\"Question:\\n{q}\\n\", '-'*20, f\"Answer:\\n{answer[0]}\\n\", '-'*20, f\"Response:\\n{responses[0]}\\n\", '-'*20, f\"Extracted:\\n{extracted_responses[0]}\",'\\n\\n')\n",
    "    return [2.0 if r==a else 0.0 for r,a in zip(extracted_responses, answer)]\n",
    "\n",
    "def int_reward_func(completions, **kwargs):\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n",
    "\n",
    "def strict_format_reward_func(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward function that checks if the completion has as specific format\n",
    "    \"\"\"\n",
    "    pattern = r\"^<reasoning>.*</reasoning>\\n<answer>.*?</answer>$\"\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    matches = [re.match(pattern, r, re.DOTALL) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def soft_format_reward_func(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward function that checks if the completion has a specific format\n",
    "    \"\"\"\n",
    "    pattern = r\"<reasoning>.*</reasoning>\\n<answer>.*?</answer>\"\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    matches = [re.match(pattern, r, re.DOTALL) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def count_xml(text):\n",
    "    count = 0.0\n",
    "    if text.count(\"<reasoning>\\n\") == 1:\n",
    "        count+=0.125\n",
    "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
    "        count+=0.125\n",
    "    if text.count(\"\\n<answer>\\n\") == 1:\n",
    "        count+=0.125\n",
    "        count -= len(text.split(\"\\n</answer>\\n\")[-1]) * 0.001\n",
    "    if text.count(\"\\n</answer>\") == 1:\n",
    "        count+=0.125\n",
    "        count-=(len(text.split(\"\\n</answer>\")[-1])-1)*0.001\n",
    "    return count\n",
    "\n",
    "def xml_count_reward_func(completions, **kwargs):\n",
    "    contents = [completion[0]['content'] for completion in completions]\n",
    "    return [count_xml(c) for c in contents]\n",
    "\n",
    "\n",
    "\n",
    "completions = [\n",
    "    [{\"role\": \"assistant\", \"content\": \"<reasoning>\\nThe sum of 1 and 2 is 3, which we multiply by 4 to get 12.\\n</reasoning>\\n<answer>\\n(1 + 2) * 4 = 12\\n</answer>\"}],\n",
    "    # [{\"role\": \"assistant\", \"content\": \"The sum of 3 and 1 is 4, which we multiply by 2 to get 8. So (3 + 1) * 2 = 8.\"}],\n",
    "]\n",
    "\n",
    "xml_count_reward_func(completions)\n",
    "# 0.382\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce813e02-e071-4465-8d0f-320d143010a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# <reasoning>\n",
    "# First, let's count the number of people in each section:\n",
    "\n",
    "# - Orchestra section: 1 (Sebastian, the drummer)\n",
    "# - Brass section: 7 people (4 trombones, 2 trumpets, 1 French horn)\n",
    "# - Strings section: 5 people (3 violins, 1 cellist, 1 contrabassist)\n",
    "# - Woodwinds section: 3 clarinets + 4 flutes = 7 people\n",
    "\n",
    "# Now, let's add up the total number of people:\n",
    "\n",
    "# Orchestra section: 1\n",
    "# Brass section: 7\n",
    "# Strings section: 5\n",
    "# Woodwinds section: 7\n",
    "\n",
    "# Total number of people in the orchestra: 1 + 7 + 5 + 7 = 20\n",
    "# </reasoning>\n",
    "\n",
    "# <answer>\n",
    "# 20\n",
    "# </answer>\n",
    "\n",
    "text = '''<reasoning>\n",
    "First, let's count the number of people in each section:\n",
    "\n",
    "- Orchestra section: 1 (Sebastian, the drummer)\n",
    "- Brass section: 7 people (4 trombones, 2 trumpets, 1 French horn)\n",
    "- Strings section: 5 people (3 violins, 1 cellist, 1 contrabassist)\n",
    "- Woodwinds section: 3 clarinets + 4 flutes = 7 people\n",
    "\n",
    "Now, let's add up the total number of people:\n",
    "\n",
    "Orchestra section: 1\n",
    "Brass section: 7\n",
    "Strings section: 5\n",
    "Woodwinds section: 7\n",
    "\n",
    "Total number of people in the orchestra: 1 + 7 + 5 + 7 = 20\n",
    "</reasoning>\n",
    "<answer>\n",
    "20\n",
    "</answer>'''\n",
    "\n",
    "\n",
    "# def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "#     \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "#     # pattern = r\"^<reasoning>.*?</reasoning><answer>\\n.*?\\n</answer>\\n$\"\n",
    "#     pattern = r\"^<reasoning>.*</reasoning>\\n<answer>.*?</answer>$\"\n",
    "#     responses = [completion[0][\"content\"] for completion in completions]\n",
    "#     matches = [re.match(pattern, r, re.DOTALL) for r in responses]\n",
    "#     return [0.5 if match else 0.0 for match in matches]\n",
    "    \n",
    "completions = [\n",
    "    [{\"role\": \"assistant\", \"content\": text}],\n",
    "    # [{\"role\": \"assistant\", \"content\": \"The sum of 3 and 1 is 4, which we multiply by 2 to get 8. So (3 + 1) * 2 = 8.\"}],\n",
    "]\n",
    "\n",
    "strict_format_reward_func(completions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5fc98e7-1478-452a-b3e1-0269fcba033e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = '''<reasoning>\n",
    "First, let's count the number of people in each section:\n",
    "\n",
    "- Orchestra section: 1 (Sebastian, the drummer)\n",
    "- Brass section: 7 people (4 trombones, 2 trumpets, 1 French horn)\n",
    "- Strings section: 5 people (3 violins, 1 cellist, 1 contrabassist)\n",
    "- Woodwinds section: 3 clarinets + 4 flutes = 7 people\n",
    "\n",
    "Now, let's add up the total number of people:\n",
    "\n",
    "Orchestra section: 1\n",
    "Brass section: 7\n",
    "Strings section: 5\n",
    "Woodwinds section: 7\n",
    "\n",
    "Total number of people in the orchestra: 1 + 7 + 5 + 7 = 20\n",
    "</reasoning>\n",
    "<answer>\n",
    "20\n",
    "</answer>'''\n",
    "\n",
    "# Use re.DOTALL to ensure . matches newline characters\n",
    "pattern = r\"^<reasoning>.*</reasoning>\\n<answer>.*?</answer>$\"\n",
    "\n",
    "match = re.match(pattern, text, re.DOTALL)\n",
    "\n",
    "if match:\n",
    "    print(\"Match found!\")\n",
    "else:\n",
    "    print(\"No match.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a60f90e-dceb-4ff5-b155-adbc6a2c9a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOConfig\n",
    "param_size = \"1.5B\"\n",
    "model_name = f\"Qwen/Qwen2.5-{param_size}-Instruct\"\n",
    "\n",
    "output_dir=f\"outputs/Qwen-{param_size}-GRPO\"\n",
    "run_name=f\"QWEN-{param_size}-GRPO-gsm8k-1\"\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=output_dir,\n",
    "    run_name=run_name,\n",
    "    learning_rate=5e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = 'cosine',\n",
    "    logging_steps=1,\n",
    "    bf16=True,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_generations=12,\n",
    "    max_prompt_length=256,\n",
    "    max_completion_length=200,\n",
    "    num_train_epochs=1,\n",
    "    save_steps=100,\n",
    "    max_grad_norm=0.1,\n",
    "    log_on_each_node=False,\n",
    "    use_vllm=False,\n",
    "    vllm_gpu_memory_utilization=0.3,\n",
    "    vllm_device='cuda:0',\n",
    "    report_to='wandb',\n",
    ")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab65df2e-a7b8-406a-beb4-fa933dba42c6",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0251fd4b-5247-4255-adfb-e9f299fbcf0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-22 21:10:46,403] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-22 21:10:46 __init__.py:183] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-22 21:10:47,022\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param= 1.5B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdruidlangde\u001b[0m (\u001b[33mdruidlangde-tencent\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/samtang/export/rl_learning/llm/wandb/run-20250222_211100-mlvdqfk8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/druidlangde-tencent/huggingface/runs/mlvdqfk8' target=\"_blank\">QWEN-1.5B-GRPO-gsm8k-1</a></strong> to <a href='https://wandb.ai/druidlangde-tencent/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/druidlangde-tencent/huggingface' target=\"_blank\">https://wandb.ai/druidlangde-tencent/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/druidlangde-tencent/huggingface/runs/mlvdqfk8' target=\"_blank\">https://wandb.ai/druidlangde-tencent/huggingface/runs/mlvdqfk8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:197: FutureWarning: `num_logits_to_keep` is deprecated and will be removed in version 4.50 for `Qwen2ForCausalLM.forward`. Use `logits_to_keep` instead.\n",
      "  return self.model.forward(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 932.00 MiB. GPU 0 has a total capacity of 23.61 GiB of which 621.50 MiB is free. Including non-PyTorch memory, this process has 21.73 GiB memory in use. Of the allocated memory 20.58 GiB is allocated by PyTorch, and 719.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 30\u001b[0m\n\u001b[1;32m     13\u001b[0m trainer \u001b[38;5;241m=\u001b[39m GRPOTrainer(\n\u001b[1;32m     14\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[1;32m     15\u001b[0m     processing_class\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# peft_config\u001b[39;00m\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparam=\u001b[39m\u001b[38;5;124m'\u001b[39m, param_size)\n\u001b[0;32m---> 30\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(output_dir)\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.10/site-packages/transformers/trainer.py:2184\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2182\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2183\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2185\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2189\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.10/site-packages/transformers/trainer.py:2544\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2537\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2538\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2539\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2540\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2541\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2542\u001b[0m )\n\u001b[1;32m   2543\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2544\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2547\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2548\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2549\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2550\u001b[0m ):\n\u001b[1;32m   2551\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2552\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.10/site-packages/transformers/trainer.py:3688\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3685\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3687\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3688\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3690\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3692\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3693\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3694\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.10/site-packages/trl/trainer/grpo_trainer.py:444\u001b[0m, in \u001b[0;36mGRPOTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(per_token_logps)\n\u001b[1;32m    443\u001b[0m num_logits_to_keep \u001b[38;5;241m=\u001b[39m completion_ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# we only need to compute the logits for the completion tokens\u001b[39;00m\n\u001b[0;32m--> 444\u001b[0m per_token_logps \u001b[38;5;241m=\u001b[39m \u001b[43mget_per_token_logps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_completion_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mref_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.10/site-packages/trl/trainer/grpo_trainer.py:432\u001b[0m, in \u001b[0;36mGRPOTrainer.compute_loss.<locals>.get_per_token_logps\u001b[0;34m(model, input_ids, num_logits_to_keep)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_per_token_logps\u001b[39m(model, input_ids, num_logits_to_keep):\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;66;03m# We add 1 to `num_logits_to_keep` because the last logits of the sequence is later excluded\u001b[39;00m\n\u001b[0;32m--> 432\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits  \u001b[38;5;66;03m# (B, L, V)\u001b[39;00m\n\u001b[1;32m    433\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# (B, L-1, V), exclude the last logit: it corresponds to the next token pred\u001b[39;00m\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;66;03m# Compute the log probabilities for the input tokens. Use a loop to reduce memory peak.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.10/site-packages/accelerate/utils/operations.py:819\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.10/site-packages/accelerate/utils/operations.py:807\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 807\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.10/site-packages/torch/amp/autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.10/site-packages/peft/peft_model.py:1719\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1718\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1719\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1720\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1725\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1726\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1728\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1730\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1732\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:197\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:841\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n\u001b[1;32m    840\u001b[0m slice_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m-\u001b[39mlogits_to_keep, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(logits_to_keep, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m logits_to_keep\n\u001b[0;32m--> 841\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslice_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    843\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 932.00 MiB. GPU 0 has a total capacity of 23.61 GiB of which 621.50 MiB is free. Including non-PyTorch memory, this process has 21.73 GiB memory in use. Of the allocated memory 20.58 GiB is allocated by PyTorch, and 719.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from trl import GRPOTrainer\n",
    "from peft import LoraConfig\n",
    "import torch\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    task_type='CAUSAL_LM'\n",
    ")\n",
    "\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model_name,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[\n",
    "        xml_count_reward_func,\n",
    "        soft_format_reward_func,\n",
    "        strict_format_reward_func,\n",
    "        int_reward_func,\n",
    "        correctness_reward_func\n",
    "    ],\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    # peft_config\n",
    ")\n",
    "\n",
    "print('param=', param_size)\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfafba93-6c1d-499c-ab97-33ab12485977",
   "metadata": {},
   "source": [
    "## inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ce6dde4-709e-428e-8fd6-37a94b65c870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "output_dir = 'outputs/Qwen-0.5B-GRPO'\n",
    "model = AutoModelForCausalLM.from_pretrained(output_dir)\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab1d89fc-971f-4c3d-89ce-064cc27996a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e279dbe8-0018-434d-9f16-5b65270de668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ": \n",
      "Dave bought 8 books about animals, 6 books about outer space, and 3 books about trains to keep him busy over the holidays. Each book cost $6. How much did Dave spend on the books?\n",
      "\n",
      ":\n",
      "books? To determine how much Dave spent on the books, we first need to calculate the total quantity of books he acquired, then multiply by each individual price.pend on the \n",
      "\n",
      "10 books are devoted to Animals: \n",
      " 4 books regarding animal, 6 Books is (9)\n",
      " Total books = sum total book\n",
      "\n",
      "20 books about outer Space\n",
      "4 6Books is 12\n",
      "Total books = 20 books\n",
      "\n",
      "3 books About Train= 3  5 books  3(20) \n",
      "\n",
      "Calculate total books = 3( 3 3)= 7 20\n",
      "\n",
      "Therefore, Total books amount to 7 4 5 5 6 = 60. Verify  60 Book  6 Cost 10 books per 6. Verification 60 books 20 cost  6 3 12\n",
      "\n",
      "In the 20 book total books, the net cost of 20 units per $6 is the whole set 20 x 6 cost  6\n",
      "\n",
      "The total spending in the books =  10 30  3.30 202.40\n",
      "\n",
      " 20.40  60 total book =   60 20  12  40 \n",
      "\n",
      "Thus, Dave  spends   books.\n",
      "\n",
      "Final answers. 120, 00        20 20      \n",
      "\n",
      " 20    20 40        \n",
      "\n",
      " 20  3. 40  0.  20  40 0.    320       20                \n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "import torch\n",
    "\n",
    "def generate_with_stream(input_text):\n",
    "    print(f\"\\n: \\n{input_text}\")\n",
    "    print(\"\\n:\")\n",
    "    \n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    streamer = TextStreamer(tokenizer)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=512,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            streamer=streamer\n",
    "        )\n",
    "    \n",
    "    # \n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# \n",
    "input_text = \"Dave bought 8 books about animals, 6 books about outer space, and 3 books about trains to keep him busy over the holidays. Each book cost $6. How much did Dave spend on the books?\"\n",
    "generate_with_stream(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c325ea07-3b7b-4009-99b7-0a43c4e1c709",
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = torch.randn(2,3,5)\n",
    "s1 = torch.randn(2,3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "503ad16d-8c72-4345-b38d-60435d528237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 5])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([s0,s1], dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "663771c5-0b17-414b-b775-a6ed08286a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 5])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([s0, s1],dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a85fbbc-b970-4585-88e3-d8777558b999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: torch.Size([2, 3, 10])\n",
      "hn shape: torch.Size([1, 2, 10])\n",
      "cn shape: torch.Size([1, 2, 10])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "#  LSTM\n",
    "lstm = nn.LSTM(input_size=5, hidden_size=10, num_layers=1, batch_first=True)\n",
    "\n",
    "#  (batch_size=2, seq_len=3, input_size=5)\n",
    "x = torch.randn(2, 3, 5)\n",
    "\n",
    "# \n",
    "output, (hn, cn) = lstm(x)\n",
    "\n",
    "print(f\"output shape: {output.shape}\")  # (2, 3, 10)\n",
    "print(f\"hn shape: {hn.shape}\")          # (1, 2, 10)\n",
    "print(f\"cn shape: {cn.shape}\")          # (1, 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16bbf2d-4246-4655-a8fe-3fc0b7d474bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
