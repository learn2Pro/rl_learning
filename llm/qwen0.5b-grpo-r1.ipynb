{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "565c28b5-1785-40de-a613-52ea34373983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: datasets in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (3.2.0)\n",
      "Requirement already satisfied: transformers in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (4.49.0.dev0)\n",
      "Requirement already satisfied: trl in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (0.15.0.dev0)\n",
      "Requirement already satisfied: torch in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: peft in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (0.3.0)\n",
      "Requirement already satisfied: wandb in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (0.19.1)\n",
      "Requirement already satisfied: filelock in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from datasets) (3.11.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from datasets) (0.26.3)\n",
      "Requirement already satisfied: packaging in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: accelerate>=0.34.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from trl) (1.3.0)\n",
      "Requirement already satisfied: rich in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from trl) (13.9.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: psutil in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from peft) (6.1.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from wandb) (2.10.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from wandb) (2.19.2)\n",
      "Requirement already satisfied: setproctitle in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from wandb) (75.1.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from rich->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from rich->trl) (2.18.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate datasets transformers trl torch peft wandb jinja2==3.1.0 vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50de8539-c021-488f-89a2-a0e6b7047e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-23 13:35:57 __init__.py:183] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 13:35:57,581\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu124\n",
      "Transformers backend: True\n",
      "3.1.4\n",
      "0.7.0\n",
      "0.14.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import trl\n",
    "import jinja2\n",
    "import vllm\n",
    "import peft\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers backend: {transformers.file_utils.is_torch_available()}\")\n",
    "print(f\"{jinja2.__version__ }\")\n",
    "print(f\"{vllm.__version__ }\")\n",
    "print(f\"{peft.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fffdad4-c8a0-48e0-8d7a-86942041152a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reference:\n",
    "\n",
    "https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb\n",
    "\"\"\"\n",
    "import re\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76e977f5-92d6-41e8-80f4-83cb29f31661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and prepare ds\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Responde in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "XML_COT_FORMAT = \"\"\"\\\n",
    "<reasoning>\n",
    "{reasoning}\n",
    "</reasoning>\n",
    "<answer>\n",
    "{answer}\n",
    "</answer>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa4fe9bd-11a4-4e25-9b66-f5d5059fec5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "asas\n"
     ]
    }
   ],
   "source": [
    "def extract_xml_answer(text:str):\n",
    "    answer = text.split(\"<answer>\")[-1]\n",
    "    answer = answer.split(\"</answer>\")[0]\n",
    "    return answer.strip()\n",
    "\n",
    "def extract_hash_answer(text:str):\n",
    "    if '####' not in text:\n",
    "        return None\n",
    "    return text.split('####')[1].strip()\n",
    "\n",
    "def get_gsm8k_questions(split='train'):\n",
    "    data = load_dataset('openai/gsm8k', 'main')[split]\n",
    "    data = data.map(lambda x: {\n",
    "        'prompt':[\n",
    "            {'role':'system', 'content': SYSTEM_PROMPT},\n",
    "            {'role':'user', 'content': x['question']}\n",
    "        ],\n",
    "        'answer': extract_hash_answer(x['answer'])\n",
    "    }) # type: ignore\n",
    "    return data\n",
    "print(extract_xml_answer(\"<answer>123</answer>\"))\n",
    "print(extract_hash_answer('12 #### asas'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "966ffbf5-3538-4908-8e8f-65be2a9b31ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'prompt'],\n",
       "    num_rows: 7473\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = get_gsm8k_questions()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2c441d5-3f93-4ef1-8f79-8171b6f8bd13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.382]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step = 0\n",
    "# reward functions\n",
    "def correctness_reward_func(prompts, completions, answer, **kwargs):\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    q = prompts[0][-1]['content'] # [{role:system},{role:user},{role:assistance}]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    if '</answer>' in responses[0] and '<answer>' in responses[0] and '<reasoning>' in responses[0] and '</reasoning>' in responses[0]:\n",
    "        # print('-'*20, f\"Question:\\n{q}\\n\", '-'*20, f\"Answer:\\n{answer[0]}\\n\", '-'*20, f\"Response:\\n{responses[0]}\\n\", '-'*20, f\"Extracted:\\n{extracted_responses[0]}\",'\\n\\n')\n",
    "        print('-'*20, f\"Question:\\n{q}\\n\", f\"Answer: {answer[0]}\\n\", f\"Extracted: {extracted_responses[0]}\")\n",
    "    return [10.0 if r==a else 0.0 for r,a in zip(extracted_responses, answer)]\n",
    "\n",
    "def int_reward_func(completions, **kwargs):\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n",
    "\n",
    "def strict_format_reward_func(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward function that checks if the completion has as specific format\n",
    "    \"\"\"\n",
    "    pattern = r\"^<reasoning>.*</reasoning>\\n<answer>.*?</answer>$\"\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    matches = [re.match(pattern, r, re.DOTALL) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def soft_format_reward_func(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward function that checks if the completion has a specific format\n",
    "    \"\"\"\n",
    "    pattern = r\"<reasoning>.*</reasoning>\\n<answer>.*?</answer>\"\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    matches = [re.match(pattern, r, re.DOTALL) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def count_xml(text):\n",
    "    count = 0.0\n",
    "    if text.count(\"<reasoning>\\n\") == 1:\n",
    "        count+=0.125\n",
    "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
    "        count+=0.125\n",
    "    if text.count(\"\\n<answer>\\n\") == 1:\n",
    "        count+=0.125\n",
    "        count -= len(text.split(\"\\n</answer>\\n\")[-1]) * 0.001\n",
    "    if text.count(\"\\n</answer>\") == 1:\n",
    "        count+=0.125\n",
    "        count-=(len(text.split(\"\\n</answer>\")[-1])-1)*0.001\n",
    "    return count\n",
    "\n",
    "def xml_count_reward_func(completions, **kwargs):\n",
    "    contents = [completion[0]['content'] for completion in completions]\n",
    "    return [count_xml(c) for c in contents]\n",
    "\n",
    "\n",
    "\n",
    "completions = [\n",
    "    [{\"role\": \"assistant\", \"content\": \"<reasoning>\\nThe sum of 1 and 2 is 3, which we multiply by 4 to get 12.\\n</reasoning>\\n<answer>\\n(1 + 2) * 4 = 12\\n</answer>\"}],\n",
    "    # [{\"role\": \"assistant\", \"content\": \"The sum of 3 and 1 is 4, which we multiply by 2 to get 8. So (3 + 1) * 2 = 8.\"}],\n",
    "]\n",
    "\n",
    "xml_count_reward_func(completions)\n",
    "# 0.382\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce813e02-e071-4465-8d0f-320d143010a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# <reasoning>\n",
    "# First, let's count the number of people in each section:\n",
    "\n",
    "# - Orchestra section: 1 (Sebastian, the drummer)\n",
    "# - Brass section: 7 people (4 trombones, 2 trumpets, 1 French horn)\n",
    "# - Strings section: 5 people (3 violins, 1 cellist, 1 contrabassist)\n",
    "# - Woodwinds section: 3 clarinets + 4 flutes = 7 people\n",
    "\n",
    "# Now, let's add up the total number of people:\n",
    "\n",
    "# Orchestra section: 1\n",
    "# Brass section: 7\n",
    "# Strings section: 5\n",
    "# Woodwinds section: 7\n",
    "\n",
    "# Total number of people in the orchestra: 1 + 7 + 5 + 7 = 20\n",
    "# </reasoning>\n",
    "\n",
    "# <answer>\n",
    "# 20\n",
    "# </answer>\n",
    "\n",
    "text = '''<reasoning>\n",
    "First, let's count the number of people in each section:\n",
    "\n",
    "- Orchestra section: 1 (Sebastian, the drummer)\n",
    "- Brass section: 7 people (4 trombones, 2 trumpets, 1 French horn)\n",
    "- Strings section: 5 people (3 violins, 1 cellist, 1 contrabassist)\n",
    "- Woodwinds section: 3 clarinets + 4 flutes = 7 people\n",
    "\n",
    "Now, let's add up the total number of people:\n",
    "\n",
    "Orchestra section: 1\n",
    "Brass section: 7\n",
    "Strings section: 5\n",
    "Woodwinds section: 7\n",
    "\n",
    "Total number of people in the orchestra: 1 + 7 + 5 + 7 = 20\n",
    "</reasoning>\n",
    "<answer>\n",
    "20\n",
    "</answer>'''\n",
    "\n",
    "\n",
    "# def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "#     \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "#     # pattern = r\"^<reasoning>.*?</reasoning><answer>\\n.*?\\n</answer>\\n$\"\n",
    "#     pattern = r\"^<reasoning>.*</reasoning>\\n<answer>.*?</answer>$\"\n",
    "#     responses = [completion[0][\"content\"] for completion in completions]\n",
    "#     matches = [re.match(pattern, r, re.DOTALL) for r in responses]\n",
    "#     return [0.5 if match else 0.0 for match in matches]\n",
    "    \n",
    "completions = [\n",
    "    [{\"role\": \"assistant\", \"content\": text}],\n",
    "    # [{\"role\": \"assistant\", \"content\": \"The sum of 3 and 1 is 4, which we multiply by 2 to get 8. So (3 + 1) * 2 = 8.\"}],\n",
    "]\n",
    "\n",
    "strict_format_reward_func(completions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5fc98e7-1478-452a-b3e1-0269fcba033e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = '''<reasoning>\n",
    "First, let's count the number of people in each section:\n",
    "\n",
    "- Orchestra section: 1 (Sebastian, the drummer)\n",
    "- Brass section: 7 people (4 trombones, 2 trumpets, 1 French horn)\n",
    "- Strings section: 5 people (3 violins, 1 cellist, 1 contrabassist)\n",
    "- Woodwinds section: 3 clarinets + 4 flutes = 7 people\n",
    "\n",
    "Now, let's add up the total number of people:\n",
    "\n",
    "Orchestra section: 1\n",
    "Brass section: 7\n",
    "Strings section: 5\n",
    "Woodwinds section: 7\n",
    "\n",
    "Total number of people in the orchestra: 1 + 7 + 5 + 7 = 20\n",
    "</reasoning>\n",
    "<answer>\n",
    "20\n",
    "</answer>'''\n",
    "\n",
    "# Use re.DOTALL to ensure . matches newline characters\n",
    "pattern = r\"^<reasoning>.*</reasoning>\\n<answer>.*?</answer>$\"\n",
    "\n",
    "match = re.match(pattern, text, re.DOTALL)\n",
    "\n",
    "if match:\n",
    "    print(\"Match found!\")\n",
    "else:\n",
    "    print(\"No match.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a60f90e-dceb-4ff5-b155-adbc6a2c9a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOConfig\n",
    "param_size = \"0.5B\"\n",
    "batch_size = 1\n",
    "model_name = f\"Qwen/Qwen2.5-{param_size}-Instruct\"\n",
    "\n",
    "output_dir=f\"outputs/Qwen-{param_size}-GRPO\"\n",
    "run_name=f\"QWEN-{param_size}-GRPO-gsm8k-1\"\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=output_dir,\n",
    "    run_name=run_name,\n",
    "    learning_rate=5e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = 'cosine',\n",
    "    logging_steps=1,\n",
    "    bf16=True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_generations=16,\n",
    "    max_prompt_length=256,\n",
    "    max_completion_length=200,\n",
    "    num_train_epochs=1,\n",
    "    save_steps=100,\n",
    "    max_grad_norm=0.1,\n",
    "    log_on_each_node=False,\n",
    "    use_vllm=False,\n",
    "    vllm_gpu_memory_utilization=0.3,\n",
    "    vllm_device='cuda:0',\n",
    "    report_to='wandb',\n",
    ")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab65df2e-a7b8-406a-beb4-fa933dba42c6",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0251fd4b-5247-4255-adfb-e9f299fbcf0c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-23 15:01:38,098] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/samtang/miniconda3/envs/rl/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-23 15:01:38 __init__.py:183] Automatically detected platform cuda.\n",
      "Launching training on one GPU.\n",
      "param= 0.5B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdruidlangde\u001b[0m (\u001b[33mdruidlangde-tencent\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/samtang/export/rl_learning/llm/wandb/run-20250223_150141-cfw80xk3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/druidlangde-tencent/huggingface/runs/cfw80xk3' target=\"_blank\">QWEN-0.5B-GRPO-gsm8k-1</a></strong> to <a href='https://wandb.ai/druidlangde-tencent/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/druidlangde-tencent/huggingface' target=\"_blank\">https://wandb.ai/druidlangde-tencent/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/druidlangde-tencent/huggingface/runs/cfw80xk3' target=\"_blank\">https://wandb.ai/druidlangde-tencent/huggingface/runs/cfw80xk3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:197: FutureWarning: `num_logits_to_keep` is deprecated and will be removed in version 4.50 for `Qwen2ForCausalLM.forward`. Use `logits_to_keep` instead.\n",
      "  return self.model.forward(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='1868' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   4/1868 00:15 < 4:07:07, 0.13 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import GRPOTrainer\n",
    "from peft import LoraConfig\n",
    "import torch\n",
    "from accelerate import notebook_launcher\n",
    "\n",
    "def main():\n",
    "    peft_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        task_type='CAUSAL_LM'\n",
    "    )\n",
    "    \n",
    "    \n",
    "    trainer = GRPOTrainer(\n",
    "        model=model_name,\n",
    "        processing_class=tokenizer,\n",
    "        reward_funcs=[\n",
    "            xml_count_reward_func,\n",
    "            soft_format_reward_func,\n",
    "            strict_format_reward_func,\n",
    "            int_reward_func,\n",
    "            correctness_reward_func\n",
    "        ],\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        peft_config=peft_config,\n",
    "        # peft_config\n",
    "    )\n",
    "    \n",
    "    print('param=', param_size)\n",
    "    trainer.train()\n",
    "    \n",
    "    trainer.save_model(output_dir)\n",
    "\n",
    "notebook_launcher(main, args=(), num_processes=1, mixed_precision='bf16')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5e5b3b-fd8d-4168-9b81-5e3371e39f7f",
   "metadata": {},
   "source": [
    "## eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c03ca04-141f-4c96-92b9-bb17174e89ff",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'prompt'],\n",
       "    num_rows: 1319\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset = get_gsm8k_questions(split='test')\n",
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438b1e03-568b-437b-99e7-79237af0ed2b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import GRPOTrainer\n",
    "\n",
    "output_dir = 'outputs/Qwen-0.5B-GRPO'\n",
    "model = AutoModelForCausalLM.from_pretrained(output_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "evaluator = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[\n",
    "        xml_count_reward_func,\n",
    "        soft_format_reward_func,\n",
    "        strict_format_reward_func,\n",
    "        int_reward_func,\n",
    "        correctness_reward_func\n",
    "    ],\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    # peft_config=peft_config,\n",
    "    # peft_config\n",
    ")\n",
    "\n",
    "metrics = evaluator.evaluate()\n",
    "metrics['eval_samples'] = len(eval_dataset)\n",
    "evaluator.log_metrics('eval', metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfafba93-6c1d-499c-ab97-33ab12485977",
   "metadata": {},
   "source": [
    "## inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ce6dde4-709e-428e-8fd6-37a94b65c870",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vllm import LLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "output_dir = 'outputs/Qwen-0.5B-GRPO'\n",
    "model = AutoModelForCausalLM.from_pretrained(output_dir).to('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e279dbe8-0018-434d-9f16-5b65270de668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "输入: \n",
      "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
      "\n",
      "输出:\n",
      "<|im_start|>system\n",
      "\n",
      "Responde in the following format:\n",
      "<reasoning>\n",
      "...\n",
      "</reasoning>\n",
      "<answer>\n",
      "...\n",
      "</answer>\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<reasoning>\n",
      "Natalia sold clips to 48 friends in April, so she sold half that amount in May, which is \\( \\frac{1}{2} \\times 48 = 24 \\) clips.\n",
      "\n",
      "To find out how many clips Natalia sold altogether in April and May, we add the number of clips sold in both months: \\( 48 + 24 = 72 \\).\n",
      "\n",
      "</reasoning>\n",
      "<answer>\n",
      "72\n",
      "</answer>\n",
      "72<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "import torch\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Responde in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "def generate_with_stream(input_text):\n",
    "    print(f\"\\n输入: \\n{input_text}\")\n",
    "    print(\"\\n输出:\")\n",
    "\n",
    "    prompts = [\n",
    "        {\"role\":\"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\":\"user\", \"content\":input_text},\n",
    "    ]\n",
    "    chats = tokenizer.apply_chat_template(prompts, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(chats, return_tensors=\"pt\").to(\"cuda\")\n",
    "    streamer = TextStreamer(tokenizer)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=512,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            streamer=streamer\n",
    "        )\n",
    "    \n",
    "    # 完整结果\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 使用\n",
    "input_text = \"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\"\n",
    "generate_with_stream(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecd7b72-d165-40ec-992d-f80be30b0d2c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## test util\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ebb5633b-efdc-4569-8806-fd168f13c554",
   "metadata": {},
   "source": [
    "s0 = torch.randn(2,3,5)\n",
    "s1 = torch.randn(2,3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "503ad16d-8c72-4345-b38d-60435d528237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 5])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([s0,s1], dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "663771c5-0b17-414b-b775-a6ed08286a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 5])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([s0, s1],dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a85fbbc-b970-4585-88e3-d8777558b999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: torch.Size([2, 3, 10])\n",
      "hn shape: torch.Size([1, 2, 10])\n",
      "cn shape: torch.Size([1, 2, 10])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "# 定义 LSTM\n",
    "lstm = nn.LSTM(input_size=5, hidden_size=10, num_layers=1, batch_first=True)\n",
    "\n",
    "# 生成随机输入 (batch_size=2, seq_len=3, input_size=5)\n",
    "x = torch.randn(2, 3, 5)\n",
    "\n",
    "# 前向传播\n",
    "output, (hn, cn) = lstm(x)\n",
    "\n",
    "print(f\"output shape: {output.shape}\")  # (2, 3, 10)\n",
    "print(f\"hn shape: {hn.shape}\")          # (1, 2, 10)\n",
    "print(f\"cn shape: {cn.shape}\")          # (1, 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16bbf2d-4246-4655-a8fe-3fc0b7d474bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
