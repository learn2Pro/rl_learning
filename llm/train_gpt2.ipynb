{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "345f723b-e0b1-4b4a-9ff0-c1ac4639831e",
   "metadata": {},
   "source": [
    "```\n",
    "Reference code for GPT-2 training and inference.\n",
    "Will save the model weights into files, to be read from C as initialization.\n",
    "\n",
    "References:\n",
    "1) the official GPT-2 TensorFlow implementation released by OpenAI:\n",
    "https://github.com/openai/gpt-2/blob/master/src/model.py\n",
    "2) huggingface/transformers PyTorch implementation:\n",
    "https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n",
    "\n",
    "Example launches to only benchmark the speed of bfloat16 compiled GPU training:\n",
    "1 GPU:\n",
    "python train_gpt2.py --write_tensors=0 --num_iterations=50 --sequence_length=1024 --compile=1 --tensorcores=1 --dtype=bfloat16\n",
    "you can also turn on flash-attention by appending --flash=1\n",
    "4 GPU:\n",
    "torchrun --standalone --nproc_per_node=4 train_gpt2.py --write_tensors=0 --num_iterations=50 --sequence_length=1024 --compile=1 --tensorcores=1 --dtype=bfloat16\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3662cadb-aa39-4316-acb2-46b0675f084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import glob\n",
    "import struct\n",
    "import inspect\n",
    "from contextlib import nullcontext\n",
    "from dataclasses import dataclass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81e05844-3ed6-4542-a834-7fe55cb27208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28066/35563342.py:9: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import ZeroRedundancyOptimizer\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import torch._inductor.config as config\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from torch.distributed.optim import ZeroRedundancyOptimizer\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47f50bd-70a8-406f-91bf-f380d8d3ed8d",
   "metadata": {},
   "source": [
    "## module in gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9798384b-e63c-47c3-8bea-4800b450945a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewGELU(nn.Module):\n",
    "    \"\"\"Careful there are a few versions of GeLU, this one is the exact one used by OpenAI\"\"\"\n",
    "    def forward(self, input):\n",
    "        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0/math.pi) * (input + 0.044715 * torch.pow(input \\\n",
    "                                                                         , 3.0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5148fe05-de06-412f-8d9a-a93b5c2b0843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gelu = NewGELU()\n",
    "# gelu(torch.randn(3,4))\n",
    "FLASH = 0\n",
    "# ts = torch.tril(torch.ones(4,4))\n",
    "# q,k = ts.split(2, dim=1) \n",
    "# q,k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89679d68-8892-4cd5-9040-3fe9e252c8c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 12, 12])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = torch.randn(4, 2, 12, 4)\n",
    "k = torch.randn(4, 2, 12, 4)\n",
    "(q @ k.transpose(-2, -1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f93d2918-0d5c-472f-990c-7bef75b610f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.LLMC_RESIDUAL_SCALE_FLAG = 1\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        # not really a 'bias', more of a mask, but following the OpenAI/HF naming through\n",
    "        self.register_buffer('bias', torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                             .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch, seq_len, n_embd\n",
    "        qkv = self.c_attn(x) # batch, seq_len, 3*n_embd\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2) # batch, seq_len, n_embd\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        if FLASH:\n",
    "            y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            # materialize the (T, T) matrix\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) # (B, nh, T, T)\n",
    "            att = att.masked_fill(self.bias[:, :, :T, :T] ==0, float('-inf')) # (B, nh, T, T)\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            y = att @ v # (B, nh, T, T)@(B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "                                               \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ea47d14-4846-45af-8665-b378e8caf357",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4*config.n_embd)\n",
    "        self.gelu = NewGELU()\n",
    "        self.c_proj = nn.Linear(4*config.n_embd, config.n_embd)\n",
    "        self.c_proj.LLMC_RESIDUAL_SCALE_FLAG = 1\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06be8403-8ac0-43c2-81db-cb855a08a1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "    def forward(self, x):\n",
    "        x = x+self.attn(self.ln_1(x))\n",
    "        x = x+self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74955579-4b5e-4d1b-a907-b6491bf0591f",
   "metadata": {},
   "source": [
    "## main module of GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b961ff61-9783-4a83-961b-a25a3af2531e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50527\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f621431d-5607-4c28-bae3-d27c482fa09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.lm_head.LLMC_SKIP_INIT = 1 # don't init this one, we will tie weights\n",
    "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "        # init all weights, use a torch rng object to be very careful\n",
    "        self.init_rng = torch.Generator()\n",
    "        self.init_rng.manual_seed(42)\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02 if not hasattr(module, 'LLMC_RESIDUAL_SCALE_FLAG') else 0.02/math.sqrt(2*self.config.n_layer)\n",
    "            # we want to skip initializing lm_head, which shares parameters with wte\n",
    "            # and wte was already initialized down below during the embedding init\n",
    "            if not hasattr(module, 'LLMC_SKIP_INIT'):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=std, generator=self.init_rng)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02, generator=self.init_rng)\n",
    "\n",
    "    def forward(self, idx, targets=None, return_logits=True):\n",
    "        device = idx.device\n",
    "        b, t = idx.size() # batch, seq_len\n",
    "        assert t<=self.config.block_size, f\"Cannot foward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) \n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = tok_emb + pos_emb\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            loss = None\n",
    "\n",
    "        if not return_logits:\n",
    "            logits = None\n",
    "        return logits, loss\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2','gpt2-medium','gpt2-large','gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(f\"Loading weights from pretrained gpt: {model_type}\")\n",
    "\n",
    "        # n_layer, n_head, n_embd\n",
    "        config_args = {\n",
    "            'gpt2': dict(n_layer=12,n_head=12,n_embd=768), # 124m \n",
    "            'gpt2-medium':  dict(n_layer=24,n_head=16,n_embd=1024), # 350m\n",
    "            'gpt2-large':  dict(n_layer=36,n_head=20,n_embd=1280), # 774m\n",
    "            'gpt2-xl':  dict(n_layer=48,n_head=25,n_embd=1600), # 1558m\n",
    "        }[model_type]\n",
    "\n",
    "        config_args['vocab_size'] = 50257\n",
    "        config_args['block_size'] = 1024\n",
    "\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask\n",
    "\n",
    "        # init a huggingface transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')]\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')]\n",
    "\n",
    "        transposed = ['attn.c_attn.weight','attn.c_proj.weight','mlp.c_fc.weight','mlp.c_proj.weight']\n",
    "\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys:{len(sd_keys_hf)}!={len(sd_keys)}\"\n",
    "        if k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hk[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hk[k])\n",
    "        return model\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type, zero_stage):\n",
    "        # start with all of the candidate parameters\n",
    "        param_dict = {pn:p for pn,p in self.named_parameters()}\n",
    "        param_dict = {pn:p for pn,p in param_dict.items() if p.requires_grad}\n",
    "\n",
    "        # create optim groups, Any parameters that is 2D will the weight decayed, otherwise no.\n",
    "        # all wieght tensors in matmuls + embeddings decay, all biases and layernorms don't\n",
    "\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim()>=2]\n",
    "        nodecay_params = [p for n,p in param_dict.items() if p.dim()<2]\n",
    "\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay':weight_decay},\n",
    "            {'params':nodecay_params, 'weight_decay':0.0},\n",
    "        ]\n",
    "\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameters tensors: {len(decay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        \n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "        if zero_stage == 1:\n",
    "            print(\"using ZeroRedundancyOptimizer\")\n",
    "            optimizer = ZeroRedundancyOptimizer(**optim_groups[0], optimizer_class=torch.optim.AdamW, lr=learing_rate, betas=betas, fused=use_fused)\n",
    "            optimizer.add_param_group(optim_groups[1])\n",
    "        else:\n",
    "            print(\"using regular AdamW\")\n",
    "            optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, fused=use_fused)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond) # B, T, vocab_size\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('-inf')\n",
    "\n",
    "            # apply softmax to convert logits to normalized probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # B, vocab_size\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            idx = torch.cat((idx, idx_next), dim=-1)\n",
    "            return idx\n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7521a9f6-67e5-4287-b597-ee25d9f628b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _peek_data_shard(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n",
    "    if header[0]!=20240520:\n",
    "        print(\"ERROR: magic number mismatch in the data .bin file!\")\n",
    "        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n",
    "        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n",
    "        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n",
    "        exit(1)\n",
    "    assert header[1] == 1, \"unsupported version\"\n",
    "    ntok = header[2] # number of tokens (claimed)\n",
    "    return ntok # for now just return the number of tokens\n",
    "\n",
    "def _load_data_shard(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        # first read the header, which is 256 int32 integers (4 bytes each)\n",
    "        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n",
    "        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n",
    "        assert header[1] == 1, \"unsupported version\"\n",
    "        ntok = header[2] # number of tokens (claimed)\n",
    "        # the rest of it are tokens, stored as uint16\n",
    "        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n",
    "    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n",
    "    return tokens\n",
    "        \n",
    "\n",
    "\n",
    "class DistributedDataLoader:\n",
    "    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n",
    "        self.process_rank = process_rank\n",
    "        self.num_processes = num_processes\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # glob files that match the pattern\n",
    "        self.files = sorted(glob.glob(filename_pattern))\n",
    "        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n",
    "\n",
    "        # load and validate all data shards, count number of tokens in total\n",
    "        ntok_total = 0\n",
    "        for fname in self.files:\n",
    "            shard_ntok = _peek_data_shard(fname)\n",
    "            assert shard_ntok >= num_processes * B * T + 1\n",
    "            ntok_total += shard_ntok\n",
    "        self.ntok_total = ntok_total\n",
    "        print0(f\"DataLoader: total number of tokens: {ntok_total:,} across {len(self.files)} files\")\n",
    "\n",
    "        # kick things off\n",
    "        self.current_shard = None\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # we're being a bit clever here: if we already had shard 0 loaded,\n",
    "        # then don't do the work to reload it, just reset the pointer\n",
    "        if self.current_shard != 0:\n",
    "            self.current_shard = 0\n",
    "            self.tokens = _load_data_shard(self.files[self.current_shard])\n",
    "        self.current_position = self.process_rank * self.B * self.T\n",
    "\n",
    "    def advance(self): # advance to next data shard\n",
    "        self.current_shard = (self.current_shard + 1) % len(self.files)\n",
    "        self.current_position = self.process_rank * self.B * self.T\n",
    "        self.tokens = _load_data_shard(self.files[self.current_shard])\n",
    "\n",
    "    def next_batch(self):\n",
    "        B = self.B\n",
    "        T = self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
    "        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n",
    "        x = (buf[:-1]).view(B, T) # inputs\n",
    "        y = (buf[1:]).view(B, T) # targets\n",
    "        # advance the start pointer in current shard\n",
    "        self.current_position += B * T * self.num_processes\n",
    "        # if loading the next batch would be out of bounds advance the shard\n",
    "        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n",
    "            self.advance()\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e196abed-4e56-4899-a826-1f2dd3e6024a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Python -> C bridge utilities for saving params/grads/activations to .bin files\n",
    "\n",
    "def write_fp32(tensor, file):\n",
    "    t = tensor.detach().cpu().to(torch.float32)\n",
    "    b = t.numpy().tobytes()\n",
    "    file.write(b)\n",
    "\n",
    "def write_bf16(tensor, file):\n",
    "    t = tensor.detach().cpu().to(torch.bfloat16)\n",
    "    # numpy doesn't have bf16 datatype so we have to trick it\n",
    "    t = t.view(torch.int16) # trick: reinterpret as int16\n",
    "    b = t.numpy().tobytes()\n",
    "    file.write(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cb46022-f960-49eb-8a00-49df8e33583a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tensors(model_tensors, L, file, dtype):\n",
    "    # writes the GPT-2 model's weights to a binary file\n",
    "    assert dtype in {\"float32\", \"bfloat16\"}\n",
    "    write_fun = write_fp32 if dtype == \"float32\" else write_bf16\n",
    "    write_fun(model_tensors[\"transformer.wte.weight\"], file) # (V, C)\n",
    "    write_fun(model_tensors[\"transformer.wpe.weight\"], file) # (T, C)\n",
    "    for i in range(L): # (L, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.ln_1.weight\"], file)\n",
    "    for i in range(L): # (L, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.ln_1.bias\"], file)\n",
    "    for i in range(L): # (L, 3C, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.attn.c_attn.weight\"], file)\n",
    "    for i in range(L): # (L, 3C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.attn.c_attn.bias\"], file)\n",
    "    for i in range(L): # (L, C, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.attn.c_proj.weight\"], file)\n",
    "    for i in range(L): # (L, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.attn.c_proj.bias\"], file)\n",
    "    for i in range(L): # (L, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.ln_2.weight\"], file)\n",
    "    for i in range(L): # (L, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.ln_2.bias\"], file)\n",
    "    for i in range(L): # (L, 4C, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.mlp.c_fc.weight\"], file)\n",
    "    for i in range(L): # (L, 4C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.mlp.c_fc.bias\"], file)\n",
    "    for i in range(L): # (L, C, 4C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.mlp.c_proj.weight\"], file)\n",
    "    for i in range(L): # (L, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.mlp.c_proj.bias\"], file)\n",
    "    write_fun(model_tensors[\"transformer.ln_f.weight\"], file) # (C, )\n",
    "    write_fun(model_tensors[\"transformer.ln_f.bias\"], file) # (C, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cd2ae75-00c7-457b-9869-00dd6758d6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def pad_vocab(tensor, multiple=128, value=0):\n",
    "    \"\"\"\n",
    "    the dimension of the vocab size in GPT-2 is 50,257\n",
    "    which is unfortunatedly a very unfriendly number for a lot of \n",
    "    matrix operations on the GPU. So we pad it to the nearest\n",
    "    friendlier multiple, e.g. 50,304 if multiple=128 when we export the weights into\n",
    "    C land. This is a NOOP algorithmeically and is only done to make the tensor operations more efficiently\n",
    "    \"\"\"\n",
    "    assert tensor.ndim == 2\n",
    "    V, C = tensor.shape\n",
    "    assert V == 50257, \"just being defensive here\"\n",
    "    Vp = ((V + multiple -1) // multiple) * multiple\n",
    "    # pad the tensor\n",
    "    pad_ros = Vp - V\n",
    "    padded = tensor if pad_rows == 0 else F.pad(tensor, (0,0,0,pad_rows), value=value)\n",
    "    assert padded.shape == (Vp, C)\n",
    "    return padded\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f879f69-ef56-4f03-84bc-eca60665e687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_model(model, filename, dtype):\n",
    "    # everything we need to instantiate the model\n",
    "    assert dtype in {\"float32\", \"bfloat16\"} # float16 todo maybe later\n",
    "    version = {\n",
    "        \"float32\": 3, # 3: all tensors are fp32, padded vocab\n",
    "        \"bfloat16\": 5, # 5: all tensors are bf16, padded vocab\n",
    "    }[dtype]\n",
    "    header = torch.zeros(256, dtype=torch.int32)\n",
    "    header[0] = 20240326 # magic\n",
    "    header[1] = version # checkpoint version\n",
    "    header[2] = model.config.block_size\n",
    "    header[3] = model.config.vocab_size\n",
    "    header[4] = model.config.n_layer\n",
    "    header[5] = model.config.n_head\n",
    "    header[6] = model.config.n_embd\n",
    "    # 2) the parameters follow the header\n",
    "    params = {name: param.cpu() for name, param in model.named_parameters()}\n",
    "    # pad the vocab to a multiple of 128 here at export, for efficiency in C\n",
    "    wte = params[\"transformer.wte.weight\"] # (V, C)\n",
    "    wte_padded = pad_vocab(wte) # (Vp, C)\n",
    "    params[\"transformer.wte.weight\"] = wte_padded # (Vp, C)\n",
    "    print(f\"padded vocab size from {wte.size(0)} to {wte_padded.size(0)}\")\n",
    "    header[7] = wte_padded.size(0) # padded vocab size store in header\n",
    "    # now write to file\n",
    "    with open(filename, \"wb\") as file:\n",
    "        file.write(header.numpy().tobytes()) # header\n",
    "        write_tensors(params, model.config.n_layer, file, dtype) # params\n",
    "    print(f\"wrote {filename}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04f32335-0b52-4566-bff1-d20df1d73e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_state(model, x, y, logits, loss, filename):\n",
    "    # the state is used for debugging.\n",
    "    # it contains information about the input, logits, loss, and the parameter gradients\n",
    "    # this can be used for checking the computation correctness in C\n",
    "    header = torch.zeros(256, dtype=torch.int32)\n",
    "    header[0] = 20240327 # magic\n",
    "    header[1] = 2 # run state version = 2 (1 -> 2 for padded vocab changes)\n",
    "    header[2] = x.size(0) # batch size of the batch, B\n",
    "    header[3] = x.size(1) # temporal extent of the batch, T\n",
    "    grads = {name: param.grad.cpu() for name, param in model.named_parameters()}\n",
    "    # pad the vocab grads here as well, to mirror write_model\n",
    "    wte_grad = grads[\"transformer.wte.weight\"] # (V, C)\n",
    "    wte_grad_padded = pad_vocab(wte_grad, value=0) # (Vp, C) # TODO later maybe pad with nan?\n",
    "    grads[\"transformer.wte.weight\"] = wte_grad_padded # (Vp, C)\n",
    "    print(f\"padded vocab size in reference grads from {wte_grad.size(0)} to {wte_grad_padded.size(0)}\")\n",
    "    with open(filename, \"wb\") as file:\n",
    "        # header\n",
    "        file.write(header.numpy().tobytes())\n",
    "        # input x\n",
    "        file.write(x.cpu().numpy().astype(\"int32\").tobytes()) # (B, T)\n",
    "        # targets y\n",
    "        file.write(y.cpu().numpy().astype(\"int32\").tobytes()) # (B, T)\n",
    "        # logits (result of the model forward pass)\n",
    "        write_fp32(logits.cpu(), file)\n",
    "        # loss (single float, result of the cross entropy loss)\n",
    "        write_fp32(loss.cpu(), file)\n",
    "        # gradients\n",
    "        write_tensors(grads, model.config.n_layer, file, \"float32\")\n",
    "    print(f\"wrote {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ae105f1-7591-4856-84d5-baac35411da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tokenizer(enc, filename):\n",
    "    n = enc.max_token_value + 1\n",
    "    header = torch.zeros(256, dtype=torch.int32)\n",
    "    header[0] = 20240328 # magic\n",
    "    header[1] = 2 # tokenizer version = 2 (1 -> 2: includes EOT token)\n",
    "    header[2] = n # number of tokens\n",
    "    header[3] = enc.eot_token # EOT token\n",
    "    with open(filename, \"wb\") as file:\n",
    "        file.write(header.numpy().tobytes())\n",
    "        for i in range(n):\n",
    "            b = enc.decode_bytes([i])\n",
    "            length = len(b)\n",
    "            assert length < 256, f\"Token length exceeds 255: {length}\"\n",
    "            file.write(struct.pack(\"<B\", length))  # Write the length as a 1-byte unsigned integer\n",
    "            file.write(b)  # Write the actual bytes\n",
    "    print(f\"wrote {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65ce8517-d41e-40c8-b659-e5ff3b92efa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print0(*args, **kwargs):\n",
    "    # modified print that only prints from the master process\n",
    "    # if this is not a distributed run, it's just a print\n",
    "    if int(os.environ.get(\"RANK\", 0)) == 0:\n",
    "        print(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a4f1ccc-339d-4f4e-97d5-2aa08a8ea55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running pytorch 2.5.1+cu124\n",
      "using device: cuda\n",
      "4096 4096\n",
      "total desired batch size: 4096\n",
      "=> calculated gradient accumulation steps: 1\n",
      "Loading weights from pretrained gpt: gpt2\n",
      "compiling the model...\n",
      "DataLoader: total number of tokens: 10,251,184,259 across 103 files\n",
      "DataLoader: total number of tokens: 100,000,000 across 1 files\n",
      "num decayed parameter tensors: 50, with 124,318,464 parameters\n",
      "num non-decayed parameters tensors: 50, with 121,344 parameters\n",
      "using fused AdamW: True\n",
      "using regular AdamW\n",
      "step    1/50 | train loss 11.025171 | norm 16.9545 | lr 1.00e-04 | (4531.45 ms | 904 tok/s)\n",
      "step    2/50 | train loss 9.992307 | norm 6.2839 | lr 1.00e-04 | (39.17 ms | 104563 tok/s)\n",
      "step    3/50 | train loss 9.658263 | norm 3.1005 | lr 1.00e-04 | (38.67 ms | 105914 tok/s)\n",
      "step    4/50 | train loss 9.540375 | norm 3.3843 | lr 1.00e-04 | (38.74 ms | 105723 tok/s)\n",
      "step    5/50 | train loss 9.338705 | norm 2.5642 | lr 1.00e-04 | (38.72 ms | 105783 tok/s)\n",
      "step    6/50 | train loss 9.201044 | norm 2.4256 | lr 1.00e-04 | (38.65 ms | 105979 tok/s)\n",
      "step    7/50 | train loss 8.992818 | norm 61.5732 | lr 1.00e-04 | (38.76 ms | 105685 tok/s)\n",
      "step    8/50 | train loss 8.935497 | norm 2.8187 | lr 1.00e-04 | (41.86 ms | 97853 tok/s)\n",
      "step    9/50 | train loss 8.773244 | norm 2.8772 | lr 1.00e-04 | (38.82 ms | 105502 tok/s)\n",
      "step   10/50 | train loss 8.658606 | norm 21.1418 | lr 1.00e-04 | (38.87 ms | 105376 tok/s)\n",
      "step   11/50 | train loss 8.449128 | norm 3.5752 | lr 1.00e-04 | (38.80 ms | 105568 tok/s)\n",
      "step   12/50 | train loss 8.277040 | norm 4.1137 | lr 1.00e-04 | (39.96 ms | 102503 tok/s)\n",
      "step   13/50 | train loss 8.079712 | norm 4.9488 | lr 1.00e-04 | (39.72 ms | 103111 tok/s)\n",
      "step   14/50 | train loss 8.014546 | norm 22.0902 | lr 1.00e-04 | (39.77 ms | 102990 tok/s)\n",
      "step   15/50 | train loss 7.706540 | norm 6.4271 | lr 1.00e-04 | (38.71 ms | 105810 tok/s)\n",
      "step   16/50 | train loss 7.489631 | norm 7.0799 | lr 1.00e-04 | (38.68 ms | 105892 tok/s)\n",
      "step   17/50 | train loss 7.243332 | norm 7.5020 | lr 1.00e-04 | (38.81 ms | 105531 tok/s)\n",
      "step   18/50 | train loss 6.961356 | norm 6.9093 | lr 1.00e-04 | (38.70 ms | 105843 tok/s)\n",
      "step   19/50 | train loss 6.734302 | norm 11.1952 | lr 1.00e-04 | (40.33 ms | 101554 tok/s)\n",
      "step   20/50 | train loss 6.508691 | norm 9.9870 | lr 1.00e-04 | (38.86 ms | 105407 tok/s)\n",
      "step   21/50 | train loss 6.271691 | norm 9.6006 | lr 1.00e-04 | (38.79 ms | 105598 tok/s)\n",
      "step   22/50 | train loss 6.167431 | norm 16.8671 | lr 1.00e-04 | (38.77 ms | 105655 tok/s)\n",
      "step   23/50 | train loss 5.894759 | norm 7.3359 | lr 1.00e-04 | (38.73 ms | 105754 tok/s)\n",
      "step   24/50 | train loss 5.696271 | norm 10.8899 | lr 1.00e-04 | (38.82 ms | 105507 tok/s)\n",
      "step   25/50 | train loss 5.494215 | norm 10.5952 | lr 1.00e-04 | (39.79 ms | 102934 tok/s)\n",
      "step   26/50 | train loss 5.296608 | norm 10.6139 | lr 1.00e-04 | (38.91 ms | 105271 tok/s)\n",
      "step   27/50 | train loss 5.083818 | norm 9.2772 | lr 1.00e-04 | (40.56 ms | 100997 tok/s)\n",
      "step   28/50 | train loss 4.920098 | norm 10.8114 | lr 1.00e-04 | (38.73 ms | 105753 tok/s)\n",
      "step   29/50 | train loss 4.699363 | norm 7.8695 | lr 1.00e-04 | (38.80 ms | 105576 tok/s)\n",
      "step   30/50 | train loss 4.543493 | norm 11.3649 | lr 1.00e-04 | (38.77 ms | 105637 tok/s)\n",
      "step   31/50 | train loss 4.329465 | norm 8.3942 | lr 1.00e-04 | (39.62 ms | 103381 tok/s)\n",
      "step   32/50 | train loss 4.152763 | norm 9.0112 | lr 1.00e-04 | (41.14 ms | 99573 tok/s)\n",
      "step   33/50 | train loss 3.944289 | norm 7.4153 | lr 1.00e-04 | (40.35 ms | 101517 tok/s)\n",
      "step   34/50 | train loss 3.769891 | norm 9.5035 | lr 1.00e-04 | (38.76 ms | 105685 tok/s)\n",
      "step   35/50 | train loss 3.607675 | norm 11.2451 | lr 1.00e-04 | (38.65 ms | 105986 tok/s)\n",
      "step   36/50 | train loss 3.418350 | norm 9.5811 | lr 1.00e-04 | (38.67 ms | 105922 tok/s)\n",
      "step   37/50 | train loss 3.246192 | norm 10.7639 | lr 1.00e-04 | (39.65 ms | 103302 tok/s)\n",
      "step   38/50 | train loss 3.130814 | norm 14.7643 | lr 1.00e-04 | (39.20 ms | 104492 tok/s)\n",
      "step   39/50 | train loss 2.959094 | norm 13.2412 | lr 1.00e-04 | (38.72 ms | 105796 tok/s)\n",
      "step   40/50 | train loss 2.806264 | norm 12.5605 | lr 1.00e-04 | (38.69 ms | 105871 tok/s)\n",
      "step   41/50 | train loss 2.637570 | norm 9.9409 | lr 1.00e-04 | (38.66 ms | 105950 tok/s)\n",
      "step   42/50 | train loss 2.508870 | norm 11.6787 | lr 1.00e-04 | (39.52 ms | 103657 tok/s)\n",
      "step   43/50 | train loss 2.370095 | norm 12.2541 | lr 1.00e-04 | (40.02 ms | 102346 tok/s)\n",
      "step   44/50 | train loss 2.231709 | norm 11.7322 | lr 1.00e-04 | (39.84 ms | 102807 tok/s)\n",
      "step   45/50 | train loss 2.103959 | norm 12.4357 | lr 1.00e-04 | (38.76 ms | 105672 tok/s)\n",
      "step   46/50 | train loss 1.964561 | norm 11.9007 | lr 1.00e-04 | (38.79 ms | 105586 tok/s)\n",
      "step   47/50 | train loss 1.855089 | norm 12.8779 | lr 1.00e-04 | (38.76 ms | 105670 tok/s)\n",
      "step   48/50 | train loss 1.711756 | norm 10.8838 | lr 1.00e-04 | (38.86 ms | 105400 tok/s)\n",
      "step   49/50 | train loss 1.624893 | norm 13.0730 | lr 1.00e-04 | (39.10 ms | 104753 tok/s)\n",
      "step   50/50 | train loss 1.474401 | norm 10.3622 | lr 1.00e-04 | (39.67 ms | 103256 tok/s)\n",
      "final 19 iters avg: 39.252ms\n",
      "peak memory consumption: 3853 MiB\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import time\n",
    "    import argparse\n",
    "    import tiktoken\n",
    "    print0(f\"Running pytorch {torch.version.__version__}\")\n",
    "\n",
    "    # default settings will overfit a tiny batch of data\n",
    "    # and save model weights and debug state to disk on the first iteration\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # file system input / output\n",
    "    parser.add_argument(\"--input_bin\", type=str, default=\"tinyshakespeare/tiny_shakespeare_val.bin\", help=\"input .bin to train on\")\n",
    "    parser.add_argument(\"--input_val_bin\", type=str, default=\"\", help=\"input .bin to eval validation loss on\")\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=\"\", help=\"output directory to which to write logs and checkpoints\")\n",
    "    parser.add_argument(\"--model\", type=str, default=\"gpt2\", help=\"gpt2|gpt2-medium|gpt2-large|gpt2-xl|d12|d24|d36|d48\")\n",
    "    # token layout for each step of the optimization\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=4, help=\"batch size, in units of #batch dimensions\")\n",
    "    parser.add_argument(\"--sequence_length\", type=int, default=64, help=\"sequence length\")\n",
    "    parser.add_argument(\"--total_batch_size\", type=int, default=4096, help=\"total desired batch size, in units of #tokens\")\n",
    "    # workload (number of steps)\n",
    "    parser.add_argument(\"--num_iterations\", type=int, default=10, help=\"number of iterations to run\")\n",
    "    parser.add_argument(\"--inference_only\", type=int, default=0, help=\"only run inference\")\n",
    "    # optimization\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=1e-4, help=\"learning rate warmup iterations\")\n",
    "    parser.add_argument(\"--warmup_iters\", type=int, default=0, help=\"learning rate warmup iterations\")\n",
    "    parser.add_argument(\"--learning_rate_decay_frac\", type=float, default=1.0, help=\"learning rate warmup iterations\")\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=0.0, help=\"weight decay\")\n",
    "    parser.add_argument(\"--grad_clip\", type=float, default=1.0, help=\"maximum gradient magnitude\")\n",
    "    # evaluation\n",
    "    parser.add_argument(\"--val_loss_every\", type=int, default=0, help=\"every how mant steps to evaluate val loss?\")\n",
    "    parser.add_argument(\"--val_max_steps\", type=int, default=20, help=\"how many batches of val to average?\")\n",
    "    parser.add_argument(\"--sample_every\", type=int, default=0, help=\"how often to sample from the model?\")\n",
    "    # debugging\n",
    "    parser.add_argument(\"--overfit_single_batch\", type=int, default=1, help=\"overfit just one batch of data\")\n",
    "    # numerics\n",
    "    parser.add_argument(\"--tensorcores\", type=int, default=0, help=\"use tensorcores\")\n",
    "    # memory management\n",
    "    parser.add_argument(\"--device\", type=str, default=\"\", help=\"by default we autodetect, or set it here\")\n",
    "    parser.add_argument(\"--compile\", type=int, default=0, help=\"torch.compile the model\")\n",
    "    parser.add_argument(\"--flash\", type=int, default=0, help=\"use flash attention\")\n",
    "    parser.add_argument(\"--dtype\", type=str, default=\"float32\", help=\"float32|float16|bfloat16\")\n",
    "    parser.add_argument(\"--zero_stage\", type=int, default=0, help=\"zero redundancy optimizer stage (0/1/2/3)\")\n",
    "    # python -> C bridge\n",
    "    parser.add_argument(\"--write_tensors\", type=int, default=1, help=\"write tensors to disk\")\n",
    "#     python train_gpt2.py --write_tensors=0 --num_iterations=50 --sequence_length=1024 --compile=1 --tensorcores=1 --dtype=bfloat16\n",
    "# you can also turn on flash-attention by appending --flash=1\n",
    "    args = parser.parse_args(['--input_bin','fineweb10B/fineweb_train_*.bin', '--input_val_bin', 'fineweb10B/fineweb_val_000000.bin' , '--write_tensors', '0', '--num_iterations', '50', '--sequence_length', '1024', '--compile', '1', \\\n",
    "                              '--tensorcores', '1', '--dtype', 'bfloat16', '--flash', '1'])\n",
    "    # args.set_defaults(model='gpt2')\n",
    "\n",
    "    # args error checking and convenience variables\n",
    "    B, T = args.batch_size, args.sequence_length\n",
    "    assert 1 <= T <= 1024\n",
    "    assert args.dtype in {\"float32\", \"float16\", \"bfloat16\"}\n",
    "    assert args.model in {\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\", \"d12\", \"d24\", \"d36\", \"d48\"}\n",
    "\n",
    "    # set up DDP (distributed data parallel). torchrun sets this env variable\n",
    "    ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
    "    if ddp:\n",
    "        # use of DDP atm demands CUDA, we set the device appropriately according to rank\n",
    "        assert torch.cuda.is_available(), \"for now i think we need CUDA for DDP\"\n",
    "        init_process_group(backend='nccl')\n",
    "        ddp_rank = int(os.environ['RANK'])\n",
    "        ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "        ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "        device = f'cuda:{ddp_local_rank}'\n",
    "        torch.cuda.set_device(device)\n",
    "        master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
    "        seed_offset = 0 # each process gets the exact same seed\n",
    "        zero_stage = args.zero_stage\n",
    "    else:\n",
    "        ddp_rank = 0\n",
    "        ddp_local_rank = 0\n",
    "        zero_stage = 0\n",
    "        ddp_world_size = 1\n",
    "        master_process = True\n",
    "        seed_offset = 0\n",
    "        # select the device\n",
    "        if args.device:\n",
    "            # provided explicitly by the user\n",
    "            device = args.device\n",
    "        else:\n",
    "            # attempt to autodetect the device\n",
    "            device = \"cpu\"\n",
    "            if torch.cuda.is_available():\n",
    "                device = \"cuda\"\n",
    "            elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "                device = \"mps\"\n",
    "    print(f\"using device: {device}\")\n",
    "    device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
    "\n",
    "    # calculate gradient accumulation from the desired total batch size and the current run configuration\n",
    "    tokens_per_fwdbwd = B * T * ddp_world_size\n",
    "    print(args.total_batch_size, tokens_per_fwdbwd)\n",
    "    assert args.total_batch_size % tokens_per_fwdbwd == 0\n",
    "    grad_accum_steps = args.total_batch_size // tokens_per_fwdbwd\n",
    "    print0(f\"total desired batch size: {args.total_batch_size}\")\n",
    "    print0(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n",
    "\n",
    "    # set up a context manager following the desired dtype and device\n",
    "    ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[args.dtype]\n",
    "    ctx = torch.amp.autocast(device_type=device_type, dtype=ptdtype) if device_type == \"cuda\" else nullcontext()\n",
    "\n",
    "    # rng / reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(42)\n",
    "\n",
    "    # set the torch precision mode to use TensorFloat32 (TF32) for matmuls\n",
    "    # docs https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html\n",
    "    if args.tensorcores:\n",
    "        torch.set_float32_matmul_precision('high')\n",
    "\n",
    "    # turn on/off flash attention\n",
    "    assert args.flash in {0, 1}\n",
    "    FLASH = args.flash\n",
    "\n",
    "    # init (and write) the tokenizer\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    if master_process and args.write_tensors: # tokenizer is technically not tensors but ok\n",
    "        write_tokenizer(enc, \"gpt2_tokenizer.bin\")\n",
    "\n",
    "    # init the model, either from scratch or from OpenAI pretrained checkpoint\n",
    "    if args.model[0] == \"d\":\n",
    "        # from scratch (random weights)\n",
    "        model_config = {\n",
    "            \"d12\": GPTConfig(block_size=1024, vocab_size=50257, n_layer=12, n_head=12, n_embd=768),\n",
    "            \"d24\": GPTConfig(block_size=1024, vocab_size=50257, n_layer=24, n_head=16, n_embd=1024),\n",
    "            \"d36\": GPTConfig(block_size=1024, vocab_size=50257, n_layer=36, n_head=20, n_embd=1280),\n",
    "            \"d48\": GPTConfig(block_size=1024, vocab_size=50257, n_layer=48, n_head=25, n_embd=1600),\n",
    "        }[args.model]\n",
    "        model = GPT(model_config)\n",
    "    else:\n",
    "        # load the GPT-2 model weights\n",
    "        model = GPT.from_pretrained(args.model)\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    if args.compile:\n",
    "        if hasattr(config, \"coordinate_descent_tuning\"):\n",
    "            config.coordinate_descent_tuning = True # suggested by @Chillee\n",
    "        print0(\"compiling the model...\")\n",
    "        model = torch.compile(model)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Our own version of a simple DistributedDataLoader\n",
    "\n",
    "    # load tokens\n",
    "    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n",
    "    val_loader = None\n",
    "    if args.input_val_bin:\n",
    "        val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # PyTorch -> C bridge: save some weights and state for C to load later as reference\n",
    "\n",
    "    # do one forward pass to generate ground truth for our C tests\n",
    "    if master_process and args.write_tensors and (not args.inference_only):\n",
    "        x, y = train_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits, loss = model(x, y)\n",
    "        loss.backward()\n",
    "        # save model params, in both float32 and bfloat16\n",
    "        model_to_size = {\"gpt2\": \"124M\", \"gpt2-medium\": \"355M\", \"gpt2-large\": \"774M\", \"gpt2-xl\": \"1558M\"}\n",
    "        model_to_size.update({f\"d{d}\": f\"d{d}\" for d in [12, 24, 36, 48]})\n",
    "        model_size_str = model_to_size[args.model] # e.g. \"124M\", or \"d12\"\n",
    "        write_model(model, f\"gpt2_{model_size_str}.bin\", dtype=\"float32\")\n",
    "        write_model(model, f\"gpt2_{model_size_str}_bf16.bin\", dtype=\"bfloat16\")\n",
    "        # save x, y, logits, loss, and parameter gradients, for debugging C\n",
    "        # always store these in fp32 to have an accurate reference (?)\n",
    "        write_state(model, x, y, logits, loss, f\"gpt2_{model_size_str}_debug_state.bin\")\n",
    "        # reset the train_loader for the optimization below\n",
    "        train_loader.reset()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # main training loop\n",
    "\n",
    "    # here we wrap model into DDP container\n",
    "    if ddp:\n",
    "        model = DDP(model, device_ids=[ddp_local_rank])\n",
    "    raw_model = model.module if ddp else model # always contains the \"raw\" unwrapped model\n",
    "\n",
    "    # init the optimizer\n",
    "    optimizer = raw_model.configure_optimizers(weight_decay=args.weight_decay,\n",
    "                                               learning_rate=args.learning_rate, betas=(0.9, 0.95),\n",
    "                                               device_type=device, zero_stage=zero_stage)\n",
    "\n",
    "    # learning rate decay scheduler (cosine with warmup)\n",
    "    def get_lr(it):\n",
    "        min_lr = args.learning_rate * args.learning_rate_decay_frac\n",
    "        # 1) linear warmup for warmup_iters steps\n",
    "        if it < args.warmup_iters:\n",
    "            return args.learning_rate * (it+1) / args.warmup_iters\n",
    "        # 2) if it > lr_decay_iters, return min learning rate\n",
    "        if it > args.num_iterations:\n",
    "            return min_lr\n",
    "        # 3) in between, use cosine decay down to min learning rate\n",
    "        decay_ratio = (it - args.warmup_iters) / (args.num_iterations - args.warmup_iters)\n",
    "        assert 0 <= decay_ratio <= 1\n",
    "        coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
    "        return min_lr + coeff * (args.learning_rate - min_lr)\n",
    "\n",
    "    # create the logging directory if it does not exist\n",
    "    logfile = None\n",
    "    if args.output_dir:\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "        logfile = os.path.join(args.output_dir, \"main.log\")\n",
    "        # create the log file \"main.log\" inside it, and wipe it clean\n",
    "        with open(logfile, \"w\") as f:\n",
    "            pass\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    timings = []\n",
    "    norm = -1.0   # dummy value to print in inference-only mode\n",
    "    for step in range(args.num_iterations + 1):\n",
    "        t0 = time.time()\n",
    "        last_step = (step == args.num_iterations)\n",
    "\n",
    "        # once in a while evaluate the validation dataset\n",
    "        if (args.val_loss_every > 0 \\\n",
    "            and (step % args.val_loss_every == 0 or last_step)) \\\n",
    "            and (val_loader is not None):\n",
    "            model.eval()\n",
    "            val_loader.reset()\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0.0\n",
    "                for _ in range(args.val_max_steps):\n",
    "                    x, y = val_loader.next_batch()\n",
    "                    x, y = x.to(device), y.to(device)\n",
    "                    _, loss = model(x, y, return_logits=False)\n",
    "                    val_loss += loss.item()\n",
    "                val_loss /= args.val_max_steps\n",
    "            # log to console and to file\n",
    "            print0(f\"val loss {val_loss}\")\n",
    "            if master_process and logfile is not None:\n",
    "                with open(logfile, \"a\") as f:\n",
    "                    f.write(\"s:%d tel:%f\\n\" % (step, val_loss))\n",
    "\n",
    "        # once in a while perform model inference on the master process\n",
    "        if (args.sample_every > 0 \\\n",
    "            and (step % args.sample_every == 0 or last_step)) \\\n",
    "            and master_process:\n",
    "            model.eval()\n",
    "            # before we end, let's also do one round of inference\n",
    "            # we'll kick off the generation with \"<|endoftext|>\", which designates the start of a new sequence\n",
    "            start_ids = [enc.eot_token]\n",
    "            xg = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "            max_new_tokens = 32\n",
    "            temperature = 1.0\n",
    "            top_k = 40\n",
    "            yg = raw_model.generate(xg, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "            print0('---------------')\n",
    "            print0(enc.decode(yg[0].tolist()))\n",
    "            print0('---------------')\n",
    "\n",
    "        # bit confusing: we want to make sure to eval and sample on 0th iteration\n",
    "        # but also after the very last iteration. so we loop for step <= num_iterations\n",
    "        # instead of just < num_iterations (one extra due to <=), only to do\n",
    "        # the validation/sampling one last time, and then we break right here as we're done.\n",
    "        if last_step:\n",
    "            break\n",
    "\n",
    "        # --------------- TRAINING SECTION BEGIN -----------------\n",
    "        model.train()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        # if we are trying to overfit a single batch, we reset the loader here\n",
    "        if args.overfit_single_batch:\n",
    "            train_loader.reset()\n",
    "        # micro-batch loop where we do gradient accumulation to reach desired total batch size\n",
    "        lossf = 0.0 # for getting the mean loss (as simple float) over the accumulation steps\n",
    "        for micro_step in range(grad_accum_steps):\n",
    "            # fetch a batch\n",
    "            x, y = train_loader.next_batch()\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            if ddp:\n",
    "                # we want only the last micro-step to sync grads in a DDP model\n",
    "                # the official way to do this is with model.no_sync(), but that is a\n",
    "                # context manager that bloats the code, so we just toggle this variable\n",
    "                model.require_backward_grad_sync = (micro_step == grad_accum_steps - 1)\n",
    "            # forward pass\n",
    "            with ctx:\n",
    "                _, loss = model(x, y, return_logits=False)\n",
    "                # we have to scale the loss to account for gradient accumulation,\n",
    "                # because the gradients just add on each successive backward().\n",
    "                # addition of gradients corresponds to a SUM in the objective, but\n",
    "                # instead of a SUM we want MEAN, so we scale the loss here\n",
    "                loss = loss / grad_accum_steps\n",
    "                lossf += loss.detach() # keep track of the mean loss\n",
    "            # backward pass\n",
    "            if not args.inference_only:\n",
    "                loss.backward()\n",
    "        if ddp:\n",
    "            dist.all_reduce(lossf, op=dist.ReduceOp.AVG)\n",
    "        lossf = lossf.item()\n",
    "        norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n",
    "        # determine and set the learning rate for this iteration\n",
    "        lr = get_lr(step)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        # step the optimizer\n",
    "        optimizer.step()\n",
    "        # --------------- TRAINING SECTION END -------------------\n",
    "        # everything that follows now is just diagnostics, prints, logging, etc.\n",
    "\n",
    "        # wait on the CPU for all device work to end so we get accurate per-iteration timings below\n",
    "        if device == \"mps\":\n",
    "            torch.mps.synchronize()\n",
    "        elif device == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        # time and print\n",
    "        t1 = time.time()\n",
    "        # the 0th iteration is often an outlier (much slower) => skip logging it\n",
    "        tokens_per_second = grad_accum_steps * ddp_world_size * B * T / (t1-t0)\n",
    "        print0(f\"step {step+1:4d}/{args.num_iterations} | train loss {lossf:.6f} | norm {norm:.4f} | lr {lr:.2e} | ({(t1-t0)*1000:.2f} ms | {tokens_per_second:.0f} tok/s)\")\n",
    "        # log to logile\n",
    "        if master_process and logfile is not None:\n",
    "            with open(logfile, \"a\") as f:\n",
    "                f.write(\"s:%d trl:%f\\n\" % (step, lossf))\n",
    "\n",
    "        # keep track of smooth timings, last 20 iterations\n",
    "        if step > 0 and step > args.num_iterations - 20:\n",
    "            timings.append(t1-t0)\n",
    "\n",
    "    # print the average of the last 20 timings, to get something smooth-ish\n",
    "    timings = timings[-20:]\n",
    "    print0(f\"final {len(timings)} iters avg: {np.mean(timings)*1000:.3f}ms\")\n",
    "    print0(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # clean up nice\n",
    "    if ddp:\n",
    "        destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3610d407-e573-4404-a9fb-50ad42b1c042",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m enc \u001b[38;5;241m=\u001b[39m tiktoken\u001b[38;5;241m.\u001b[39mget_encoding(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m start_ids \u001b[38;5;241m=\u001b[39m [enc\u001b[38;5;241m.\u001b[39meot_token]\n\u001b[0;32m----> 7\u001b[0m xg \u001b[38;5;241m=\u001b[39m (\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m])\n\u001b[1;32m      8\u001b[0m max_new_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m      9\u001b[0m temperature \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# model.eval()\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "start_ids = [enc.eot_token]\n",
    "xg = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "max_new_tokens = 32\n",
    "temperature = 1.0\n",
    "top_k = 40\n",
    "yg = model.generate(xg, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "print0('---------------')\n",
    "print0(enc.decode(yg[0].tolist()))\n",
    "print0('---------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e0c51de4-2729-4be2-b29f-c47311c00cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--epochs\", type=str, default=\"dev/data/tinyshakespeare/tiny_shakespeare_val.bin\", help=\"input .bin to train on\")\n",
    "args = parser.parse_args([\"--epochs\", \"30\"])\n",
    "# args['model'] = 'gpt2'\n",
    "# args.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "66fbc9f5-9a3f-4622-ac32-95862fe1b0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.1605,  0.0077, -0.6231, -0.2747, -1.0238],\n",
      "         [-1.3200, -1.1642, -0.0118,  0.8298, -0.0060],\n",
      "         [ 0.2925, -0.8810,  1.3063, -1.0034, -1.1594]],\n",
      "\n",
      "        [[-0.0797, -1.0448,  0.9792,  0.9146,  0.3367],\n",
      "         [-1.9930, -2.1351,  2.1813,  0.4463, -0.0707],\n",
      "         [ 0.5384, -0.2732, -2.4335,  0.1077,  0.4818]]])\n",
      "tensor([[[ 1.1605,  0.0077, -0.2747],\n",
      "         [ 0.8298, -0.0060, -0.0118],\n",
      "         [ 1.3063,  0.2925, -0.8810]],\n",
      "\n",
      "        [[ 0.9792,  0.9146,  0.3367],\n",
      "         [ 2.1813,  0.4463, -0.0707],\n",
      "         [ 0.5384,  0.4818,  0.1077]]])\n",
      "tensor([[[-0.2747],\n",
      "         [-0.0118],\n",
      "         [-0.8810]],\n",
      "\n",
      "        [[ 0.3367],\n",
      "         [-0.0707],\n",
      "         [ 0.1077]]])\n",
      "tensor([[[ 1.1605,  0.0077,  0.0000, -0.2747,  0.0000],\n",
      "         [ 0.0000,  0.0000, -0.0118,  0.8298, -0.0060],\n",
      "         [ 0.2925, -0.8810,  1.3063,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.9792,  0.9146,  0.3367],\n",
      "         [ 0.0000,  0.0000,  2.1813,  0.4463, -0.0707],\n",
      "         [ 0.5384,  0.0000,  0.0000,  0.1077,  0.4818]]])\n",
      "tensor([[ 0.2925, -0.8810,  1.3063,  0.0000,  0.0000],\n",
      "        [ 0.5384,  0.0000,  0.0000,  0.1077,  0.4818]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.randn(2,3,5)\n",
    "print(logits)\n",
    "v, _ = torch.topk(logits, 3)\n",
    "print(v)\n",
    "logits[logits<v[:,:,[-1]]] = 0.0\n",
    "print(v[:,:,[-1]])\n",
    "print(logits)\n",
    "print(logits[:,-1,:])\n",
    "torch.multinomial(F.softmax(logits[:,-1,:], dim=-1), num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496a9df2-c1f3-42ab-a1be-7def3aad48ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
