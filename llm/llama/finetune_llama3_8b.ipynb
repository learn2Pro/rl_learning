{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: datasets in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (3.1.0)\n",
      "Requirement already satisfied: accelerate in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (1.1.1)\n",
      "Requirement already satisfied: filelock in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from datasets) (2.1.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from datasets) (3.11.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from datasets) (0.26.3)\n",
      "Requirement already satisfied: packaging in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: psutil in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from accelerate) (6.1.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from accelerate) (2.5.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: networkx in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForLanguageModeling, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.1.0'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "datasets.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['HTTP_PROXY'] = 'http://127.0.0.1:7890'\n",
    "os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7890'\n",
    "os.environ['SOCKS_PROXY'] = 'http://127.0.0.1:7890'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HF_HOME'] = 'https://huggingface.mirror.sjtu.edu.cn/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "login(token='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', add_to_git_credential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# model initialize and tokenizer\n",
    "# model_name = 'Qwen/Qwen2.5-7B'\n",
    "model_name = 'Qwen/Qwen2-0.5B'\n",
    "model_path = '../../https://huggingface.mirror.sjtu.edu.cn/hub/models--Qwen--Qwen2-0.5B/snapshots/91d2aff3f957f99e4c74c962f2f408dcc88a18d8'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# config = AutoConfig.for_model(model_type='llama', hidden_size=256, num_attention_heads=16, num_hidden_layers=4, num_key_value_heads=8)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello world! i am trying to create a simple web application using react and react-router-dom. i have a component called \"Home\" which renders a list of items. i want to add a button to the home component which will redirect to a new component called \"About\" when clicked. how can i achieve this? i have tried using the following code but it doesn\\'t work:\\n\\nimport React from \\'react\\';\\nimport { BrowserRouter as Router, Route, Link } from \\'react-router-dom\\';\\n\\nfunction Home() {\\n return (\\n <div>\\n <h1>Home</h1>\\n <ul>\\n {items.map(item => (\\n <li key={item.id}>\\n <Link to={`/about/${item.id}`}>{item.name}</Link>\\n </li>\\n ))}\\n </ul>\\n </div>\\n );\\n}\\n\\nfunction About() {\\n return (\\n <div>\\n <h1>About</h1>\\n <p>Here is a brief description of my project.</p>\\n </div>\\n );\\n}\\n\\nexport default Home;\\n\\ncan you help me with this? thank you!\\nYou can achieve this by using the `BrowserRouter` component from `react-router-dom` and passing the `Route` component as a prop to the `Home` component. Here\\'s how you can do it:\\n\\nimport React from \\'react\\';\\nimport { BrowserRouter as Router, Route, Link } from \\'react-router-dom\\';\\n\\nfunction Home() {\\n return (\\n <div>\\n <h1>Home</h1>\\n <ul>\\n {items.map(item => (\\n <li key={item.id}>\\n <Link to={`/about/${item.id}`}>{item.name}</Link>\\n </li>\\n ))}\\n </ul>\\n </div>\\n );\\n}\\n\\nfunction About() {\\n return (\\n <div>\\n <h1>About</h1>\\n <p>Here is a brief description of my project.</p>\\n </div>\\n );\\n}\\n\\nexport default Home;\\n\\nIn this code, we\\'re using the `BrowserRouter` component to create a new `Route` component. The `Route` component takes a `props` object that contains the `Route` component\\'s `to` prop. We\\'re passing the `Route` component as a prop to the `Home` component, which renders the `ul` element with the `items` array of `Link` components.\\n\\nThe `Link` component is used to create a link to the `about` component. The `to` prop is set to the `Route` component\\'s `to` prop, which is the URL of the `About` component.\\n\\nBy using this approach, you can achieve the desired behavior of redirecting to the `About` component when the `Home` component is clicked.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer('hello world!', return_tensors='pt').to(device)\n",
    "\n",
    "# generate text\n",
    "outputs = model.generate(inputs['input_ids'], max_length=50)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14990,  1879,     0]], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://127.0.0.1:7890'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['HTTPS_PROXY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers.trainer_pt_utils import LabelSmoother\n",
    "\n",
    "IGNORE_TOKEN_ID = LabelSmoother.ignore_index\n",
    "def preprocess(\n",
    "    sources,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    max_len: int,\n",
    "    system_message: str = \"You are a helpful assistant.\"\n",
    "):\n",
    "    roles = {\"user\": \"<|im_start|>user\", \"assistant\": \"<|im_start|>assistant\"}\n",
    "\n",
    "    # im_start = tokenizer.bos_token_id\n",
    "    im_end = tokenizer.eos_token_id\n",
    "    nl_tokens = tokenizer('\\n').input_ids\n",
    "    _system = tokenizer('system').input_ids + nl_tokens\n",
    "    _user = tokenizer('user').input_ids + nl_tokens\n",
    "    _assistant = tokenizer('assistant').input_ids + nl_tokens\n",
    "\n",
    "    # Apply prompt templates\n",
    "    input_ids, targets = [], []\n",
    "    for i, source in enumerate(sources):\n",
    "\n",
    "        input_id, target = [], []\n",
    "        system = _system + tokenizer(system_message).input_ids + [im_end] + nl_tokens\n",
    "        input_id += system\n",
    "        target += [IGNORE_TOKEN_ID] * (len(system)-2) + [im_end] + nl_tokens\n",
    "        assert len(input_id) == len(target)\n",
    "        for j, sentence in enumerate(source.split('.')):\n",
    "            # print(f'{sentence}\\n')\n",
    "            role = '<|im_start|>user'\n",
    "            _input_id = tokenizer(role).input_ids + nl_tokens + \\\n",
    "                tokenizer(sentence).input_ids + [im_end] + nl_tokens\n",
    "            input_id += _input_id\n",
    "            _target = [IGNORE_TOKEN_ID] * (len(_input_id)-2) + [im_end] + nl_tokens\n",
    "            target += _target\n",
    "        # print('input_id/target', input_id, target)\n",
    "        assert len(input_id) == len(target)\n",
    "        input_id += [tokenizer.pad_token_id] * (max_len - len(input_id))\n",
    "        target += [IGNORE_TOKEN_ID] * (max_len - len(target))\n",
    "        input_ids.append(input_id[:max_len])\n",
    "        targets.append(target[:max_len])\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.int)\n",
    "    targets = torch.tensor(targets, dtype=torch.int)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\":input_ids,\n",
    "        \"labels\":targets,\n",
    "        \"attention_mask\":input_ids.ne(tokenizer.pad_token_id),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# load data\n",
    "dataset_name= 'noanabeshima/TinyStoriesV2'\n",
    "ds = load_dataset(dataset_name, cache_dir='../../datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Once upon a time, there was a reliable otter named Ollie. He lived in a river with his family. They all loved to play and swim together.\\nOne day, Ollie\\'s mom said, \"Ollie, hurry and get some fish for dinner!\" Ollie swam fast to catch fish. He saw his friend, the duck. \"Hi, Ollie!\" said the duck. \"Hi, duck!\" said Ollie. \"I need to hurry and catch fish for my family.\"\\nWhile Ollie was catching fish, he found a big shiny stone. He thought, \"This is not a fish, but it is so pretty!\" Ollie took the shiny stone home to show his family. They all looked at the shiny stone and smiled. The shiny stone made everyone happy, and they forgot about the fish for dinner.',\n",
       " 'One day, a little boy named Tim went to the park. He saw a big tiger. The tiger was not mean, but very easy to play with. Tim and the tiger played all day. They had lots of fun.\\nThen, something unexpected happened. The tiger started to shake. Tim was scared. He did not know what was going on. But then, the tiger turned into a nice dog. Tim was very surprised.\\nTim and the dog played together now. They were very happy. The dog was easy to play with too. At the end of the day, Tim went home with his new friend.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train']['text'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "def preprocess_data(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "tokenized_dataset = ds.map(preprocess_data, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2717694\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 27629\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_dataset['train']['input_ids'][:1]\n",
    "# tokenized_dataset['train']['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Dataset' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenized_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m tokenized_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      2\u001b[0m tokenized_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenized_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      3\u001b[0m tokenized_dataset\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Dataset' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "# tokenized_dataset['train'][\"labels\"] = tokenized_dataset['train'][\"input_ids\"]\n",
    "# tokenized_dataset['validation'][\"labels\"] = tokenized_dataset['validation'][\"input_ids\"]\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2717694/2717694 [03:19<00:00, 13648.69 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27629/27629 [00:01<00:00, 14694.58 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# Wrap input_ids, attention_mask, and labels in a TensorDataset\n",
    "dataset = TensorDataset(\n",
    "    tokenized_dataset['train'][\"input_ids\"],\n",
    "    tokenized_dataset['train'][\"attention_mask\"],\n",
    "    tokenized_dataset['train'][\"labels\"]\n",
    ")\n",
    "\n",
    "# Create DataLoader for batching\n",
    "batch_size = 8\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_4962/1835065921.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "  0%|          | 0/254784 [10:07<?, ?it/s]\n",
      "  0%|          | 0/254784 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The model did not return a loss from the inputs, only the following keys: logits,past_key_values. For reference, the inputs it received are input_ids,attention_mask.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 30\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Step 4: Define Trainer and train the model\u001b[39;00m\n\u001b[1;32m     22\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     23\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     24\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     28\u001b[0m )\n\u001b[0;32m---> 30\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.10/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.10/site-packages/transformers/trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2479\u001b[0m )\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2487\u001b[0m ):\n\u001b[1;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.10/site-packages/transformers/trainer.py:3579\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3578\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3579\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3581\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3584\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3585\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.10/site-packages/transformers/trainer.py:3654\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3653\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m outputs:\n\u001b[0;32m-> 3654\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3655\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model did not return a loss from the inputs, only the following keys: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3656\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(outputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. For reference, the inputs it received are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(inputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3657\u001b[0m         )\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m# We don't use .loss here since the model may return tuples instead of ModelOutput.\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: The model did not return a loss from the inputs, only the following keys: logits,past_key_values. For reference, the inputs it received are input_ids,attention_mask."
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen_finetuned\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=50,\n",
    "    fp16=True,  # Use FP16 for faster training (if GPU supports it)\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# Step 4: Define Trainer and train the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./fine_tuned_llama2_7b\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_llama2_7b\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
