{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/learn2Pro/rl_learning/blob/master/llm/ddp_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJfWZDf23i2i"
   },
   "source": [
    "# basics\n",
    "  - https://github.com/pytorch/examples/tree/main/distributed/ddp-tutorial-series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8RkmkeZh3qey"
   },
   "source": [
    "![gpus](https://github.com/chunhuizhang/pytorch_distribute_tutorials/blob/main/imgs/gpus.png?raw=true)\n",
    "> multiple GPUs in a single machine/server/node：单机多卡\n",
    "\n",
    "- 分布式数据并行时，模型（model parameters）/优化器（optimizer states）每张卡都会拷贝一份（replicas）\n",
    "  - DDP 始终在卡间维持着模型参数和优化器状态的同步一致性在整个训练过程中；\n",
    "- Data Parallel，batch input，通过 DistributedSampler split & 分发到不同的 gpus 上\n",
    "  - 此时虽然模型/optimizer 相同，但因为数据输入不同，导致 loss 不同，反向传播时计算到的梯度也会不同\n",
    "  - 此时 ddp 如何保证卡间，model/optimizer 的同步一致性呢\n",
    "  - ring all-reduce algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2nyovtkU70d0"
   },
   "source": [
    "![12](https://github.com/chunhuizhang/pytorch_distribute_tutorials/blob/main/imgs/syn-gpus.png?raw=true)\n",
    "- 如上图所示，ring all-reduce algorithm\n",
    "  - 首先会将所有的 gpu cards 连成一个 ring\n",
    "  - 其同步过程，不需要等待所有的卡都计算完一轮梯度，\n",
    "  - 经过这个同步过程之后，所有的卡的 models/optimizers 就都会保持一致的状态；\n",
    "## Ring AllReduce Algorithm\n",
    "- 李沐：参数服务器；\n",
    "  - https://d2l.ai/chapter_computational-performance/parameterserver.html\n",
    "  - https://www.cs.cmu.edu/~muli/file/ps.pdf\n",
    "- 计算和同步的几个过程\n",
    "  - GPUs 分别计算损失（forward）和梯度（backward）\n",
    "  - 梯度的聚合\n",
    "  -（模型/优化器）参数的更新及广播（broadcast）；\n",
    "- HPC（high performance computing）的基础算法\n",
    "- Ring 环形拓扑结构\n",
    "  - 百度提出来的；\n",
    "  - 环形的，logical 的（逻辑的，非物理的）\n",
    "  - 两个过程（基于环形逻辑拓扑）\n",
    "    - scatter-reduce\n",
    "    - all gather（broadcast）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SO7osRzV9lcx"
   },
   "source": [
    "## scatter and reduce\n",
    "![1](https://github.com/chunhuizhang/pytorch_distribute_tutorials/blob/main/imgs/multigpus_1.png?raw=true)\n",
    "## ring reduce\n",
    "![2](https://github.com/chunhuizhang/pytorch_distribute_tutorials/blob/main/imgs/ring.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYyRaX1_-D58"
   },
   "source": [
    "## ddp相关基本概念\n",
    "node, rank, world_size\n",
    "> 不太严谨的理解\n",
    "\n",
    "- world，world_size：\n",
    "\n",
    "  - world：as a group containing all the processes for your distributed training.\n",
    "    - 通常，每一个 gpu 代表一个进程（process）\n",
    "    - world 内的 process 可以彼此通信，所以有 ddp 分布式训练的；\n",
    "- rank\n",
    "\n",
    "  - rank: is the unique ID given to a process, 进程级别的概念，rank 是为了标识、识别进程，因为进程间（process）需要通信；\n",
    "  - local rank：is the a unique local ID for processes running in a single node\n",
    "- node 理解为一个 server，2个servers（多机，机器之间需要通信）就是2个nodes\n",
    "\n",
    "  - 比如每个 node/server/machine 各有4张卡（4 gpus），一共 2 个node/server；\n",
    "  - world_size: 2*4 == 8\n",
    "  - ranks: [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "  - local_rank: [0, 1, 2, 3], [0, 1, 2, 3]\n",
    "\n",
    "  ```python\n",
    "  def ddp_setup(rank, world_size):\n",
    "      \"\"\"\n",
    "      Args:\n",
    "          rank: Unique identifier of each process\n",
    "          world_size: Total number of processes\n",
    "      \"\"\"\n",
    "      os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "      os.environ[\"MASTER_PORT\"] = \"12355\"\n",
    "      init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
    "      torch.cuda.set_device(rank)\n",
    "  ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "am_6CeQc3SbM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMYiZVPsFQyEAZV+PX0guSH",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
