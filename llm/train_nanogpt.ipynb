{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c65cf985-3cd5-441c-b538-ad876e085bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8681/35563342.py:9: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import ZeroRedundancyOptimizer\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import torch._inductor.config as config\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from torch.distributed.optim import ZeroRedundancyOptimizer\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5764b0f0-8a77-4e4c-8f43-15b548f339ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewGELU(nn.Module):\n",
    "    \"\"\"Careful there are a few versions of GeLU, this one is the exact one used by OpenAI\"\"\"\n",
    "    def forward(self, input):\n",
    "        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0/math.pi) * (input + 0.044715 * torch.pow(input \\\n",
    "                                                                         , 3.0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42e91090-ac1b-4a6c-bfd3-e2390a347a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.LLMC_RESIDUAL_SCALE_FLAG = 1\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        # not really a 'bias', more of a mask, but following the OpenAI/HF naming through\n",
    "        self.register_buffer('bias', torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                             .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch, seq_len, n_embd\n",
    "        qkv = self.c_attn(x) # batch, seq_len, 3*n_embd\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2) # batch, seq_len, n_embd\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        if FLASH:\n",
    "            y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            # materialize the (T, T) matrix\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) # (B, nh, T, T)\n",
    "            att = att.masked_fill(self.bias[:, :, :T, :T] ==0, float('-inf')) # (B, nh, T, T)\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            y = att @ v # (B, nh, T, T)@(B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4057c961-06bd-458a-aa3b-f00ab9017283",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4*config.n_embd)\n",
    "        self.gelu = NewGELU()\n",
    "        self.c_proj = nn.Linear(4*config.n_embd, config.n_embd)\n",
    "        self.c_proj.LLMC_RESIDUAL_SCALE_FLAG = 1\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23373d97-57dc-456b-ad60-b5ec12dbcb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "    def forward(self, x):\n",
    "        x = x+self.attn(self.ln_1(x))\n",
    "        x = x+self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dc996e8-5dd1-4abf-84cf-0fe72eb8ab7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50527\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8b5655f-764b-477c-9649-9923dac44131",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.lm_head.LLMC_SKIP_INIT = 1 # don't init this one, we will tie weights\n",
    "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "        # init all weights, use a torch rng object to be very careful\n",
    "        self.init_rng = torch.Generator()\n",
    "        self.init_rng.manual_seed(42)\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02 if not hasattr(module, 'LLMC_RESIDUAL_SCALE_FLAG') else 0.02/math.sqrt(2*self.config.n_layer)\n",
    "            # we want to skip initializing lm_head, which shares parameters with wte\n",
    "            # and wte was already initialized down below during the embedding init\n",
    "            if not hasattr(module, 'LLMC_SKIP_INIT'):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=std, generator=self.init_rng)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02, generator=self.init_rng)\n",
    "\n",
    "    def forward(self, idx, targets=None, return_logits=True):\n",
    "        device = idx.device\n",
    "        b, t = idx.size() # batch, seq_len\n",
    "        assert t<=self.config.block_size, f\"Cannot foward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) \n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = tok_emb + pos_emb\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            loss = None\n",
    "\n",
    "        if not return_logits:\n",
    "            logits = None\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond) # B, T, vocab_size\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('-inf')\n",
    "\n",
    "            # apply softmax to convert logits to normalized probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # B, vocab_size\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            idx = torch.cat((idx, idx_next), dim=-1)\n",
    "            return idx\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7657bdd-ca8d-4708-bbe1-4f67228397e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class GPTConfig:\n",
    "#     block_size: int = 1024\n",
    "#     vocab_size: int = 50527\n",
    "#     n_layer: int = 12\n",
    "#     n_head: int = 12\n",
    "#     n_embd: int = 768\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from itertools import chain\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "n_block = 1024\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "config = GPTConfig(block_size=n_block, vocab_size=50257, n_layer=6, n_head=12, n_embd=768)\n",
    "model = GPT(config).to(device)\n",
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2c82a6a-533b-475a-8fb3-758e379faa2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total param 81.912576m\n"
     ]
    }
   ],
   "source": [
    "def print_parameters(model):\n",
    "    num_param = sum([param.numel() for param in model.parameters() if param.requires_grad])\n",
    "    print(f'total param {num_param/1000/1000}m')\n",
    "\n",
    "print_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50c83854-2af4-4aef-bd43-a3a0b33078a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81378ddca655495c85dedb2ee127d91d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/366 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a401ec20383c496d83a9edb4f19cc4bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/366 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[28156,   243,   163,  ...,   239, 44165,   247],\n",
      "        [39355,   225, 44165,  ..., 50256, 50256, 50256],\n",
      "        [  325,    78, 42468,  ...,   114, 29785,   112],\n",
      "        ...,\n",
      "        [20015,   236, 22522,  ...,   100, 26344,   114],\n",
      "        [36685,   224, 19526,  ...,    95,   252, 27950],\n",
      "        [44293,   119,   163,  ..., 38519,   163,   122]]), tensor([[  243,   163,   121,  ..., 44165,   247, 50256],\n",
      "        [  225, 44165,   247,  ..., 50256, 50256, 50256],\n",
      "        [   78, 42468, 20015,  ..., 29785,   112, 50256],\n",
      "        ...,\n",
      "        [  236, 22522,   252,  ..., 26344,   114, 50256],\n",
      "        [  224, 19526,   243,  ...,   252, 27950, 50256],\n",
      "        [  119,   163,   244,  ...,   163,   122, 50256]]))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from itertools import chain\n",
    "\n",
    "ds = load_dataset(\"p208p2002/wudao\",streaming=True, split=\"train\")\n",
    "\n",
    "def encode(examples):\n",
    "    return tokenizer(examples['title'], examples['content'], truncation=True, padding='max_length')\n",
    "\n",
    "def collate_fn(examples, n_block, pad_token_id):\n",
    "    x = torch.tensor([x['input_ids'] for x in examples], dtype=torch.long)\n",
    "    y = torch.tensor([x['input_ids'][1:]+[tokenizer.eos_token_id] for x in examples], dtype=torch.long)\n",
    "    # print(x.shape, y.shape)\n",
    "    return x, y\n",
    "\n",
    "ds = ds.map(encode, batched=True)\n",
    "train_loader = DataLoader(ds, batch_size=12, collate_fn=lambda x: collate_fn(x,n_block, tokenizer.eos_token_id))\n",
    "item = next(iter(train_loader))\n",
    "print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ec43cbb-9cfe-44aa-ba71-6f8631a214b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国首都是哪? upward\n"
     ]
    }
   ],
   "source": [
    "FLASH = 0\n",
    "def sample(model, query, max_new_tokens=128):\n",
    "    tokens = torch.tensor(tokenizer.encode(query), dtype=torch.long).unsqueeze(0)\n",
    "    outputs = model.generate(tokens.to(device), max_new_tokens)\n",
    "    return tokenizer.decode(outputs.view(-1).cpu().numpy())\n",
    "\n",
    "print(sample(model, \"中国首都是哪?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8efb4239-a7c5-44d0-9e9c-a78d8a9b4223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import wandb\n",
    "scaler = GradScaler()  # 创建 GradScaler 对象\n",
    "wandb.init()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, betas=(0.9,0.95))\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000)\n",
    "\n",
    "def eval(model, val_loader):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    for x, y in val_loader:\n",
    "        with torch.no_grad():\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits, loss = model(x, targets=y)\n",
    "            val_loss+=loss.item()\n",
    "    return val_loss\n",
    "            \n",
    "def train(model, optimizer, scheduler, train_loader, grad_clip=1.0):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    grad_norm = -1.0\n",
    "    for idx, (x, y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        with autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits, loss = model(x, targets=y)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        # clip grad\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        # adjust lr\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        # grad_norm = torch.sqrt(sum(p.grad.norm()**2 for p in model.parameters() if p.grad is not None))\n",
    "        # Compute total gradient norm (L2 norm)\n",
    "        # grad_norm = torch.sqrt(sum(p.grad.norm() ** 2 for p in model.parameters() if p.grad is not None))\n",
    "\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            print(f'Epoch {epoch}, Step: {idx} Learing: {lr:.10f} Loss: {loss.item():.4f} Grad Norm: {grad_norm:.4f}')\n",
    "            wandb.log({'step':idx, 'train/loss':loss.item(), 'learning_rate': lr, 'grad_norm': grad_norm})\n",
    "        if idx % 1000 == 0:\n",
    "            print(sample(model, \"中国首都是哪?\"))\n",
    "        if idx % 5000 == 0:\n",
    "            dt = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            # 假设 model 是您的模型实例\n",
    "            torch.save(model.state_dict(), f'ouputs/nanogpt/checkpoint-{idx}/model_weights.pth')\n",
    "            # model.save_pretrained(f'outputs/nanogpt/checkpoint-{idx}/', safe_serialization=False)\n",
    "            \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3da132e-aeaa-40a2-bc68-90a3c47dffd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step: 0 Learing: 0.0000100000 Loss: 10.8660 Grad Norm: 12.7340\n",
      "中国首都是哪? pitching\n",
      "Epoch 0, Step: 100 Learing: 0.0000097504 Loss: 7.7208 Grad Norm: 3.7821\n",
      "Epoch 0, Step: 200 Learing: 0.0000090358 Loss: 6.3536 Grad Norm: 3.1613\n",
      "Epoch 0, Step: 300 Learing: 0.0000079262 Loss: 5.4538 Grad Norm: 2.6412\n",
      "Epoch 0, Step: 400 Learing: 0.0000065301 Loss: 4.8298 Grad Norm: 2.2347\n",
      "Epoch 0, Step: 500 Learing: 0.0000049843 Loss: 4.3690 Grad Norm: 2.2557\n",
      "Epoch 0, Step: 600 Learing: 0.0000034400 Loss: 4.3995 Grad Norm: 2.5969\n",
      "Epoch 0, Step: 700 Learing: 0.0000020484 Loss: 4.2486 Grad Norm: 3.2801\n",
      "Epoch 0, Step: 800 Learing: 0.0000009457 Loss: 4.1137 Grad Norm: 1.8781\n",
      "Epoch 0, Step: 900 Learing: 0.0000002399 Loss: 4.0051 Grad Norm: 1.5855\n",
      "Epoch 0, Step: 1000 Learing: 0.0000000000 Loss: 4.1859 Grad Norm: 1.9173\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 1100 Learing: 0.0000002496 Loss: 4.1343 Grad Norm: 1.7015\n",
      "Epoch 0, Step: 1200 Learing: 0.0000009642 Loss: 4.1070 Grad Norm: 1.6356\n",
      "Epoch 0, Step: 1300 Learing: 0.0000020738 Loss: 4.0517 Grad Norm: 1.9210\n",
      "Epoch 0, Step: 1400 Learing: 0.0000034699 Loss: 4.0041 Grad Norm: 1.7892\n",
      "Epoch 0, Step: 1500 Learing: 0.0000050157 Loss: 3.9613 Grad Norm: 2.0461\n",
      "Epoch 0, Step: 1600 Learing: 0.0000065600 Loss: 3.5507 Grad Norm: 1.9561\n",
      "Epoch 0, Step: 1700 Learing: 0.0000079516 Loss: 3.6419 Grad Norm: 1.8971\n",
      "Epoch 0, Step: 1800 Learing: 0.0000090543 Loss: 3.5728 Grad Norm: 2.1346\n",
      "Epoch 0, Step: 1900 Learing: 0.0000097601 Loss: 3.6228 Grad Norm: 1.9786\n",
      "Epoch 0, Step: 2000 Learing: 0.0000100000 Loss: 3.5507 Grad Norm: 2.3212\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 2100 Learing: 0.0000097504 Loss: 3.5906 Grad Norm: 2.2665\n",
      "Epoch 0, Step: 2200 Learing: 0.0000090358 Loss: 3.5281 Grad Norm: 2.5462\n",
      "Epoch 0, Step: 2300 Learing: 0.0000079262 Loss: 3.5037 Grad Norm: 2.8611\n",
      "Epoch 0, Step: 2400 Learing: 0.0000065301 Loss: 3.5702 Grad Norm: 2.4005\n",
      "Epoch 0, Step: 2500 Learing: 0.0000049843 Loss: 3.5559 Grad Norm: 3.0695\n",
      "Epoch 0, Step: 2600 Learing: 0.0000034400 Loss: 3.4207 Grad Norm: 2.1404\n",
      "Epoch 0, Step: 2700 Learing: 0.0000020484 Loss: 3.5625 Grad Norm: 2.4356\n",
      "Epoch 0, Step: 2800 Learing: 0.0000009457 Loss: 3.4223 Grad Norm: 2.2228\n",
      "Epoch 0, Step: 2900 Learing: 0.0000002399 Loss: 3.3668 Grad Norm: 2.0411\n",
      "Epoch 0, Step: 3000 Learing: 0.0000000000 Loss: 3.3764 Grad Norm: 2.0207\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 3100 Learing: 0.0000002496 Loss: 3.4998 Grad Norm: 2.2257\n",
      "Epoch 0, Step: 3200 Learing: 0.0000009642 Loss: 3.5409 Grad Norm: 2.5711\n",
      "Epoch 0, Step: 3300 Learing: 0.0000020738 Loss: 3.5360 Grad Norm: 2.1755\n",
      "Epoch 0, Step: 3400 Learing: 0.0000034699 Loss: 3.3811 Grad Norm: 2.6192\n",
      "Epoch 0, Step: 3500 Learing: 0.0000050157 Loss: 3.2688 Grad Norm: 2.9037\n",
      "Epoch 0, Step: 3600 Learing: 0.0000065600 Loss: 3.3185 Grad Norm: 2.3811\n",
      "Epoch 0, Step: 3700 Learing: 0.0000079516 Loss: 3.4074 Grad Norm: 3.2060\n",
      "Epoch 0, Step: 3800 Learing: 0.0000090543 Loss: 3.4811 Grad Norm: 2.9119\n",
      "Epoch 0, Step: 3900 Learing: 0.0000097601 Loss: 3.3819 Grad Norm: 2.4977\n",
      "Epoch 0, Step: 4000 Learing: 0.0000100000 Loss: 3.5073 Grad Norm: 3.6446\n",
      "中国首都是哪?\"\n",
      "Epoch 0, Step: 4100 Learing: 0.0000097504 Loss: 3.3500 Grad Norm: 2.7217\n",
      "Epoch 0, Step: 4200 Learing: 0.0000090358 Loss: 3.3926 Grad Norm: 3.6016\n",
      "Epoch 0, Step: 4300 Learing: 0.0000079262 Loss: 3.3126 Grad Norm: 3.1185\n",
      "Epoch 0, Step: 4400 Learing: 0.0000065301 Loss: 3.5244 Grad Norm: 3.4134\n",
      "Epoch 0, Step: 4500 Learing: 0.0000049843 Loss: 3.4855 Grad Norm: 3.8325\n",
      "Epoch 0, Step: 4600 Learing: 0.0000034400 Loss: 3.4488 Grad Norm: 3.0518\n",
      "Epoch 0, Step: 4700 Learing: 0.0000020484 Loss: 3.4136 Grad Norm: 3.1010\n",
      "Epoch 0, Step: 4800 Learing: 0.0000009457 Loss: 3.4995 Grad Norm: 2.9429\n",
      "Epoch 0, Step: 4900 Learing: 0.0000002399 Loss: 3.4552 Grad Norm: 2.9180\n",
      "Epoch 0, Step: 5000 Learing: 0.0000000000 Loss: 3.5368 Grad Norm: 2.8085\n",
      "中国首都是哪?中\n",
      "Epoch 0, Step: 5100 Learing: 0.0000002496 Loss: 3.3637 Grad Norm: 2.8640\n",
      "Epoch 0, Step: 5200 Learing: 0.0000009642 Loss: 3.3975 Grad Norm: 2.7023\n",
      "Epoch 0, Step: 5300 Learing: 0.0000020738 Loss: 3.4345 Grad Norm: 3.2329\n",
      "Epoch 0, Step: 5400 Learing: 0.0000034699 Loss: 3.3526 Grad Norm: 3.0027\n",
      "Epoch 0, Step: 5500 Learing: 0.0000050157 Loss: 3.4166 Grad Norm: 3.1808\n",
      "Epoch 0, Step: 5600 Learing: 0.0000065600 Loss: 3.4233 Grad Norm: 3.8127\n",
      "Epoch 0, Step: 5700 Learing: 0.0000079516 Loss: 3.3787 Grad Norm: 3.8438\n",
      "Epoch 0, Step: 5800 Learing: 0.0000090543 Loss: 3.4257 Grad Norm: 3.7246\n",
      "Epoch 0, Step: 5900 Learing: 0.0000097601 Loss: 3.3461 Grad Norm: 3.7549\n",
      "Epoch 0, Step: 6000 Learing: 0.0000100000 Loss: 3.3736 Grad Norm: 4.1648\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 6100 Learing: 0.0000097504 Loss: 3.2172 Grad Norm: 4.6143\n",
      "Epoch 0, Step: 6200 Learing: 0.0000090358 Loss: 3.3276 Grad Norm: 5.3357\n",
      "Epoch 0, Step: 6300 Learing: 0.0000079262 Loss: 3.3064 Grad Norm: 4.6400\n",
      "Epoch 0, Step: 6400 Learing: 0.0000065301 Loss: 3.3037 Grad Norm: 4.6072\n",
      "Epoch 0, Step: 6500 Learing: 0.0000049843 Loss: 3.1730 Grad Norm: 5.1997\n",
      "Epoch 0, Step: 6600 Learing: 0.0000034400 Loss: 3.2004 Grad Norm: 4.8757\n",
      "Epoch 0, Step: 6700 Learing: 0.0000020484 Loss: 3.2501 Grad Norm: 7.1826\n",
      "Epoch 0, Step: 6800 Learing: 0.0000009457 Loss: 3.1403 Grad Norm: 3.8737\n",
      "Epoch 0, Step: 6900 Learing: 0.0000002399 Loss: 3.1253 Grad Norm: 4.1572\n",
      "Epoch 0, Step: 7000 Learing: 0.0000000000 Loss: 3.1193 Grad Norm: 4.0558\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 7100 Learing: 0.0000002496 Loss: 3.3980 Grad Norm: 5.5912\n",
      "Epoch 0, Step: 7200 Learing: 0.0000009642 Loss: 3.2832 Grad Norm: 4.6776\n",
      "Epoch 0, Step: 7300 Learing: 0.0000020738 Loss: 3.3223 Grad Norm: 7.9356\n",
      "Epoch 0, Step: 7400 Learing: 0.0000034699 Loss: 3.0526 Grad Norm: 5.2616\n",
      "Epoch 0, Step: 7500 Learing: 0.0000050157 Loss: 2.9242 Grad Norm: 3.8800\n",
      "Epoch 0, Step: 7600 Learing: 0.0000065600 Loss: 3.2236 Grad Norm: 4.5956\n",
      "Epoch 0, Step: 7700 Learing: 0.0000079516 Loss: 3.1315 Grad Norm: 4.6007\n",
      "Epoch 0, Step: 7800 Learing: 0.0000090543 Loss: 2.8186 Grad Norm: 4.3801\n",
      "Epoch 0, Step: 7900 Learing: 0.0000097601 Loss: 3.1786 Grad Norm: 5.1405\n",
      "Epoch 0, Step: 8000 Learing: 0.0000100000 Loss: 3.0903 Grad Norm: 5.1957\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 8100 Learing: 0.0000097504 Loss: 3.0341 Grad Norm: 5.2472\n",
      "Epoch 0, Step: 8200 Learing: 0.0000090358 Loss: 3.0777 Grad Norm: 4.7245\n",
      "Epoch 0, Step: 8300 Learing: 0.0000079262 Loss: 3.0302 Grad Norm: 4.5429\n",
      "Epoch 0, Step: 8400 Learing: 0.0000065301 Loss: 2.9838 Grad Norm: 6.0172\n",
      "Epoch 0, Step: 8500 Learing: 0.0000049843 Loss: 3.0195 Grad Norm: 4.8308\n",
      "Epoch 0, Step: 8600 Learing: 0.0000034400 Loss: 3.1742 Grad Norm: 5.6901\n",
      "Epoch 0, Step: 8700 Learing: 0.0000020484 Loss: 3.1333 Grad Norm: 4.4875\n",
      "Epoch 0, Step: 8800 Learing: 0.0000009457 Loss: 3.1403 Grad Norm: 4.7344\n",
      "Epoch 0, Step: 8900 Learing: 0.0000002399 Loss: 2.8496 Grad Norm: 3.7487\n",
      "Epoch 0, Step: 9000 Learing: 0.0000000000 Loss: 2.9411 Grad Norm: 4.1867\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 9100 Learing: 0.0000002496 Loss: 2.9824 Grad Norm: 3.4520\n",
      "Epoch 0, Step: 9200 Learing: 0.0000009642 Loss: 2.8072 Grad Norm: 4.2199\n",
      "Epoch 0, Step: 9300 Learing: 0.0000020738 Loss: 2.9833 Grad Norm: 3.9548\n",
      "Epoch 0, Step: 9400 Learing: 0.0000034699 Loss: 2.8558 Grad Norm: 4.2000\n",
      "Epoch 0, Step: 9500 Learing: 0.0000050157 Loss: 2.8719 Grad Norm: 5.5233\n",
      "Epoch 0, Step: 9600 Learing: 0.0000065600 Loss: 2.9630 Grad Norm: 4.9397\n",
      "Epoch 0, Step: 9700 Learing: 0.0000079516 Loss: 3.0574 Grad Norm: 5.5214\n",
      "Epoch 0, Step: 9800 Learing: 0.0000090543 Loss: 2.9789 Grad Norm: 6.1275\n",
      "Epoch 0, Step: 9900 Learing: 0.0000097601 Loss: 3.0111 Grad Norm: 4.7458\n",
      "Epoch 0, Step: 10000 Learing: 0.0000100000 Loss: 2.8329 Grad Norm: 5.9937\n",
      "中国首都是哪?s\n",
      "Epoch 0, Step: 10100 Learing: 0.0000097504 Loss: 2.8975 Grad Norm: 4.5753\n",
      "Epoch 0, Step: 10200 Learing: 0.0000090358 Loss: 2.9885 Grad Norm: 5.7108\n",
      "Epoch 0, Step: 10300 Learing: 0.0000079262 Loss: 3.0045 Grad Norm: 5.5145\n",
      "Epoch 0, Step: 10400 Learing: 0.0000065301 Loss: 2.8437 Grad Norm: 4.4328\n",
      "Epoch 0, Step: 10500 Learing: 0.0000049843 Loss: 2.8463 Grad Norm: 6.0787\n",
      "Epoch 0, Step: 10600 Learing: 0.0000034400 Loss: 2.8488 Grad Norm: 5.2592\n",
      "Epoch 0, Step: 10700 Learing: 0.0000020484 Loss: 2.9988 Grad Norm: 4.2810\n",
      "Epoch 0, Step: 10800 Learing: 0.0000009457 Loss: 3.0105 Grad Norm: 4.1925\n",
      "Epoch 0, Step: 10900 Learing: 0.0000002399 Loss: 2.9576 Grad Norm: 4.2061\n",
      "Epoch 0, Step: 11000 Learing: 0.0000000000 Loss: 2.9907 Grad Norm: 3.9263\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 11100 Learing: 0.0000002496 Loss: 2.8583 Grad Norm: 3.6981\n",
      "Epoch 0, Step: 11200 Learing: 0.0000009642 Loss: 2.9259 Grad Norm: 4.6752\n",
      "Epoch 0, Step: 11300 Learing: 0.0000020738 Loss: 2.7802 Grad Norm: 4.6130\n",
      "Epoch 0, Step: 11400 Learing: 0.0000034699 Loss: 2.9509 Grad Norm: 5.5228\n",
      "Epoch 0, Step: 11500 Learing: 0.0000050157 Loss: 2.8903 Grad Norm: 5.9547\n",
      "Epoch 0, Step: 11600 Learing: 0.0000065600 Loss: 2.9943 Grad Norm: 6.3584\n",
      "Epoch 0, Step: 11700 Learing: 0.0000079516 Loss: 2.9269 Grad Norm: 4.9937\n",
      "Epoch 0, Step: 11800 Learing: 0.0000090543 Loss: 2.7963 Grad Norm: 6.3323\n",
      "Epoch 0, Step: 11900 Learing: 0.0000097601 Loss: 2.9135 Grad Norm: 5.4744\n",
      "Epoch 0, Step: 12000 Learing: 0.0000100000 Loss: 2.9277 Grad Norm: 6.0822\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 12100 Learing: 0.0000097504 Loss: 2.9548 Grad Norm: 6.0805\n",
      "Epoch 0, Step: 12200 Learing: 0.0000090358 Loss: 2.9851 Grad Norm: 6.1186\n",
      "Epoch 0, Step: 12300 Learing: 0.0000079262 Loss: 3.0144 Grad Norm: 6.4792\n",
      "Epoch 0, Step: 12400 Learing: 0.0000065301 Loss: 2.8759 Grad Norm: 4.9320\n",
      "Epoch 0, Step: 12500 Learing: 0.0000049843 Loss: 2.8920 Grad Norm: 5.2668\n",
      "Epoch 0, Step: 12600 Learing: 0.0000034400 Loss: 2.9641 Grad Norm: 5.5357\n",
      "Epoch 0, Step: 12700 Learing: 0.0000020484 Loss: 2.8010 Grad Norm: 5.9544\n",
      "Epoch 0, Step: 12800 Learing: 0.0000009457 Loss: 2.7949 Grad Norm: 4.8398\n",
      "Epoch 0, Step: 12900 Learing: 0.0000002399 Loss: 2.9211 Grad Norm: 5.8169\n",
      "Epoch 0, Step: 13000 Learing: 0.0000000000 Loss: 2.8349 Grad Norm: 4.7382\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 13100 Learing: 0.0000002496 Loss: 2.9430 Grad Norm: 5.9283\n",
      "Epoch 0, Step: 13200 Learing: 0.0000009642 Loss: 2.8457 Grad Norm: 5.0188\n",
      "Epoch 0, Step: 13300 Learing: 0.0000020738 Loss: 2.9056 Grad Norm: 6.6483\n",
      "Epoch 0, Step: 13400 Learing: 0.0000034699 Loss: 2.6997 Grad Norm: 6.6115\n",
      "Epoch 0, Step: 13500 Learing: 0.0000050157 Loss: 2.9320 Grad Norm: 7.5066\n",
      "Epoch 0, Step: 13600 Learing: 0.0000065600 Loss: 2.8262 Grad Norm: 5.9717\n",
      "Epoch 0, Step: 13700 Learing: 0.0000079516 Loss: 2.4053 Grad Norm: 6.2153\n",
      "Epoch 0, Step: 13800 Learing: 0.0000090543 Loss: 2.7427 Grad Norm: 5.4585\n",
      "Epoch 0, Step: 13900 Learing: 0.0000097601 Loss: 2.6863 Grad Norm: 6.0353\n",
      "Epoch 0, Step: 14000 Learing: 0.0000100000 Loss: 2.8530 Grad Norm: 5.1190\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 14100 Learing: 0.0000097504 Loss: 2.8799 Grad Norm: 5.9495\n",
      "Epoch 0, Step: 14200 Learing: 0.0000090358 Loss: 2.8155 Grad Norm: 4.9808\n",
      "Epoch 0, Step: 14300 Learing: 0.0000079262 Loss: 2.8355 Grad Norm: 6.1622\n",
      "Epoch 0, Step: 14400 Learing: 0.0000065301 Loss: 2.7311 Grad Norm: 5.7243\n",
      "Epoch 0, Step: 14500 Learing: 0.0000049843 Loss: 2.7933 Grad Norm: 5.6522\n",
      "Epoch 0, Step: 14600 Learing: 0.0000034400 Loss: 2.7253 Grad Norm: 5.8644\n",
      "Epoch 0, Step: 14700 Learing: 0.0000020484 Loss: 2.7371 Grad Norm: 4.6095\n",
      "Epoch 0, Step: 14800 Learing: 0.0000009457 Loss: 2.8551 Grad Norm: 5.1096\n",
      "Epoch 0, Step: 14900 Learing: 0.0000002399 Loss: 2.6669 Grad Norm: 4.7054\n",
      "Epoch 0, Step: 15000 Learing: 0.0000000000 Loss: 2.8965 Grad Norm: 9.5193\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 15100 Learing: 0.0000002496 Loss: 2.7499 Grad Norm: 5.4296\n",
      "Epoch 0, Step: 15200 Learing: 0.0000009642 Loss: 2.7381 Grad Norm: 5.0536\n",
      "Epoch 0, Step: 15300 Learing: 0.0000020738 Loss: 2.7930 Grad Norm: 5.0859\n",
      "Epoch 0, Step: 15400 Learing: 0.0000034699 Loss: 2.7600 Grad Norm: 4.9395\n",
      "Epoch 0, Step: 15500 Learing: 0.0000050157 Loss: 2.7022 Grad Norm: 6.0278\n",
      "Epoch 0, Step: 15600 Learing: 0.0000065600 Loss: 2.6877 Grad Norm: 7.3145\n",
      "Epoch 0, Step: 15700 Learing: 0.0000079516 Loss: 2.7463 Grad Norm: 4.9438\n",
      "Epoch 0, Step: 15800 Learing: 0.0000090543 Loss: 2.6611 Grad Norm: 7.3237\n",
      "Epoch 0, Step: 15900 Learing: 0.0000097601 Loss: 2.6098 Grad Norm: 5.4989\n",
      "Epoch 0, Step: 16000 Learing: 0.0000100000 Loss: 2.6967 Grad Norm: 5.9127\n",
      "中国首都是哪?人\n",
      "Epoch 0, Step: 16100 Learing: 0.0000097504 Loss: 2.7135 Grad Norm: 6.4083\n",
      "Epoch 0, Step: 16200 Learing: 0.0000090358 Loss: 2.7607 Grad Norm: 4.9613\n",
      "Epoch 0, Step: 16300 Learing: 0.0000079262 Loss: 2.8943 Grad Norm: 6.1887\n",
      "Epoch 0, Step: 16400 Learing: 0.0000065301 Loss: 2.6041 Grad Norm: 5.7195\n",
      "Epoch 0, Step: 16500 Learing: 0.0000049843 Loss: 2.8119 Grad Norm: 5.5494\n",
      "Epoch 0, Step: 16600 Learing: 0.0000034400 Loss: 2.8729 Grad Norm: 5.2349\n",
      "Epoch 0, Step: 16700 Learing: 0.0000020484 Loss: 2.6399 Grad Norm: 4.6343\n",
      "Epoch 0, Step: 16800 Learing: 0.0000009457 Loss: 2.6575 Grad Norm: 5.1564\n",
      "Epoch 0, Step: 16900 Learing: 0.0000002399 Loss: 2.5325 Grad Norm: 4.5743\n",
      "Epoch 0, Step: 17000 Learing: 0.0000000000 Loss: 2.5950 Grad Norm: 5.2866\n",
      "中国首都是哪?大\n",
      "Epoch 0, Step: 17100 Learing: 0.0000002496 Loss: 2.7242 Grad Norm: 4.9948\n",
      "Epoch 0, Step: 17200 Learing: 0.0000009642 Loss: 2.6907 Grad Norm: 5.9007\n",
      "Epoch 0, Step: 17300 Learing: 0.0000020738 Loss: 2.7341 Grad Norm: 5.9818\n",
      "Epoch 0, Step: 17400 Learing: 0.0000034699 Loss: 2.6852 Grad Norm: 6.2017\n",
      "Epoch 0, Step: 17500 Learing: 0.0000050157 Loss: 2.6230 Grad Norm: 5.6515\n",
      "Epoch 0, Step: 17600 Learing: 0.0000065600 Loss: 2.5681 Grad Norm: 5.6664\n",
      "Epoch 0, Step: 17700 Learing: 0.0000079516 Loss: 2.6797 Grad Norm: 7.3366\n",
      "Epoch 0, Step: 17800 Learing: 0.0000090543 Loss: 2.8107 Grad Norm: 5.9611\n",
      "Epoch 0, Step: 17900 Learing: 0.0000097601 Loss: 2.7678 Grad Norm: 5.8505\n",
      "Epoch 0, Step: 18000 Learing: 0.0000100000 Loss: 2.8423 Grad Norm: 6.2811\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 18100 Learing: 0.0000097504 Loss: 2.5999 Grad Norm: 7.0295\n",
      "Epoch 0, Step: 18200 Learing: 0.0000090358 Loss: 2.5437 Grad Norm: 6.1819\n",
      "Epoch 0, Step: 18300 Learing: 0.0000079262 Loss: 2.8394 Grad Norm: 6.4830\n",
      "Epoch 0, Step: 18400 Learing: 0.0000065301 Loss: 2.5240 Grad Norm: 5.9013\n",
      "Epoch 0, Step: 18500 Learing: 0.0000049843 Loss: 2.6611 Grad Norm: 5.9156\n",
      "Epoch 0, Step: 18600 Learing: 0.0000034400 Loss: 2.5293 Grad Norm: 5.4049\n",
      "Epoch 0, Step: 18700 Learing: 0.0000020484 Loss: 2.7251 Grad Norm: 5.5094\n",
      "Epoch 0, Step: 18800 Learing: 0.0000009457 Loss: 2.5980 Grad Norm: 5.8583\n",
      "Epoch 0, Step: 18900 Learing: 0.0000002399 Loss: 2.7336 Grad Norm: 5.0868\n",
      "Epoch 0, Step: 19000 Learing: 0.0000000000 Loss: 2.7395 Grad Norm: 4.9381\n",
      "中国首都是哪?武\n",
      "Epoch 0, Step: 19100 Learing: 0.0000002496 Loss: 2.7094 Grad Norm: 5.2883\n",
      "Epoch 0, Step: 19200 Learing: 0.0000009642 Loss: 2.7377 Grad Norm: 5.6938\n",
      "Epoch 0, Step: 19300 Learing: 0.0000020738 Loss: 2.5122 Grad Norm: 5.7060\n",
      "Epoch 0, Step: 19400 Learing: 0.0000034699 Loss: 2.2717 Grad Norm: 5.1546\n",
      "Epoch 0, Step: 19500 Learing: 0.0000050157 Loss: 2.6705 Grad Norm: 5.5136\n",
      "Epoch 0, Step: 19600 Learing: 0.0000065600 Loss: 2.6047 Grad Norm: 6.6930\n",
      "Epoch 0, Step: 19700 Learing: 0.0000079516 Loss: 2.5214 Grad Norm: 6.0040\n",
      "Epoch 0, Step: 19800 Learing: 0.0000090543 Loss: 2.6243 Grad Norm: 5.8246\n",
      "Epoch 0, Step: 19900 Learing: 0.0000097601 Loss: 2.6956 Grad Norm: 6.2209\n",
      "Epoch 0, Step: 20000 Learing: 0.0000100000 Loss: 2.5702 Grad Norm: 7.6473\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 20100 Learing: 0.0000097504 Loss: 2.6320 Grad Norm: 6.0263\n",
      "Epoch 0, Step: 20200 Learing: 0.0000090358 Loss: 2.6699 Grad Norm: 6.4553\n",
      "Epoch 0, Step: 20300 Learing: 0.0000079262 Loss: 2.5596 Grad Norm: 6.6571\n",
      "Epoch 0, Step: 20400 Learing: 0.0000065301 Loss: 2.5146 Grad Norm: 5.2528\n",
      "Epoch 0, Step: 20500 Learing: 0.0000049843 Loss: 2.3379 Grad Norm: 5.7731\n",
      "Epoch 0, Step: 20600 Learing: 0.0000034400 Loss: 2.5345 Grad Norm: 6.3209\n",
      "Epoch 0, Step: 20700 Learing: 0.0000020484 Loss: 2.5758 Grad Norm: 5.6779\n",
      "Epoch 0, Step: 20800 Learing: 0.0000009457 Loss: 2.5645 Grad Norm: 5.1581\n",
      "Epoch 0, Step: 20900 Learing: 0.0000002399 Loss: 2.3623 Grad Norm: 4.9336\n",
      "Epoch 0, Step: 21000 Learing: 0.0000000000 Loss: 2.4620 Grad Norm: 4.9649\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 21100 Learing: 0.0000002496 Loss: 2.4083 Grad Norm: 5.2595\n",
      "Epoch 0, Step: 21200 Learing: 0.0000009642 Loss: 2.7015 Grad Norm: 5.2397\n",
      "Epoch 0, Step: 21300 Learing: 0.0000020738 Loss: 2.6938 Grad Norm: 5.5523\n",
      "Epoch 0, Step: 21400 Learing: 0.0000034699 Loss: 2.4804 Grad Norm: 6.1759\n",
      "Epoch 0, Step: 21500 Learing: 0.0000050157 Loss: 2.5950 Grad Norm: 5.7638\n",
      "Epoch 0, Step: 21600 Learing: 0.0000065600 Loss: 2.6270 Grad Norm: 6.5459\n",
      "Epoch 0, Step: 21700 Learing: 0.0000079516 Loss: 2.6493 Grad Norm: 7.3875\n",
      "Epoch 0, Step: 21800 Learing: 0.0000090543 Loss: 2.6251 Grad Norm: 5.8078\n",
      "Epoch 0, Step: 21900 Learing: 0.0000097601 Loss: 2.5742 Grad Norm: 6.6229\n",
      "Epoch 0, Step: 22000 Learing: 0.0000100000 Loss: 2.5031 Grad Norm: 6.8984\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 22100 Learing: 0.0000097504 Loss: 2.4809 Grad Norm: 6.6237\n",
      "Epoch 0, Step: 22200 Learing: 0.0000090358 Loss: 2.4658 Grad Norm: 5.8605\n",
      "Epoch 0, Step: 22300 Learing: 0.0000079262 Loss: 2.4816 Grad Norm: 6.2116\n",
      "Epoch 0, Step: 22400 Learing: 0.0000065301 Loss: 2.4300 Grad Norm: 5.9651\n",
      "Epoch 0, Step: 22500 Learing: 0.0000049843 Loss: 2.5230 Grad Norm: 6.1076\n",
      "Epoch 0, Step: 22600 Learing: 0.0000034400 Loss: 2.7076 Grad Norm: 6.1193\n",
      "Epoch 0, Step: 22700 Learing: 0.0000020484 Loss: 2.5617 Grad Norm: 5.3261\n",
      "Epoch 0, Step: 22800 Learing: 0.0000009457 Loss: 2.4898 Grad Norm: 4.9204\n",
      "Epoch 0, Step: 22900 Learing: 0.0000002399 Loss: 2.3920 Grad Norm: 5.1134\n",
      "Epoch 0, Step: 23000 Learing: 0.0000000000 Loss: 2.5395 Grad Norm: 6.2875\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 23100 Learing: 0.0000002496 Loss: 2.3130 Grad Norm: 5.0692\n",
      "Epoch 0, Step: 23200 Learing: 0.0000009642 Loss: 2.5387 Grad Norm: 5.6929\n",
      "Epoch 0, Step: 23300 Learing: 0.0000020738 Loss: 2.4526 Grad Norm: 5.0541\n",
      "Epoch 0, Step: 23400 Learing: 0.0000034699 Loss: 2.3455 Grad Norm: 5.6745\n",
      "Epoch 0, Step: 23500 Learing: 0.0000050157 Loss: 2.2364 Grad Norm: 5.4933\n",
      "Epoch 0, Step: 23600 Learing: 0.0000065600 Loss: 2.3174 Grad Norm: 6.2438\n",
      "Epoch 0, Step: 23700 Learing: 0.0000079516 Loss: 2.5129 Grad Norm: 5.7247\n",
      "Epoch 0, Step: 23800 Learing: 0.0000090543 Loss: 2.2839 Grad Norm: 7.3803\n",
      "Epoch 0, Step: 23900 Learing: 0.0000097601 Loss: 2.3703 Grad Norm: 6.3698\n",
      "Epoch 0, Step: 24000 Learing: 0.0000100000 Loss: 2.4827 Grad Norm: 5.5099\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 24100 Learing: 0.0000097504 Loss: 2.5019 Grad Norm: 6.4946\n",
      "Epoch 0, Step: 24200 Learing: 0.0000090358 Loss: 2.4143 Grad Norm: 7.2692\n",
      "Epoch 0, Step: 24300 Learing: 0.0000079262 Loss: 2.5333 Grad Norm: 7.3609\n",
      "Epoch 0, Step: 24400 Learing: 0.0000065301 Loss: 2.3098 Grad Norm: 6.0504\n",
      "Epoch 0, Step: 24500 Learing: 0.0000049843 Loss: 2.3257 Grad Norm: 6.6619\n",
      "Epoch 0, Step: 24600 Learing: 0.0000034400 Loss: 2.4369 Grad Norm: 6.0568\n",
      "Epoch 0, Step: 24700 Learing: 0.0000020484 Loss: 2.1895 Grad Norm: 5.6137\n",
      "Epoch 0, Step: 24800 Learing: 0.0000009457 Loss: 2.2970 Grad Norm: 5.3082\n",
      "Epoch 0, Step: 24900 Learing: 0.0000002399 Loss: 2.5306 Grad Norm: 5.1586\n",
      "Epoch 0, Step: 25000 Learing: 0.0000000000 Loss: 2.5876 Grad Norm: 5.2930\n",
      "中国首都是哪?天\n",
      "Epoch 0, Step: 25100 Learing: 0.0000002496 Loss: 2.5025 Grad Norm: 4.8293\n",
      "Epoch 0, Step: 25200 Learing: 0.0000009642 Loss: 2.4601 Grad Norm: 5.4189\n",
      "Epoch 0, Step: 25300 Learing: 0.0000020738 Loss: 2.4532 Grad Norm: 5.2631\n",
      "Epoch 0, Step: 25400 Learing: 0.0000034699 Loss: 2.4641 Grad Norm: 6.0707\n",
      "Epoch 0, Step: 25500 Learing: 0.0000050157 Loss: 2.3097 Grad Norm: 6.0779\n",
      "Epoch 0, Step: 25600 Learing: 0.0000065600 Loss: 2.3581 Grad Norm: 6.4143\n",
      "Epoch 0, Step: 25700 Learing: 0.0000079516 Loss: 2.2430 Grad Norm: 6.7920\n",
      "Epoch 0, Step: 25800 Learing: 0.0000090543 Loss: 2.4135 Grad Norm: 7.2525\n",
      "Epoch 0, Step: 25900 Learing: 0.0000097601 Loss: 2.3018 Grad Norm: 6.1351\n",
      "Epoch 0, Step: 26000 Learing: 0.0000100000 Loss: 2.4286 Grad Norm: 6.2350\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 26100 Learing: 0.0000097504 Loss: 2.2817 Grad Norm: 6.7450\n",
      "Epoch 0, Step: 26200 Learing: 0.0000090358 Loss: 2.3705 Grad Norm: 6.2161\n",
      "Epoch 0, Step: 26300 Learing: 0.0000079262 Loss: 2.3044 Grad Norm: 5.9885\n",
      "Epoch 0, Step: 26400 Learing: 0.0000065301 Loss: 2.5007 Grad Norm: 5.5305\n",
      "Epoch 0, Step: 26500 Learing: 0.0000049843 Loss: 2.4492 Grad Norm: 6.1966\n",
      "Epoch 0, Step: 26600 Learing: 0.0000034400 Loss: 2.0785 Grad Norm: 5.4292\n",
      "Epoch 0, Step: 26700 Learing: 0.0000020484 Loss: 2.2079 Grad Norm: 5.3467\n",
      "Epoch 0, Step: 26800 Learing: 0.0000009457 Loss: 2.2446 Grad Norm: 5.3805\n",
      "Epoch 0, Step: 26900 Learing: 0.0000002399 Loss: 2.2144 Grad Norm: 5.1071\n",
      "Epoch 0, Step: 27000 Learing: 0.0000000000 Loss: 2.2626 Grad Norm: 5.2744\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 27100 Learing: 0.0000002496 Loss: 2.3369 Grad Norm: 5.1970\n",
      "Epoch 0, Step: 27200 Learing: 0.0000009642 Loss: 2.3507 Grad Norm: 5.6000\n",
      "Epoch 0, Step: 27300 Learing: 0.0000020738 Loss: 2.2634 Grad Norm: 6.0363\n",
      "Epoch 0, Step: 27400 Learing: 0.0000034699 Loss: 2.2073 Grad Norm: 5.9540\n",
      "Epoch 0, Step: 27500 Learing: 0.0000050157 Loss: 2.2417 Grad Norm: 5.6358\n",
      "Epoch 0, Step: 27600 Learing: 0.0000065600 Loss: 2.2460 Grad Norm: 6.1497\n",
      "Epoch 0, Step: 27700 Learing: 0.0000079516 Loss: 2.1948 Grad Norm: 5.3933\n",
      "Epoch 0, Step: 27800 Learing: 0.0000090543 Loss: 2.2333 Grad Norm: 6.0346\n",
      "Epoch 0, Step: 27900 Learing: 0.0000097601 Loss: 2.3441 Grad Norm: 6.8128\n",
      "Epoch 0, Step: 28000 Learing: 0.0000100000 Loss: 2.2163 Grad Norm: 6.1971\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 28100 Learing: 0.0000097504 Loss: 2.2933 Grad Norm: 6.0871\n",
      "Epoch 0, Step: 28200 Learing: 0.0000090358 Loss: 2.3421 Grad Norm: 6.7138\n",
      "Epoch 0, Step: 28300 Learing: 0.0000079262 Loss: 2.2042 Grad Norm: 6.6690\n",
      "Epoch 0, Step: 28400 Learing: 0.0000065301 Loss: 2.2739 Grad Norm: 6.3059\n",
      "Epoch 0, Step: 28500 Learing: 0.0000049843 Loss: 2.2635 Grad Norm: 5.8710\n",
      "Epoch 0, Step: 28600 Learing: 0.0000034400 Loss: 2.1731 Grad Norm: 6.0252\n",
      "Epoch 0, Step: 28700 Learing: 0.0000020484 Loss: 2.2445 Grad Norm: 5.7905\n",
      "Epoch 0, Step: 28800 Learing: 0.0000009457 Loss: 2.3131 Grad Norm: 5.6917\n",
      "Epoch 0, Step: 28900 Learing: 0.0000002399 Loss: 2.2377 Grad Norm: 5.8782\n",
      "Epoch 0, Step: 29000 Learing: 0.0000000000 Loss: 2.2571 Grad Norm: 7.3587\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 29100 Learing: 0.0000002496 Loss: 2.4473 Grad Norm: 5.8972\n",
      "Epoch 0, Step: 29200 Learing: 0.0000009642 Loss: 2.3398 Grad Norm: 5.7741\n",
      "Epoch 0, Step: 29300 Learing: 0.0000020738 Loss: 2.0680 Grad Norm: 4.9850\n",
      "Epoch 0, Step: 29400 Learing: 0.0000034699 Loss: 2.4215 Grad Norm: 6.1622\n",
      "Epoch 0, Step: 29500 Learing: 0.0000050157 Loss: 2.3332 Grad Norm: 5.6789\n",
      "Epoch 0, Step: 29600 Learing: 0.0000065600 Loss: 2.1576 Grad Norm: 5.6256\n",
      "Epoch 0, Step: 29700 Learing: 0.0000079516 Loss: 2.3013 Grad Norm: 6.4215\n",
      "Epoch 0, Step: 29800 Learing: 0.0000090543 Loss: 2.1523 Grad Norm: 6.3262\n",
      "Epoch 0, Step: 29900 Learing: 0.0000097601 Loss: 2.1313 Grad Norm: 5.9954\n",
      "Epoch 0, Step: 30000 Learing: 0.0000100000 Loss: 2.1032 Grad Norm: 6.5651\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 30100 Learing: 0.0000097504 Loss: 2.2049 Grad Norm: 6.4705\n",
      "Epoch 0, Step: 30200 Learing: 0.0000090358 Loss: 2.2158 Grad Norm: 6.3115\n",
      "Epoch 0, Step: 30300 Learing: 0.0000079262 Loss: 2.1697 Grad Norm: 6.1496\n",
      "Epoch 0, Step: 30400 Learing: 0.0000065301 Loss: 2.2376 Grad Norm: 5.3001\n",
      "Epoch 0, Step: 30500 Learing: 0.0000049843 Loss: 2.0399 Grad Norm: 5.7724\n",
      "Epoch 0, Step: 30600 Learing: 0.0000034400 Loss: 2.1982 Grad Norm: 5.5903\n",
      "Epoch 0, Step: 30700 Learing: 0.0000020484 Loss: 2.0740 Grad Norm: 5.4245\n",
      "Epoch 0, Step: 30800 Learing: 0.0000009457 Loss: 2.1539 Grad Norm: 5.1006\n",
      "Epoch 0, Step: 30900 Learing: 0.0000002399 Loss: 2.2422 Grad Norm: 5.3969\n",
      "Epoch 0, Step: 31000 Learing: 0.0000000000 Loss: 2.1058 Grad Norm: 4.7624\n",
      "中国首都是哪??\n",
      "Epoch 0, Step: 31100 Learing: 0.0000002496 Loss: 2.3117 Grad Norm: 4.9438\n",
      "Epoch 0, Step: 31200 Learing: 0.0000009642 Loss: 2.2246 Grad Norm: 5.2084\n",
      "Epoch 0, Step: 31300 Learing: 0.0000020738 Loss: 1.9763 Grad Norm: 4.6930\n",
      "Epoch 0, Step: 31400 Learing: 0.0000034699 Loss: 1.9776 Grad Norm: 5.4456\n",
      "Epoch 0, Step: 31500 Learing: 0.0000050157 Loss: 2.1106 Grad Norm: 5.4763\n",
      "Epoch 0, Step: 31600 Learing: 0.0000065600 Loss: 2.1344 Grad Norm: 5.9866\n",
      "Epoch 0, Step: 31700 Learing: 0.0000079516 Loss: 1.8336 Grad Norm: 6.3685\n",
      "Epoch 0, Step: 31800 Learing: 0.0000090543 Loss: 1.9512 Grad Norm: 5.9555\n",
      "Epoch 0, Step: 31900 Learing: 0.0000097601 Loss: 2.0926 Grad Norm: 6.4100\n",
      "Epoch 0, Step: 32000 Learing: 0.0000100000 Loss: 1.9235 Grad Norm: 5.7281\n",
      "中国首都是哪?武\n",
      "Epoch 0, Step: 32100 Learing: 0.0000097504 Loss: 2.1076 Grad Norm: 5.7640\n",
      "Epoch 0, Step: 32200 Learing: 0.0000090358 Loss: 2.2273 Grad Norm: 6.0609\n",
      "Epoch 0, Step: 32300 Learing: 0.0000079262 Loss: 1.9215 Grad Norm: 5.9502\n",
      "Epoch 0, Step: 32400 Learing: 0.0000065301 Loss: 2.0967 Grad Norm: 5.4955\n",
      "Epoch 0, Step: 32500 Learing: 0.0000049843 Loss: 1.9733 Grad Norm: 5.0575\n",
      "Epoch 0, Step: 32600 Learing: 0.0000034400 Loss: 2.0872 Grad Norm: 5.1737\n",
      "Epoch 0, Step: 32700 Learing: 0.0000020484 Loss: 2.3205 Grad Norm: 5.3133\n",
      "Epoch 0, Step: 32800 Learing: 0.0000009457 Loss: 2.4118 Grad Norm: 5.7978\n",
      "Epoch 0, Step: 32900 Learing: 0.0000002399 Loss: 2.2241 Grad Norm: 5.3285\n",
      "Epoch 0, Step: 33000 Learing: 0.0000000000 Loss: 1.9357 Grad Norm: 4.4270\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 33100 Learing: 0.0000002496 Loss: 2.3426 Grad Norm: 4.8256\n",
      "Epoch 0, Step: 33200 Learing: 0.0000009642 Loss: 2.1502 Grad Norm: 4.7590\n",
      "Epoch 0, Step: 33300 Learing: 0.0000020738 Loss: 2.1337 Grad Norm: 5.7713\n",
      "Epoch 0, Step: 33400 Learing: 0.0000034699 Loss: 2.1441 Grad Norm: 5.3022\n",
      "Epoch 0, Step: 33500 Learing: 0.0000050157 Loss: 2.1724 Grad Norm: 6.0707\n",
      "Epoch 0, Step: 33600 Learing: 0.0000065600 Loss: 2.0374 Grad Norm: 5.3444\n",
      "Epoch 0, Step: 33700 Learing: 0.0000079516 Loss: 2.0765 Grad Norm: 5.8268\n",
      "Epoch 0, Step: 33800 Learing: 0.0000090543 Loss: 2.1840 Grad Norm: 5.7537\n",
      "Epoch 0, Step: 33900 Learing: 0.0000097601 Loss: 2.0278 Grad Norm: 6.0280\n",
      "Epoch 0, Step: 34000 Learing: 0.0000100000 Loss: 2.0682 Grad Norm: 6.1374\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 34100 Learing: 0.0000097504 Loss: 2.0546 Grad Norm: 5.7089\n",
      "Epoch 0, Step: 34200 Learing: 0.0000090358 Loss: 2.0934 Grad Norm: 5.8330\n",
      "Epoch 0, Step: 34300 Learing: 0.0000079262 Loss: 2.1969 Grad Norm: 6.2603\n",
      "Epoch 0, Step: 34400 Learing: 0.0000065301 Loss: 2.2342 Grad Norm: 5.5246\n",
      "Epoch 0, Step: 34500 Learing: 0.0000049843 Loss: 2.0216 Grad Norm: 5.6143\n",
      "Epoch 0, Step: 34600 Learing: 0.0000034400 Loss: 2.0466 Grad Norm: 5.5540\n",
      "Epoch 0, Step: 34700 Learing: 0.0000020484 Loss: 2.0613 Grad Norm: 5.4726\n",
      "Epoch 0, Step: 34800 Learing: 0.0000009457 Loss: 2.1912 Grad Norm: 5.5529\n",
      "Epoch 0, Step: 34900 Learing: 0.0000002399 Loss: 2.0492 Grad Norm: 4.7865\n",
      "Epoch 0, Step: 35000 Learing: 0.0000000000 Loss: 2.1872 Grad Norm: 4.8541\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 35100 Learing: 0.0000002496 Loss: 1.9899 Grad Norm: 4.9188\n",
      "Epoch 0, Step: 35200 Learing: 0.0000009642 Loss: 2.3480 Grad Norm: 5.3552\n",
      "Epoch 0, Step: 35300 Learing: 0.0000020738 Loss: 2.1683 Grad Norm: 5.0990\n",
      "Epoch 0, Step: 35400 Learing: 0.0000034699 Loss: 2.1941 Grad Norm: 5.3203\n",
      "Epoch 0, Step: 35500 Learing: 0.0000050157 Loss: 2.2512 Grad Norm: 5.7181\n",
      "Epoch 0, Step: 35600 Learing: 0.0000065600 Loss: 2.1716 Grad Norm: 5.6135\n",
      "Epoch 0, Step: 35700 Learing: 0.0000079516 Loss: 2.1281 Grad Norm: 5.3247\n",
      "Epoch 0, Step: 35800 Learing: 0.0000090543 Loss: 2.0188 Grad Norm: 6.2868\n",
      "Epoch 0, Step: 35900 Learing: 0.0000097601 Loss: 2.2075 Grad Norm: 6.0696\n",
      "Epoch 0, Step: 36000 Learing: 0.0000100000 Loss: 2.0676 Grad Norm: 6.1476\n",
      "中国首都是哪?1\n",
      "Epoch 0, Step: 36100 Learing: 0.0000097504 Loss: 2.0590 Grad Norm: 5.2881\n",
      "Epoch 0, Step: 36200 Learing: 0.0000090358 Loss: 2.2950 Grad Norm: 6.0826\n",
      "Epoch 0, Step: 36300 Learing: 0.0000079262 Loss: 2.1177 Grad Norm: 5.4321\n",
      "Epoch 0, Step: 36400 Learing: 0.0000065301 Loss: 1.9161 Grad Norm: 5.1485\n",
      "Epoch 0, Step: 36500 Learing: 0.0000049843 Loss: 2.1616 Grad Norm: 5.3884\n",
      "Epoch 0, Step: 36600 Learing: 0.0000034400 Loss: 2.0296 Grad Norm: 5.1165\n",
      "Epoch 0, Step: 36700 Learing: 0.0000020484 Loss: 2.0802 Grad Norm: 4.8564\n",
      "Epoch 0, Step: 36800 Learing: 0.0000009457 Loss: 2.2099 Grad Norm: 4.9717\n",
      "Epoch 0, Step: 36900 Learing: 0.0000002399 Loss: 2.0386 Grad Norm: 4.6071\n",
      "Epoch 0, Step: 37000 Learing: 0.0000000000 Loss: 2.0780 Grad Norm: 4.7012\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 37100 Learing: 0.0000002496 Loss: 2.1024 Grad Norm: 4.9499\n",
      "Epoch 0, Step: 37200 Learing: 0.0000009642 Loss: 2.1225 Grad Norm: 4.7528\n",
      "Epoch 0, Step: 37300 Learing: 0.0000020738 Loss: 2.1838 Grad Norm: 5.7367\n",
      "Epoch 0, Step: 37400 Learing: 0.0000034699 Loss: 1.9498 Grad Norm: 6.0997\n",
      "Epoch 0, Step: 37500 Learing: 0.0000050157 Loss: 2.0347 Grad Norm: 5.1218\n",
      "Epoch 0, Step: 37600 Learing: 0.0000065600 Loss: 2.1631 Grad Norm: 5.2167\n",
      "Epoch 0, Step: 37700 Learing: 0.0000079516 Loss: 2.1446 Grad Norm: 6.2590\n",
      "Epoch 0, Step: 37800 Learing: 0.0000090543 Loss: 1.9768 Grad Norm: 5.4696\n",
      "Epoch 0, Step: 37900 Learing: 0.0000097601 Loss: 2.0388 Grad Norm: 5.6117\n",
      "Epoch 0, Step: 38000 Learing: 0.0000100000 Loss: 2.1461 Grad Norm: 5.6837\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 38100 Learing: 0.0000097504 Loss: 2.0921 Grad Norm: 5.2267\n",
      "Epoch 0, Step: 38200 Learing: 0.0000090358 Loss: 2.0266 Grad Norm: 5.2890\n",
      "Epoch 0, Step: 38300 Learing: 0.0000079262 Loss: 1.9405 Grad Norm: 5.4927\n",
      "Epoch 0, Step: 38400 Learing: 0.0000065301 Loss: 1.9229 Grad Norm: 5.2068\n",
      "Epoch 0, Step: 38500 Learing: 0.0000049843 Loss: 2.1108 Grad Norm: 5.2843\n",
      "Epoch 0, Step: 38600 Learing: 0.0000034400 Loss: 2.0726 Grad Norm: 4.7903\n",
      "Epoch 0, Step: 38700 Learing: 0.0000020484 Loss: 2.0908 Grad Norm: 4.9674\n",
      "Epoch 0, Step: 38800 Learing: 0.0000009457 Loss: 2.1312 Grad Norm: 4.6922\n",
      "Epoch 0, Step: 38900 Learing: 0.0000002399 Loss: 2.0962 Grad Norm: 4.9980\n",
      "Epoch 0, Step: 39000 Learing: 0.0000000000 Loss: 2.2377 Grad Norm: 5.2681\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 39100 Learing: 0.0000002496 Loss: 2.0756 Grad Norm: 4.5761\n",
      "Epoch 0, Step: 39200 Learing: 0.0000009642 Loss: 2.1260 Grad Norm: 4.7292\n",
      "Epoch 0, Step: 39300 Learing: 0.0000020738 Loss: 1.9924 Grad Norm: 4.8312\n",
      "Epoch 0, Step: 39400 Learing: 0.0000034699 Loss: 2.0965 Grad Norm: 5.2620\n",
      "Epoch 0, Step: 39500 Learing: 0.0000050157 Loss: 2.0073 Grad Norm: 5.5723\n",
      "Epoch 0, Step: 39600 Learing: 0.0000065600 Loss: 1.9850 Grad Norm: 5.8441\n",
      "Epoch 0, Step: 39700 Learing: 0.0000079516 Loss: 2.0572 Grad Norm: 5.0117\n",
      "Epoch 0, Step: 39800 Learing: 0.0000090543 Loss: 1.9753 Grad Norm: 5.3211\n",
      "Epoch 0, Step: 39900 Learing: 0.0000097601 Loss: 2.0597 Grad Norm: 6.0618\n",
      "Epoch 0, Step: 40000 Learing: 0.0000100000 Loss: 2.0149 Grad Norm: 6.3273\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 40100 Learing: 0.0000097504 Loss: 2.0689 Grad Norm: 5.4492\n",
      "Epoch 0, Step: 40200 Learing: 0.0000090358 Loss: 2.0197 Grad Norm: 5.1798\n",
      "Epoch 0, Step: 40300 Learing: 0.0000079262 Loss: 2.1582 Grad Norm: 6.0777\n",
      "Epoch 0, Step: 40400 Learing: 0.0000065301 Loss: 2.2424 Grad Norm: 5.4845\n",
      "Epoch 0, Step: 40500 Learing: 0.0000049843 Loss: 2.2550 Grad Norm: 5.0816\n",
      "Epoch 0, Step: 40600 Learing: 0.0000034400 Loss: 2.1336 Grad Norm: 5.2657\n",
      "Epoch 0, Step: 40700 Learing: 0.0000020484 Loss: 2.1603 Grad Norm: 4.8225\n",
      "Epoch 0, Step: 40800 Learing: 0.0000009457 Loss: 2.2061 Grad Norm: 4.7122\n",
      "Epoch 0, Step: 40900 Learing: 0.0000002399 Loss: 2.4046 Grad Norm: 5.2726\n",
      "Epoch 0, Step: 41000 Learing: 0.0000000000 Loss: 2.0844 Grad Norm: 4.4538\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 41100 Learing: 0.0000002496 Loss: 2.2953 Grad Norm: 5.0785\n",
      "Epoch 0, Step: 41200 Learing: 0.0000009642 Loss: 2.2700 Grad Norm: 4.7337\n",
      "Epoch 0, Step: 41300 Learing: 0.0000020738 Loss: 2.1570 Grad Norm: 4.6491\n",
      "Epoch 0, Step: 41400 Learing: 0.0000034699 Loss: 2.2119 Grad Norm: 4.8745\n",
      "Epoch 0, Step: 41500 Learing: 0.0000050157 Loss: 2.1567 Grad Norm: 4.7370\n",
      "Epoch 0, Step: 41600 Learing: 0.0000065600 Loss: 2.1124 Grad Norm: 6.1565\n",
      "Epoch 0, Step: 41700 Learing: 0.0000079516 Loss: 2.3046 Grad Norm: 5.0936\n",
      "Epoch 0, Step: 41800 Learing: 0.0000090543 Loss: 1.8427 Grad Norm: 4.8548\n",
      "Epoch 0, Step: 41900 Learing: 0.0000097601 Loss: 2.0244 Grad Norm: 5.5037\n",
      "Epoch 0, Step: 42000 Learing: 0.0000100000 Loss: 1.9080 Grad Norm: 5.0769\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 42100 Learing: 0.0000097504 Loss: 1.8670 Grad Norm: 5.1772\n",
      "Epoch 0, Step: 42200 Learing: 0.0000090358 Loss: 1.8255 Grad Norm: 5.1285\n",
      "Epoch 0, Step: 42300 Learing: 0.0000079262 Loss: 1.7731 Grad Norm: 4.6839\n",
      "Epoch 0, Step: 42400 Learing: 0.0000065301 Loss: 1.9374 Grad Norm: 5.5680\n",
      "Epoch 0, Step: 42500 Learing: 0.0000049843 Loss: 1.6148 Grad Norm: 4.5152\n",
      "Epoch 0, Step: 42600 Learing: 0.0000034400 Loss: 2.1505 Grad Norm: 4.5807\n",
      "Epoch 0, Step: 42700 Learing: 0.0000020484 Loss: 2.1655 Grad Norm: 5.4100\n",
      "Epoch 0, Step: 42800 Learing: 0.0000009457 Loss: 2.0651 Grad Norm: 4.8238\n",
      "Epoch 0, Step: 42900 Learing: 0.0000002399 Loss: 2.0463 Grad Norm: 4.8549\n",
      "Epoch 0, Step: 43000 Learing: 0.0000000000 Loss: 2.2290 Grad Norm: 5.0520\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 43100 Learing: 0.0000002496 Loss: 2.0208 Grad Norm: 4.4605\n",
      "Epoch 0, Step: 43200 Learing: 0.0000009642 Loss: 2.1366 Grad Norm: 4.6397\n",
      "Epoch 0, Step: 43300 Learing: 0.0000020738 Loss: 2.1218 Grad Norm: 4.6589\n",
      "Epoch 0, Step: 43400 Learing: 0.0000034699 Loss: 2.1579 Grad Norm: 5.2350\n",
      "Epoch 0, Step: 43500 Learing: 0.0000050157 Loss: 2.0220 Grad Norm: 5.2320\n",
      "Epoch 0, Step: 43600 Learing: 0.0000065600 Loss: 2.0877 Grad Norm: 4.7905\n",
      "Epoch 0, Step: 43700 Learing: 0.0000079516 Loss: 2.2179 Grad Norm: 5.3164\n",
      "Epoch 0, Step: 43800 Learing: 0.0000090543 Loss: 2.0877 Grad Norm: 5.3563\n",
      "Epoch 0, Step: 43900 Learing: 0.0000097601 Loss: 2.1768 Grad Norm: 5.1483\n",
      "Epoch 0, Step: 44000 Learing: 0.0000100000 Loss: 2.0421 Grad Norm: 5.0881\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 44100 Learing: 0.0000097504 Loss: 1.9889 Grad Norm: 5.0712\n",
      "Epoch 0, Step: 44200 Learing: 0.0000090358 Loss: 1.9584 Grad Norm: 5.5532\n",
      "Epoch 0, Step: 44300 Learing: 0.0000079262 Loss: 2.1218 Grad Norm: 5.0066\n",
      "Epoch 0, Step: 44400 Learing: 0.0000065301 Loss: 2.1649 Grad Norm: 5.2676\n",
      "Epoch 0, Step: 44500 Learing: 0.0000049843 Loss: 2.0389 Grad Norm: 4.7255\n",
      "Epoch 0, Step: 44600 Learing: 0.0000034400 Loss: 2.0967 Grad Norm: 4.7699\n",
      "Epoch 0, Step: 44700 Learing: 0.0000020484 Loss: 2.1118 Grad Norm: 4.7165\n",
      "Epoch 0, Step: 44800 Learing: 0.0000009457 Loss: 2.2365 Grad Norm: 4.6464\n",
      "Epoch 0, Step: 44900 Learing: 0.0000002399 Loss: 2.0215 Grad Norm: 4.4241\n",
      "Epoch 0, Step: 45000 Learing: 0.0000000000 Loss: 1.9686 Grad Norm: 4.7686\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 45100 Learing: 0.0000002496 Loss: 2.0131 Grad Norm: 4.0716\n",
      "Epoch 0, Step: 45200 Learing: 0.0000009642 Loss: 2.0907 Grad Norm: 4.5983\n",
      "Epoch 0, Step: 45300 Learing: 0.0000020738 Loss: 1.9791 Grad Norm: 4.7001\n",
      "Epoch 0, Step: 45400 Learing: 0.0000034699 Loss: 2.1255 Grad Norm: 4.6789\n",
      "Epoch 0, Step: 45500 Learing: 0.0000050157 Loss: 2.0479 Grad Norm: 5.3253\n",
      "Epoch 0, Step: 45600 Learing: 0.0000065600 Loss: 1.9801 Grad Norm: 4.9340\n",
      "Epoch 0, Step: 45700 Learing: 0.0000079516 Loss: 2.0841 Grad Norm: 5.2714\n",
      "Epoch 0, Step: 45800 Learing: 0.0000090543 Loss: 2.1460 Grad Norm: 5.1728\n",
      "Epoch 0, Step: 45900 Learing: 0.0000097601 Loss: 2.0308 Grad Norm: 4.9101\n",
      "Epoch 0, Step: 46000 Learing: 0.0000100000 Loss: 2.0078 Grad Norm: 5.1026\n",
      "中国首都是哪?三\n",
      "Epoch 0, Step: 46100 Learing: 0.0000097504 Loss: 2.0146 Grad Norm: 5.2781\n",
      "Epoch 0, Step: 46200 Learing: 0.0000090358 Loss: 2.0550 Grad Norm: 5.0696\n",
      "Epoch 0, Step: 46300 Learing: 0.0000079262 Loss: 2.0520 Grad Norm: 5.1926\n",
      "Epoch 0, Step: 46400 Learing: 0.0000065301 Loss: 1.9858 Grad Norm: 5.3019\n",
      "Epoch 0, Step: 46500 Learing: 0.0000049843 Loss: 1.8996 Grad Norm: 4.5783\n",
      "Epoch 0, Step: 46600 Learing: 0.0000034400 Loss: 2.1227 Grad Norm: 4.7551\n",
      "Epoch 0, Step: 46700 Learing: 0.0000020484 Loss: 2.0456 Grad Norm: 5.0865\n",
      "Epoch 0, Step: 46800 Learing: 0.0000009457 Loss: 1.9663 Grad Norm: 4.6308\n",
      "Epoch 0, Step: 46900 Learing: 0.0000002399 Loss: 2.0234 Grad Norm: 4.4647\n",
      "Epoch 0, Step: 47000 Learing: 0.0000000000 Loss: 2.0180 Grad Norm: 4.1636\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 47100 Learing: 0.0000002496 Loss: 2.0419 Grad Norm: 4.4383\n",
      "Epoch 0, Step: 47200 Learing: 0.0000009642 Loss: 1.9732 Grad Norm: 4.1798\n",
      "Epoch 0, Step: 47300 Learing: 0.0000020738 Loss: 2.0763 Grad Norm: 5.3198\n",
      "Epoch 0, Step: 47400 Learing: 0.0000034699 Loss: 2.0301 Grad Norm: 4.6653\n",
      "Epoch 0, Step: 47500 Learing: 0.0000050157 Loss: 1.9250 Grad Norm: 5.2518\n",
      "Epoch 0, Step: 47600 Learing: 0.0000065600 Loss: 2.1140 Grad Norm: 4.8249\n",
      "Epoch 0, Step: 47700 Learing: 0.0000079516 Loss: 2.0556 Grad Norm: 4.9998\n",
      "Epoch 0, Step: 47800 Learing: 0.0000090543 Loss: 2.0420 Grad Norm: 4.9263\n",
      "Epoch 0, Step: 47900 Learing: 0.0000097601 Loss: 2.0230 Grad Norm: 5.2415\n",
      "Epoch 0, Step: 48000 Learing: 0.0000100000 Loss: 1.9554 Grad Norm: 5.1103\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 48100 Learing: 0.0000097504 Loss: 1.9726 Grad Norm: 5.3243\n",
      "Epoch 0, Step: 48200 Learing: 0.0000090358 Loss: 1.9947 Grad Norm: 5.4457\n",
      "Epoch 0, Step: 48300 Learing: 0.0000079262 Loss: 2.0696 Grad Norm: 5.6047\n",
      "Epoch 0, Step: 48400 Learing: 0.0000065301 Loss: 2.2207 Grad Norm: 4.8098\n",
      "Epoch 0, Step: 48500 Learing: 0.0000049843 Loss: 2.1589 Grad Norm: 5.5471\n",
      "Epoch 0, Step: 48600 Learing: 0.0000034400 Loss: 2.1113 Grad Norm: 5.1495\n",
      "Epoch 0, Step: 48700 Learing: 0.0000020484 Loss: 2.0369 Grad Norm: 4.5448\n",
      "Epoch 0, Step: 48800 Learing: 0.0000009457 Loss: 2.0384 Grad Norm: 4.1774\n",
      "Epoch 0, Step: 48900 Learing: 0.0000002399 Loss: 2.0951 Grad Norm: 4.6077\n",
      "Epoch 0, Step: 49000 Learing: 0.0000000000 Loss: 2.1468 Grad Norm: 4.7393\n",
      "中国首都是哪?b\n",
      "Epoch 0, Step: 49100 Learing: 0.0000002496 Loss: 1.9259 Grad Norm: 4.3626\n",
      "Epoch 0, Step: 49200 Learing: 0.0000009642 Loss: 1.9057 Grad Norm: 5.1282\n",
      "Epoch 0, Step: 49300 Learing: 0.0000020738 Loss: 2.0839 Grad Norm: 5.2847\n",
      "Epoch 0, Step: 49400 Learing: 0.0000034699 Loss: 2.1244 Grad Norm: 4.9693\n",
      "Epoch 0, Step: 49500 Learing: 0.0000050157 Loss: 1.9885 Grad Norm: 4.6954\n",
      "Epoch 0, Step: 49600 Learing: 0.0000065600 Loss: 1.8483 Grad Norm: 4.8801\n",
      "Epoch 0, Step: 49700 Learing: 0.0000079516 Loss: 2.3952 Grad Norm: 5.2884\n",
      "Epoch 0, Step: 49800 Learing: 0.0000090543 Loss: 2.0788 Grad Norm: 4.9449\n",
      "Epoch 0, Step: 49900 Learing: 0.0000097601 Loss: 1.9400 Grad Norm: 5.0793\n",
      "Epoch 0, Step: 50000 Learing: 0.0000100000 Loss: 2.2231 Grad Norm: 5.4250\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 50100 Learing: 0.0000097504 Loss: 2.0671 Grad Norm: 4.8442\n",
      "Epoch 0, Step: 50200 Learing: 0.0000090358 Loss: 1.9381 Grad Norm: 5.4141\n",
      "Epoch 0, Step: 50300 Learing: 0.0000079262 Loss: 2.0077 Grad Norm: 4.7905\n",
      "Epoch 0, Step: 50400 Learing: 0.0000065301 Loss: 1.9141 Grad Norm: 4.7373\n",
      "Epoch 0, Step: 50500 Learing: 0.0000049843 Loss: 1.8206 Grad Norm: 5.0718\n",
      "Epoch 0, Step: 50600 Learing: 0.0000034400 Loss: 1.7517 Grad Norm: 4.7733\n",
      "Epoch 0, Step: 50700 Learing: 0.0000020484 Loss: 1.6450 Grad Norm: 4.7018\n",
      "Epoch 0, Step: 50800 Learing: 0.0000009457 Loss: 1.8284 Grad Norm: 4.6766\n",
      "Epoch 0, Step: 50900 Learing: 0.0000002399 Loss: 2.0336 Grad Norm: 4.6576\n",
      "Epoch 0, Step: 51000 Learing: 0.0000000000 Loss: 2.0869 Grad Norm: 5.1092\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 51100 Learing: 0.0000002496 Loss: 2.0784 Grad Norm: 4.5741\n",
      "Epoch 0, Step: 51200 Learing: 0.0000009642 Loss: 2.1008 Grad Norm: 4.2405\n",
      "Epoch 0, Step: 51300 Learing: 0.0000020738 Loss: 1.9544 Grad Norm: 4.5942\n",
      "Epoch 0, Step: 51400 Learing: 0.0000034699 Loss: 2.0528 Grad Norm: 4.8024\n",
      "Epoch 0, Step: 51500 Learing: 0.0000050157 Loss: 2.0245 Grad Norm: 4.8732\n",
      "Epoch 0, Step: 51600 Learing: 0.0000065600 Loss: 2.0042 Grad Norm: 4.6720\n",
      "Epoch 0, Step: 51700 Learing: 0.0000079516 Loss: 2.0345 Grad Norm: 4.9004\n",
      "Epoch 0, Step: 51800 Learing: 0.0000090543 Loss: 1.9937 Grad Norm: 4.9790\n",
      "Epoch 0, Step: 51900 Learing: 0.0000097601 Loss: 2.2007 Grad Norm: 5.0630\n",
      "Epoch 0, Step: 52000 Learing: 0.0000100000 Loss: 2.0858 Grad Norm: 5.0814\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 52100 Learing: 0.0000097504 Loss: 1.8767 Grad Norm: 4.9166\n",
      "Epoch 0, Step: 52200 Learing: 0.0000090358 Loss: 2.0634 Grad Norm: 4.9389\n",
      "Epoch 0, Step: 52300 Learing: 0.0000079262 Loss: 2.0941 Grad Norm: 4.9110\n",
      "Epoch 0, Step: 52400 Learing: 0.0000065301 Loss: 1.9849 Grad Norm: 4.8839\n",
      "Epoch 0, Step: 52500 Learing: 0.0000049843 Loss: 2.1266 Grad Norm: 4.9122\n",
      "Epoch 0, Step: 52600 Learing: 0.0000034400 Loss: 1.8851 Grad Norm: 4.5798\n",
      "Epoch 0, Step: 52700 Learing: 0.0000020484 Loss: 1.9734 Grad Norm: 4.1913\n",
      "Epoch 0, Step: 52800 Learing: 0.0000009457 Loss: 2.0069 Grad Norm: 4.6609\n",
      "Epoch 0, Step: 52900 Learing: 0.0000002399 Loss: 2.0595 Grad Norm: 4.3024\n",
      "Epoch 0, Step: 53000 Learing: 0.0000000000 Loss: 2.0196 Grad Norm: 4.6763\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 53100 Learing: 0.0000002496 Loss: 1.8915 Grad Norm: 4.2859\n",
      "Epoch 0, Step: 53200 Learing: 0.0000009642 Loss: 2.0997 Grad Norm: 4.5230\n",
      "Epoch 0, Step: 53300 Learing: 0.0000020738 Loss: 1.8658 Grad Norm: 4.2352\n",
      "Epoch 0, Step: 53400 Learing: 0.0000034699 Loss: 1.9504 Grad Norm: 4.7295\n",
      "Epoch 0, Step: 53500 Learing: 0.0000050157 Loss: 1.9992 Grad Norm: 4.8259\n",
      "Epoch 0, Step: 53600 Learing: 0.0000065600 Loss: 1.9461 Grad Norm: 4.7614\n",
      "Epoch 0, Step: 53700 Learing: 0.0000079516 Loss: 2.0878 Grad Norm: 4.6565\n",
      "Epoch 0, Step: 53800 Learing: 0.0000090543 Loss: 1.9414 Grad Norm: 4.6963\n",
      "Epoch 0, Step: 53900 Learing: 0.0000097601 Loss: 1.9341 Grad Norm: 4.7572\n",
      "Epoch 0, Step: 54000 Learing: 0.0000100000 Loss: 2.0508 Grad Norm: 5.0864\n",
      "中国首都是哪?武\n",
      "Epoch 0, Step: 54100 Learing: 0.0000097504 Loss: 2.0029 Grad Norm: 4.9142\n",
      "Epoch 0, Step: 54200 Learing: 0.0000090358 Loss: 2.0865 Grad Norm: 4.9405\n",
      "Epoch 0, Step: 54300 Learing: 0.0000079262 Loss: 2.0268 Grad Norm: 4.4924\n",
      "Epoch 0, Step: 54400 Learing: 0.0000065301 Loss: 2.0541 Grad Norm: 5.0139\n",
      "Epoch 0, Step: 54500 Learing: 0.0000049843 Loss: 2.0915 Grad Norm: 4.7044\n",
      "Epoch 0, Step: 54600 Learing: 0.0000034400 Loss: 1.8586 Grad Norm: 4.5779\n",
      "Epoch 0, Step: 54700 Learing: 0.0000020484 Loss: 2.0214 Grad Norm: 4.3600\n",
      "Epoch 0, Step: 54800 Learing: 0.0000009457 Loss: 1.9769 Grad Norm: 4.2639\n",
      "Epoch 0, Step: 54900 Learing: 0.0000002399 Loss: 2.0464 Grad Norm: 4.2162\n",
      "Epoch 0, Step: 55000 Learing: 0.0000000000 Loss: 2.0403 Grad Norm: 4.4393\n",
      "中国首都是哪?不\n",
      "Epoch 0, Step: 55100 Learing: 0.0000002496 Loss: 1.9972 Grad Norm: 4.2183\n",
      "Epoch 0, Step: 55200 Learing: 0.0000009642 Loss: 1.8704 Grad Norm: 4.1809\n",
      "Epoch 0, Step: 55300 Learing: 0.0000020738 Loss: 1.9205 Grad Norm: 4.6171\n",
      "Epoch 0, Step: 55400 Learing: 0.0000034699 Loss: 1.9703 Grad Norm: 4.3926\n",
      "Epoch 0, Step: 55500 Learing: 0.0000050157 Loss: 1.9324 Grad Norm: 5.0202\n",
      "Epoch 0, Step: 55600 Learing: 0.0000065600 Loss: 2.0236 Grad Norm: 4.7127\n",
      "Epoch 0, Step: 55700 Learing: 0.0000079516 Loss: 1.9767 Grad Norm: 4.8553\n",
      "Epoch 0, Step: 55800 Learing: 0.0000090543 Loss: 1.9387 Grad Norm: 4.6461\n",
      "Epoch 0, Step: 55900 Learing: 0.0000097601 Loss: 1.9015 Grad Norm: 5.5148\n",
      "Epoch 0, Step: 56000 Learing: 0.0000100000 Loss: 2.0171 Grad Norm: 5.3044\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 56100 Learing: 0.0000097504 Loss: 2.0018 Grad Norm: 4.8141\n",
      "Epoch 0, Step: 56200 Learing: 0.0000090358 Loss: 1.9011 Grad Norm: 4.7780\n",
      "Epoch 0, Step: 56300 Learing: 0.0000079262 Loss: 1.9514 Grad Norm: 5.0257\n",
      "Epoch 0, Step: 56400 Learing: 0.0000065301 Loss: 2.2017 Grad Norm: 4.6949\n",
      "Epoch 0, Step: 56500 Learing: 0.0000049843 Loss: 2.1259 Grad Norm: 5.1409\n",
      "Epoch 0, Step: 56600 Learing: 0.0000034400 Loss: 2.1505 Grad Norm: 4.3161\n",
      "Epoch 0, Step: 56700 Learing: 0.0000020484 Loss: 2.0762 Grad Norm: 4.3117\n",
      "Epoch 0, Step: 56800 Learing: 0.0000009457 Loss: 1.7905 Grad Norm: 4.3807\n",
      "Epoch 0, Step: 56900 Learing: 0.0000002399 Loss: 1.9333 Grad Norm: 4.3355\n",
      "Epoch 0, Step: 57000 Learing: 0.0000000000 Loss: 1.9191 Grad Norm: 5.0151\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 57100 Learing: 0.0000002496 Loss: 2.0056 Grad Norm: 4.5712\n",
      "Epoch 0, Step: 57200 Learing: 0.0000009642 Loss: 1.9670 Grad Norm: 4.2884\n",
      "Epoch 0, Step: 57300 Learing: 0.0000020738 Loss: 2.1186 Grad Norm: 4.8501\n",
      "Epoch 0, Step: 57400 Learing: 0.0000034699 Loss: 1.9839 Grad Norm: 4.2849\n",
      "Epoch 0, Step: 57500 Learing: 0.0000050157 Loss: 1.8501 Grad Norm: 4.8113\n",
      "Epoch 0, Step: 57600 Learing: 0.0000065600 Loss: 2.1124 Grad Norm: 4.8856\n",
      "Epoch 0, Step: 57700 Learing: 0.0000079516 Loss: 1.9771 Grad Norm: 4.7493\n",
      "Epoch 0, Step: 57800 Learing: 0.0000090543 Loss: 2.0124 Grad Norm: 5.1746\n",
      "Epoch 0, Step: 57900 Learing: 0.0000097601 Loss: 1.9834 Grad Norm: 4.8214\n",
      "Epoch 0, Step: 58000 Learing: 0.0000100000 Loss: 1.8799 Grad Norm: 4.9555\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 58100 Learing: 0.0000097504 Loss: 1.9385 Grad Norm: 4.8763\n",
      "Epoch 0, Step: 58200 Learing: 0.0000090358 Loss: 1.9342 Grad Norm: 4.8989\n",
      "Epoch 0, Step: 58300 Learing: 0.0000079262 Loss: 2.0120 Grad Norm: 4.8118\n",
      "Epoch 0, Step: 58400 Learing: 0.0000065301 Loss: 2.1020 Grad Norm: 5.1154\n",
      "Epoch 0, Step: 58500 Learing: 0.0000049843 Loss: 1.8575 Grad Norm: 4.4832\n",
      "Epoch 0, Step: 58600 Learing: 0.0000034400 Loss: 2.0455 Grad Norm: 4.3273\n",
      "Epoch 0, Step: 58700 Learing: 0.0000020484 Loss: 1.9890 Grad Norm: 4.4009\n",
      "Epoch 0, Step: 58800 Learing: 0.0000009457 Loss: 1.9519 Grad Norm: 4.1001\n",
      "Epoch 0, Step: 58900 Learing: 0.0000002399 Loss: 1.9841 Grad Norm: 4.4456\n",
      "Epoch 0, Step: 59000 Learing: 0.0000000000 Loss: 2.0246 Grad Norm: 5.4500\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 59100 Learing: 0.0000002496 Loss: 1.9171 Grad Norm: 4.1659\n",
      "Epoch 0, Step: 59200 Learing: 0.0000009642 Loss: 1.8104 Grad Norm: 4.5652\n",
      "Epoch 0, Step: 59300 Learing: 0.0000020738 Loss: 1.8392 Grad Norm: 4.2784\n",
      "Epoch 0, Step: 59400 Learing: 0.0000034699 Loss: 2.0794 Grad Norm: 4.4441\n",
      "Epoch 0, Step: 59500 Learing: 0.0000050157 Loss: 2.0369 Grad Norm: 4.6345\n",
      "Epoch 0, Step: 59600 Learing: 0.0000065600 Loss: 1.9278 Grad Norm: 4.9101\n",
      "Epoch 0, Step: 59700 Learing: 0.0000079516 Loss: 2.0044 Grad Norm: 4.9958\n",
      "Epoch 0, Step: 59800 Learing: 0.0000090543 Loss: 2.0019 Grad Norm: 5.1081\n",
      "Epoch 0, Step: 59900 Learing: 0.0000097601 Loss: 2.0148 Grad Norm: 5.0371\n",
      "Epoch 0, Step: 60000 Learing: 0.0000100000 Loss: 1.9809 Grad Norm: 5.0228\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 60100 Learing: 0.0000097504 Loss: 2.0479 Grad Norm: 4.9937\n",
      "Epoch 0, Step: 60200 Learing: 0.0000090358 Loss: 1.8641 Grad Norm: 4.6190\n",
      "Epoch 0, Step: 60300 Learing: 0.0000079262 Loss: 2.0568 Grad Norm: 4.4323\n",
      "Epoch 0, Step: 60400 Learing: 0.0000065301 Loss: 2.0120 Grad Norm: 4.6110\n",
      "Epoch 0, Step: 60500 Learing: 0.0000049843 Loss: 2.0517 Grad Norm: 4.1813\n",
      "Epoch 0, Step: 60600 Learing: 0.0000034400 Loss: 1.9225 Grad Norm: 5.0933\n",
      "Epoch 0, Step: 60700 Learing: 0.0000020484 Loss: 1.9265 Grad Norm: 4.2651\n",
      "Epoch 0, Step: 60800 Learing: 0.0000009457 Loss: 1.9801 Grad Norm: 4.1541\n",
      "Epoch 0, Step: 60900 Learing: 0.0000002399 Loss: 1.9827 Grad Norm: 4.4060\n",
      "Epoch 0, Step: 61000 Learing: 0.0000000000 Loss: 2.0825 Grad Norm: 5.3461\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 61100 Learing: 0.0000002496 Loss: 1.8388 Grad Norm: 4.7034\n",
      "Epoch 0, Step: 61200 Learing: 0.0000009642 Loss: 1.8966 Grad Norm: 4.1273\n",
      "Epoch 0, Step: 61300 Learing: 0.0000020738 Loss: 1.9985 Grad Norm: 4.2791\n",
      "Epoch 0, Step: 61400 Learing: 0.0000034699 Loss: 2.2431 Grad Norm: 4.4962\n",
      "Epoch 0, Step: 61500 Learing: 0.0000050157 Loss: 1.7449 Grad Norm: 4.6164\n",
      "Epoch 0, Step: 61600 Learing: 0.0000065600 Loss: 2.0902 Grad Norm: 4.5894\n",
      "Epoch 0, Step: 61700 Learing: 0.0000079516 Loss: 2.1485 Grad Norm: 4.8884\n",
      "Epoch 0, Step: 61800 Learing: 0.0000090543 Loss: 2.0404 Grad Norm: 5.0041\n",
      "Epoch 0, Step: 61900 Learing: 0.0000097601 Loss: 1.9036 Grad Norm: 4.7407\n",
      "Epoch 0, Step: 62000 Learing: 0.0000100000 Loss: 1.9433 Grad Norm: 4.6964\n",
      "中国首都是哪?dd\n",
      "Epoch 0, Step: 62100 Learing: 0.0000097504 Loss: 1.9105 Grad Norm: 5.4580\n",
      "Epoch 0, Step: 62200 Learing: 0.0000090358 Loss: 1.8347 Grad Norm: 4.9045\n",
      "Epoch 0, Step: 62300 Learing: 0.0000079262 Loss: 2.0649 Grad Norm: 4.7537\n",
      "Epoch 0, Step: 62400 Learing: 0.0000065301 Loss: 1.9637 Grad Norm: 4.7646\n",
      "Epoch 0, Step: 62500 Learing: 0.0000049843 Loss: 1.8924 Grad Norm: 4.7206\n",
      "Epoch 0, Step: 62600 Learing: 0.0000034400 Loss: 1.9625 Grad Norm: 4.3929\n",
      "Epoch 0, Step: 62700 Learing: 0.0000020484 Loss: 1.8986 Grad Norm: 4.6037\n",
      "Epoch 0, Step: 62800 Learing: 0.0000009457 Loss: 2.2383 Grad Norm: 4.3920\n",
      "Epoch 0, Step: 62900 Learing: 0.0000002399 Loss: 1.7448 Grad Norm: 4.1138\n",
      "Epoch 0, Step: 63000 Learing: 0.0000000000 Loss: 1.9079 Grad Norm: 4.1694\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 63100 Learing: 0.0000002496 Loss: 2.0481 Grad Norm: 3.9961\n",
      "Epoch 0, Step: 63200 Learing: 0.0000009642 Loss: 2.0054 Grad Norm: 4.1372\n",
      "Epoch 0, Step: 63300 Learing: 0.0000020738 Loss: 1.8518 Grad Norm: 4.1660\n",
      "Epoch 0, Step: 63400 Learing: 0.0000034699 Loss: 2.0630 Grad Norm: 4.3445\n",
      "Epoch 0, Step: 63500 Learing: 0.0000050157 Loss: 2.0678 Grad Norm: 4.4268\n",
      "Epoch 0, Step: 63600 Learing: 0.0000065600 Loss: 1.9567 Grad Norm: 4.5619\n",
      "Epoch 0, Step: 63700 Learing: 0.0000079516 Loss: 1.9771 Grad Norm: 4.8168\n",
      "Epoch 0, Step: 63800 Learing: 0.0000090543 Loss: 2.0152 Grad Norm: 4.6131\n",
      "Epoch 0, Step: 63900 Learing: 0.0000097601 Loss: 1.5563 Grad Norm: 7.4771\n",
      "Epoch 0, Step: 64000 Learing: 0.0000100000 Loss: 1.8105 Grad Norm: 4.9987\n",
      "中国首都是哪?中\n",
      "Epoch 0, Step: 64100 Learing: 0.0000097504 Loss: 1.7413 Grad Norm: 4.5028\n",
      "Epoch 0, Step: 64200 Learing: 0.0000090358 Loss: 2.0740 Grad Norm: 4.4333\n",
      "Epoch 0, Step: 64300 Learing: 0.0000079262 Loss: 0.7851 Grad Norm: 6.2737\n",
      "Epoch 0, Step: 64400 Learing: 0.0000065301 Loss: 1.1937 Grad Norm: 7.5382\n",
      "Epoch 0, Step: 64500 Learing: 0.0000049843 Loss: 2.4721 Grad Norm: 4.5074\n",
      "Epoch 0, Step: 64600 Learing: 0.0000034400 Loss: 2.3138 Grad Norm: 5.1933\n",
      "Epoch 0, Step: 64700 Learing: 0.0000020484 Loss: 2.0437 Grad Norm: 4.5506\n",
      "Epoch 0, Step: 64800 Learing: 0.0000009457 Loss: 2.0659 Grad Norm: 5.9635\n",
      "Epoch 0, Step: 64900 Learing: 0.0000002399 Loss: 2.1727 Grad Norm: 3.9947\n",
      "Epoch 0, Step: 65000 Learing: 0.0000000000 Loss: 2.0562 Grad Norm: 4.7619\n",
      "中国首都是哪?8\n",
      "Epoch 0, Step: 65100 Learing: 0.0000002496 Loss: 2.0486 Grad Norm: 4.6077\n",
      "Epoch 0, Step: 65200 Learing: 0.0000009642 Loss: 2.1848 Grad Norm: 4.2013\n",
      "Epoch 0, Step: 65300 Learing: 0.0000020738 Loss: 2.2319 Grad Norm: 4.2980\n",
      "Epoch 0, Step: 65400 Learing: 0.0000034699 Loss: 2.0920 Grad Norm: 4.1339\n",
      "Epoch 0, Step: 65500 Learing: 0.0000050157 Loss: 2.1226 Grad Norm: 4.5476\n",
      "Epoch 0, Step: 65600 Learing: 0.0000065600 Loss: 2.0217 Grad Norm: 4.2952\n",
      "Epoch 0, Step: 65700 Learing: 0.0000079516 Loss: 1.9340 Grad Norm: 4.2282\n",
      "Epoch 0, Step: 65800 Learing: 0.0000090543 Loss: 2.1089 Grad Norm: 4.8980\n",
      "Epoch 0, Step: 65900 Learing: 0.0000097601 Loss: 1.9415 Grad Norm: 4.9074\n",
      "Epoch 0, Step: 66000 Learing: 0.0000100000 Loss: 1.7950 Grad Norm: 4.0591\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 66100 Learing: 0.0000097504 Loss: 2.0990 Grad Norm: 4.7775\n",
      "Epoch 0, Step: 66200 Learing: 0.0000090358 Loss: 2.1007 Grad Norm: 4.8769\n",
      "Epoch 0, Step: 66300 Learing: 0.0000079262 Loss: 1.8661 Grad Norm: 4.3617\n",
      "Epoch 0, Step: 66400 Learing: 0.0000065301 Loss: 2.0737 Grad Norm: 4.1667\n",
      "Epoch 0, Step: 66500 Learing: 0.0000049843 Loss: 2.2268 Grad Norm: 4.2903\n",
      "Epoch 0, Step: 66600 Learing: 0.0000034400 Loss: 2.2637 Grad Norm: 4.0732\n",
      "Epoch 0, Step: 66700 Learing: 0.0000020484 Loss: 2.0450 Grad Norm: 4.1159\n",
      "Epoch 0, Step: 66800 Learing: 0.0000009457 Loss: 2.0699 Grad Norm: 3.9231\n",
      "Epoch 0, Step: 66900 Learing: 0.0000002399 Loss: 2.2171 Grad Norm: 4.2280\n",
      "Epoch 0, Step: 67000 Learing: 0.0000000000 Loss: 1.9129 Grad Norm: 3.6570\n",
      "中国首都是哪??\n",
      "Epoch 0, Step: 67100 Learing: 0.0000002496 Loss: 1.8076 Grad Norm: 4.1037\n",
      "Epoch 0, Step: 67200 Learing: 0.0000009642 Loss: 2.0161 Grad Norm: 4.1230\n",
      "Epoch 0, Step: 67300 Learing: 0.0000020738 Loss: 1.8468 Grad Norm: 4.0822\n",
      "Epoch 0, Step: 67400 Learing: 0.0000034699 Loss: 1.9814 Grad Norm: 4.6676\n",
      "Epoch 0, Step: 67500 Learing: 0.0000050157 Loss: 1.8546 Grad Norm: 4.0723\n",
      "Epoch 0, Step: 67600 Learing: 0.0000065600 Loss: 1.8654 Grad Norm: 4.1679\n",
      "Epoch 0, Step: 67700 Learing: 0.0000079516 Loss: 2.0234 Grad Norm: 4.3537\n",
      "Epoch 0, Step: 67800 Learing: 0.0000090543 Loss: 1.9331 Grad Norm: 4.6567\n",
      "Epoch 0, Step: 67900 Learing: 0.0000097601 Loss: 1.9771 Grad Norm: 4.5271\n",
      "Epoch 0, Step: 68000 Learing: 0.0000100000 Loss: 2.0274 Grad Norm: 4.3676\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 68100 Learing: 0.0000097504 Loss: 1.9809 Grad Norm: 4.4480\n",
      "Epoch 0, Step: 68200 Learing: 0.0000090358 Loss: 2.0218 Grad Norm: 4.4022\n",
      "Epoch 0, Step: 68300 Learing: 0.0000079262 Loss: 1.7965 Grad Norm: 4.2987\n",
      "Epoch 0, Step: 68400 Learing: 0.0000065301 Loss: 1.9525 Grad Norm: 4.2394\n",
      "Epoch 0, Step: 68500 Learing: 0.0000049843 Loss: 1.9634 Grad Norm: 4.5329\n",
      "Epoch 0, Step: 68600 Learing: 0.0000034400 Loss: 1.9548 Grad Norm: 4.4067\n",
      "Epoch 0, Step: 68700 Learing: 0.0000020484 Loss: 1.8477 Grad Norm: 4.1459\n",
      "Epoch 0, Step: 68800 Learing: 0.0000009457 Loss: 1.8783 Grad Norm: 3.9459\n",
      "Epoch 0, Step: 68900 Learing: 0.0000002399 Loss: 1.9112 Grad Norm: 3.8032\n",
      "Epoch 0, Step: 69000 Learing: 0.0000000000 Loss: 1.9720 Grad Norm: 4.0773\n",
      "中国首都是哪?66\n",
      "Epoch 0, Step: 69100 Learing: 0.0000002496 Loss: 2.0033 Grad Norm: 4.3060\n",
      "Epoch 0, Step: 69200 Learing: 0.0000009642 Loss: 1.8901 Grad Norm: 4.0774\n",
      "Epoch 0, Step: 69300 Learing: 0.0000020738 Loss: 2.0213 Grad Norm: 4.3672\n",
      "Epoch 0, Step: 69400 Learing: 0.0000034699 Loss: 1.8345 Grad Norm: 4.4467\n",
      "Epoch 0, Step: 69500 Learing: 0.0000050157 Loss: 1.9004 Grad Norm: 4.1708\n",
      "Epoch 0, Step: 69600 Learing: 0.0000065600 Loss: 1.9601 Grad Norm: 4.3981\n",
      "Epoch 0, Step: 69700 Learing: 0.0000079516 Loss: 2.0485 Grad Norm: 4.4140\n",
      "Epoch 0, Step: 69800 Learing: 0.0000090543 Loss: 1.9124 Grad Norm: 4.5892\n",
      "Epoch 0, Step: 69900 Learing: 0.0000097601 Loss: 1.8781 Grad Norm: 4.2608\n",
      "Epoch 0, Step: 70000 Learing: 0.0000100000 Loss: 1.9059 Grad Norm: 4.3965\n",
      "中国首都是哪?女\n",
      "Epoch 0, Step: 70100 Learing: 0.0000097504 Loss: 1.9145 Grad Norm: 4.6648\n",
      "Epoch 0, Step: 70200 Learing: 0.0000090358 Loss: 2.0529 Grad Norm: 4.6472\n",
      "Epoch 0, Step: 70300 Learing: 0.0000079262 Loss: 1.9132 Grad Norm: 4.2442\n",
      "Epoch 0, Step: 70400 Learing: 0.0000065301 Loss: 2.1137 Grad Norm: 4.5436\n",
      "Epoch 0, Step: 70500 Learing: 0.0000049843 Loss: 2.0268 Grad Norm: 4.3680\n",
      "Epoch 0, Step: 70600 Learing: 0.0000034400 Loss: 1.8302 Grad Norm: 4.0548\n",
      "Epoch 0, Step: 70700 Learing: 0.0000020484 Loss: 1.9362 Grad Norm: 4.0522\n",
      "Epoch 0, Step: 70800 Learing: 0.0000009457 Loss: 1.9756 Grad Norm: 4.0033\n",
      "Epoch 0, Step: 70900 Learing: 0.0000002399 Loss: 1.8891 Grad Norm: 3.9884\n",
      "Epoch 0, Step: 71000 Learing: 0.0000000000 Loss: 2.1462 Grad Norm: 4.3587\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 71100 Learing: 0.0000002496 Loss: 1.7053 Grad Norm: 3.6119\n",
      "Epoch 0, Step: 71200 Learing: 0.0000009642 Loss: 1.8445 Grad Norm: 4.0199\n",
      "Epoch 0, Step: 71300 Learing: 0.0000020738 Loss: 2.2124 Grad Norm: 4.5417\n",
      "Epoch 0, Step: 71400 Learing: 0.0000034699 Loss: 2.2746 Grad Norm: 4.5036\n",
      "Epoch 0, Step: 71500 Learing: 0.0000050157 Loss: 1.9979 Grad Norm: 4.3081\n",
      "Epoch 0, Step: 71600 Learing: 0.0000065600 Loss: 2.0216 Grad Norm: 4.6042\n",
      "Epoch 0, Step: 71700 Learing: 0.0000079516 Loss: 2.0783 Grad Norm: 4.2540\n",
      "Epoch 0, Step: 71800 Learing: 0.0000090543 Loss: 2.0439 Grad Norm: 4.2339\n",
      "Epoch 0, Step: 71900 Learing: 0.0000097601 Loss: 2.0312 Grad Norm: 4.2270\n",
      "Epoch 0, Step: 72000 Learing: 0.0000100000 Loss: 1.8395 Grad Norm: 5.2217\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 72100 Learing: 0.0000097504 Loss: 2.0311 Grad Norm: 4.2373\n",
      "Epoch 0, Step: 72200 Learing: 0.0000090358 Loss: 2.0604 Grad Norm: 4.4821\n",
      "Epoch 0, Step: 72300 Learing: 0.0000079262 Loss: 1.8419 Grad Norm: 4.3012\n",
      "Epoch 0, Step: 72400 Learing: 0.0000065301 Loss: 1.9628 Grad Norm: 4.4683\n",
      "Epoch 0, Step: 72500 Learing: 0.0000049843 Loss: 1.8748 Grad Norm: 4.1349\n",
      "Epoch 0, Step: 72600 Learing: 0.0000034400 Loss: 1.9943 Grad Norm: 4.2346\n",
      "Epoch 0, Step: 72700 Learing: 0.0000020484 Loss: 2.0762 Grad Norm: 4.1539\n",
      "Epoch 0, Step: 72800 Learing: 0.0000009457 Loss: 1.8751 Grad Norm: 3.9438\n",
      "Epoch 0, Step: 72900 Learing: 0.0000002399 Loss: 1.9783 Grad Norm: 4.0029\n",
      "Epoch 0, Step: 73000 Learing: 0.0000000000 Loss: 1.9197 Grad Norm: 4.1443\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 73100 Learing: 0.0000002496 Loss: 2.0913 Grad Norm: 3.8116\n",
      "Epoch 0, Step: 73200 Learing: 0.0000009642 Loss: 2.0628 Grad Norm: 3.7729\n",
      "Epoch 0, Step: 73300 Learing: 0.0000020738 Loss: 2.1103 Grad Norm: 3.8087\n",
      "Epoch 0, Step: 73400 Learing: 0.0000034699 Loss: 1.9121 Grad Norm: 3.9994\n",
      "Epoch 0, Step: 73500 Learing: 0.0000050157 Loss: 2.0721 Grad Norm: 4.3353\n",
      "Epoch 0, Step: 73600 Learing: 0.0000065600 Loss: 1.8488 Grad Norm: 4.1909\n",
      "Epoch 0, Step: 73700 Learing: 0.0000079516 Loss: 1.9597 Grad Norm: 4.2343\n",
      "Epoch 0, Step: 73800 Learing: 0.0000090543 Loss: 2.0401 Grad Norm: 4.4789\n",
      "Epoch 0, Step: 73900 Learing: 0.0000097601 Loss: 1.8495 Grad Norm: 4.3369\n",
      "Epoch 0, Step: 74000 Learing: 0.0000100000 Loss: 1.9050 Grad Norm: 4.6977\n",
      "中国首都是哪?天\n",
      "Epoch 0, Step: 74100 Learing: 0.0000097504 Loss: 1.9368 Grad Norm: 4.5736\n",
      "Epoch 0, Step: 74200 Learing: 0.0000090358 Loss: 1.8969 Grad Norm: 4.4729\n",
      "Epoch 0, Step: 74300 Learing: 0.0000079262 Loss: 1.9419 Grad Norm: 4.3181\n",
      "Epoch 0, Step: 74400 Learing: 0.0000065301 Loss: 2.1261 Grad Norm: 4.1735\n",
      "Epoch 0, Step: 74500 Learing: 0.0000049843 Loss: 2.0972 Grad Norm: 4.4836\n",
      "Epoch 0, Step: 74600 Learing: 0.0000034400 Loss: 2.1337 Grad Norm: 4.0205\n",
      "Epoch 0, Step: 74700 Learing: 0.0000020484 Loss: 1.8948 Grad Norm: 3.9943\n",
      "Epoch 0, Step: 74800 Learing: 0.0000009457 Loss: 1.8988 Grad Norm: 3.9962\n",
      "Epoch 0, Step: 74900 Learing: 0.0000002399 Loss: 1.8995 Grad Norm: 3.9685\n",
      "Epoch 0, Step: 75000 Learing: 0.0000000000 Loss: 1.8765 Grad Norm: 3.9654\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 75100 Learing: 0.0000002496 Loss: 1.8733 Grad Norm: 4.1110\n",
      "Epoch 0, Step: 75200 Learing: 0.0000009642 Loss: 1.8099 Grad Norm: 4.0301\n",
      "Epoch 0, Step: 75300 Learing: 0.0000020738 Loss: 1.5411 Grad Norm: 4.0336\n",
      "Epoch 0, Step: 75400 Learing: 0.0000034699 Loss: 1.8951 Grad Norm: 4.0611\n",
      "Epoch 0, Step: 75500 Learing: 0.0000050157 Loss: 2.0538 Grad Norm: 4.3044\n",
      "Epoch 0, Step: 75600 Learing: 0.0000065600 Loss: 1.8035 Grad Norm: 4.2406\n",
      "Epoch 0, Step: 75700 Learing: 0.0000079516 Loss: 1.8989 Grad Norm: 4.4808\n",
      "Epoch 0, Step: 75800 Learing: 0.0000090543 Loss: 2.0334 Grad Norm: 4.1735\n",
      "Epoch 0, Step: 75900 Learing: 0.0000097601 Loss: 2.3448 Grad Norm: 4.8643\n",
      "Epoch 0, Step: 76000 Learing: 0.0000100000 Loss: 2.1449 Grad Norm: 4.6658\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 76100 Learing: 0.0000097504 Loss: 1.9057 Grad Norm: 4.3462\n",
      "Epoch 0, Step: 76200 Learing: 0.0000090358 Loss: 1.9888 Grad Norm: 4.2584\n",
      "Epoch 0, Step: 76300 Learing: 0.0000079262 Loss: 1.8726 Grad Norm: 4.3478\n",
      "Epoch 0, Step: 76400 Learing: 0.0000065301 Loss: 1.9949 Grad Norm: 4.0823\n",
      "Epoch 0, Step: 76500 Learing: 0.0000049843 Loss: 2.2400 Grad Norm: 3.9557\n",
      "Epoch 0, Step: 76600 Learing: 0.0000034400 Loss: 1.8497 Grad Norm: 4.1699\n",
      "Epoch 0, Step: 76700 Learing: 0.0000020484 Loss: 1.9664 Grad Norm: 4.0925\n",
      "Epoch 0, Step: 76800 Learing: 0.0000009457 Loss: 1.8610 Grad Norm: 3.9493\n",
      "Epoch 0, Step: 76900 Learing: 0.0000002399 Loss: 1.9418 Grad Norm: 3.7725\n",
      "Epoch 0, Step: 77000 Learing: 0.0000000000 Loss: 1.8716 Grad Norm: 3.9134\n",
      "中国首都是哪?中\n",
      "Epoch 0, Step: 77100 Learing: 0.0000002496 Loss: 2.0098 Grad Norm: 4.2147\n",
      "Epoch 0, Step: 77200 Learing: 0.0000009642 Loss: 1.4166 Grad Norm: 3.8734\n",
      "Epoch 0, Step: 77300 Learing: 0.0000020738 Loss: 1.7952 Grad Norm: 3.9037\n",
      "Epoch 0, Step: 77400 Learing: 0.0000034699 Loss: 1.9374 Grad Norm: 4.2169\n",
      "Epoch 0, Step: 77500 Learing: 0.0000050157 Loss: 2.0442 Grad Norm: 4.2519\n",
      "Epoch 0, Step: 77600 Learing: 0.0000065600 Loss: 1.9370 Grad Norm: 4.5540\n",
      "Epoch 0, Step: 77700 Learing: 0.0000079516 Loss: 1.9039 Grad Norm: 4.6677\n",
      "Epoch 0, Step: 77800 Learing: 0.0000090543 Loss: 1.7329 Grad Norm: 4.3238\n",
      "Epoch 0, Step: 77900 Learing: 0.0000097601 Loss: 1.9107 Grad Norm: 4.8830\n",
      "Epoch 0, Step: 78000 Learing: 0.0000100000 Loss: 1.8055 Grad Norm: 4.0712\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 78100 Learing: 0.0000097504 Loss: 1.7909 Grad Norm: 4.3033\n",
      "Epoch 0, Step: 78200 Learing: 0.0000090358 Loss: 1.9545 Grad Norm: 4.0421\n",
      "Epoch 0, Step: 78300 Learing: 0.0000079262 Loss: 2.1007 Grad Norm: 4.1435\n",
      "Epoch 0, Step: 78400 Learing: 0.0000065301 Loss: 2.0391 Grad Norm: 4.1849\n",
      "Epoch 0, Step: 78500 Learing: 0.0000049843 Loss: 1.8972 Grad Norm: 4.1501\n",
      "Epoch 0, Step: 78600 Learing: 0.0000034400 Loss: 1.8674 Grad Norm: 4.5553\n",
      "Epoch 0, Step: 78700 Learing: 0.0000020484 Loss: 1.9548 Grad Norm: 4.0676\n",
      "Epoch 0, Step: 78800 Learing: 0.0000009457 Loss: 1.7901 Grad Norm: 4.1982\n",
      "Epoch 0, Step: 78900 Learing: 0.0000002399 Loss: 2.1379 Grad Norm: 4.0448\n",
      "Epoch 0, Step: 79000 Learing: 0.0000000000 Loss: 1.7972 Grad Norm: 4.2697\n",
      "中国首都是哪?一\n",
      "Epoch 0, Step: 79100 Learing: 0.0000002496 Loss: 1.8366 Grad Norm: 4.3229\n",
      "Epoch 0, Step: 79200 Learing: 0.0000009642 Loss: 2.0147 Grad Norm: 4.0397\n",
      "Epoch 0, Step: 79300 Learing: 0.0000020738 Loss: 1.9724 Grad Norm: 3.9184\n",
      "Epoch 0, Step: 79400 Learing: 0.0000034699 Loss: 2.0303 Grad Norm: 4.3962\n",
      "Epoch 0, Step: 79500 Learing: 0.0000050157 Loss: 1.8608 Grad Norm: 4.2749\n",
      "Epoch 0, Step: 79600 Learing: 0.0000065600 Loss: 1.8703 Grad Norm: 4.4373\n",
      "Epoch 0, Step: 79700 Learing: 0.0000079516 Loss: 1.9769 Grad Norm: 4.4008\n",
      "Epoch 0, Step: 79800 Learing: 0.0000090543 Loss: 1.8823 Grad Norm: 4.3780\n",
      "Epoch 0, Step: 79900 Learing: 0.0000097601 Loss: 1.9587 Grad Norm: 4.5056\n",
      "Epoch 0, Step: 80000 Learing: 0.0000100000 Loss: 2.0099 Grad Norm: 4.2946\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 80100 Learing: 0.0000097504 Loss: 1.8470 Grad Norm: 4.3286\n",
      "Epoch 0, Step: 80200 Learing: 0.0000090358 Loss: 2.0399 Grad Norm: 4.6481\n",
      "Epoch 0, Step: 80300 Learing: 0.0000079262 Loss: 1.6465 Grad Norm: 4.4256\n",
      "Epoch 0, Step: 80400 Learing: 0.0000065301 Loss: 1.9610 Grad Norm: 4.3876\n",
      "Epoch 0, Step: 80500 Learing: 0.0000049843 Loss: 1.8673 Grad Norm: 3.8784\n",
      "Epoch 0, Step: 80600 Learing: 0.0000034400 Loss: 1.9116 Grad Norm: 4.0174\n",
      "Epoch 0, Step: 80700 Learing: 0.0000020484 Loss: 1.9130 Grad Norm: 4.0822\n",
      "Epoch 0, Step: 80800 Learing: 0.0000009457 Loss: 2.0127 Grad Norm: 4.0663\n",
      "Epoch 0, Step: 80900 Learing: 0.0000002399 Loss: 2.0257 Grad Norm: 3.7002\n",
      "Epoch 0, Step: 81000 Learing: 0.0000000000 Loss: 1.9462 Grad Norm: 3.8918\n",
      "中国首都是哪?不\n",
      "Epoch 0, Step: 81100 Learing: 0.0000002496 Loss: 1.9384 Grad Norm: 4.0316\n",
      "Epoch 0, Step: 81200 Learing: 0.0000009642 Loss: 2.0889 Grad Norm: 4.0131\n",
      "Epoch 0, Step: 81300 Learing: 0.0000020738 Loss: 2.0688 Grad Norm: 4.1545\n",
      "Epoch 0, Step: 81400 Learing: 0.0000034699 Loss: 2.0562 Grad Norm: 4.6830\n",
      "Epoch 0, Step: 81500 Learing: 0.0000050157 Loss: 2.0978 Grad Norm: 3.9421\n",
      "Epoch 0, Step: 81600 Learing: 0.0000065600 Loss: 1.9269 Grad Norm: 4.1026\n",
      "Epoch 0, Step: 81700 Learing: 0.0000079516 Loss: 1.9500 Grad Norm: 3.8738\n",
      "Epoch 0, Step: 81800 Learing: 0.0000090543 Loss: 2.1047 Grad Norm: 4.4701\n",
      "Epoch 0, Step: 81900 Learing: 0.0000097601 Loss: 1.9519 Grad Norm: 4.1336\n",
      "Epoch 0, Step: 82000 Learing: 0.0000100000 Loss: 2.0741 Grad Norm: 4.4485\n",
      "中国首都是哪?中\n",
      "Epoch 0, Step: 82100 Learing: 0.0000097504 Loss: 1.4228 Grad Norm: 3.9027\n",
      "Epoch 0, Step: 82200 Learing: 0.0000090358 Loss: 2.0521 Grad Norm: 4.6018\n",
      "Epoch 0, Step: 82300 Learing: 0.0000079262 Loss: 1.8668 Grad Norm: 4.4190\n",
      "Epoch 0, Step: 82400 Learing: 0.0000065301 Loss: 2.0142 Grad Norm: 3.9580\n",
      "Epoch 0, Step: 82500 Learing: 0.0000049843 Loss: 1.9003 Grad Norm: 3.9726\n",
      "Epoch 0, Step: 82600 Learing: 0.0000034400 Loss: 2.0743 Grad Norm: 4.0648\n",
      "Epoch 0, Step: 82700 Learing: 0.0000020484 Loss: 1.7583 Grad Norm: 3.8820\n",
      "Epoch 0, Step: 82800 Learing: 0.0000009457 Loss: 1.9216 Grad Norm: 4.0144\n",
      "Epoch 0, Step: 82900 Learing: 0.0000002399 Loss: 2.0934 Grad Norm: 4.0314\n",
      "Epoch 0, Step: 83000 Learing: 0.0000000000 Loss: 1.7764 Grad Norm: 5.2117\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 83100 Learing: 0.0000002496 Loss: 1.7988 Grad Norm: 3.7124\n",
      "Epoch 0, Step: 83200 Learing: 0.0000009642 Loss: 1.8958 Grad Norm: 4.4483\n",
      "Epoch 0, Step: 83300 Learing: 0.0000020738 Loss: 1.7927 Grad Norm: 3.7719\n",
      "Epoch 0, Step: 83400 Learing: 0.0000034699 Loss: 1.8618 Grad Norm: 4.0876\n",
      "Epoch 0, Step: 83500 Learing: 0.0000050157 Loss: 1.9149 Grad Norm: 4.4013\n",
      "Epoch 0, Step: 83600 Learing: 0.0000065600 Loss: 2.1513 Grad Norm: 4.4483\n",
      "Epoch 0, Step: 83700 Learing: 0.0000079516 Loss: 1.9713 Grad Norm: 4.2235\n",
      "Epoch 0, Step: 83800 Learing: 0.0000090543 Loss: 1.8012 Grad Norm: 4.4801\n",
      "Epoch 0, Step: 83900 Learing: 0.0000097601 Loss: 2.1856 Grad Norm: 4.7777\n",
      "Epoch 0, Step: 84000 Learing: 0.0000100000 Loss: 2.0678 Grad Norm: 4.5203\n",
      "中国首都是哪?�\n",
      "Epoch 0, Step: 84100 Learing: 0.0000097504 Loss: 1.9938 Grad Norm: 4.6640\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def set_seed(seed:int):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "set_seed(42)\n",
    "for epoch in range(1):\n",
    "    train_loss = train(model, optimizer, scheduler, train_loader)\n",
    "    val_loss = 0.0\n",
    "    print(f'Epoch={epoch} Train Loss={train_loss/len(train_loader):.4f} Val Loss={val_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda08473-08a8-4a5d-9801-1a0884b23a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "dt = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "# model.save_pretrained(f'outputs/nanogpt-{dt}', safe_serialization=False)\n",
    "torch.save(model.state_dict(), f'ouputs/nanogpt/model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cad7e0b-01d0-486d-b966-a6e443538e92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
