{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bertviz\n",
      "  Downloading bertviz-1.4.0-py3-none-any.whl (157 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.6/157.6 kB\u001b[0m \u001b[31m554.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.0 in /Users/tangyun/opt/miniconda3/envs/rl/lib/python3.10/site-packages (from bertviz) (2.0.1)\n",
      "Requirement already satisfied: transformers>=2.0 in /Users/tangyun/opt/miniconda3/envs/rl/lib/python3.10/site-packages (from bertviz) (4.30.2)\n",
      "Requirement already satisfied: tqdm in /Users/tangyun/opt/miniconda3/envs/rl/lib/python3.10/site-packages (from bertviz) (4.65.0)\n",
      "Requirement already satisfied: requests in /Users/tangyun/opt/miniconda3/envs/rl/lib/python3.10/site-packages (from bertviz) (2.31.0)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.26.155-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m661.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex in /Users/tangyun/opt/miniconda3/envs/rl/lib/python3.10/site-packages (from bertviz) (2023.6.3)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-macosx_10_9_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /Users/tangyun/opt/miniconda3/envs/rl/lib/python3.10/site-packages (from torch>=1.0->bertviz) (4.6.3)\n",
      "Requirement already satisfied: sympy in /Users/tangyun/opt/miniconda3/envs/rl/lib/python3.10/site-packages (from torch>=1.0->bertviz) (1.12)\n",
      "Requirement already satisfied: jinja2 in /Users/tangyun/opt/miniconda3/envs/rl/lib/python3.10/site-packages (from torch>=1.0->bertviz) (3.1.2)\n",
      "Requirement already satisfied: networkx in /Users/tangyun/opt/miniconda3/envs/rl/lib/python3.10/site-packages (from torch>=1.0->bertviz) (3.1)\n",
      "Requirement already satisfied: filelock in /Users/tangyun/opt/miniconda3/envs/rl/lib/python3.10/site-packages (from torch>=1.0->bertviz) (3.12.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/tangyun/opt/miniconda3/envs/rl/lib/python3.10/site-packages (from transformers>=2.0->bertviz) (0.3.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/tangyun/opt/miniconda3/envs/rl/lib/python3.10/site-packages (from transformers>=2.0->bertviz) (1.24.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/tangyun/opt/miniconda3/envs/rl/lib/python3.10/site-packages (from transformers>=2.0->bertviz) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /Users/tangyun/opt/miniconda3/envs/rl/lib/python3.10/site-packages (from transformers>=2.0->bertviz) (0.15.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/tangyun/opt/miniconda3/envs/rl/lib/python3.10/site-packages (from transformers>=2.0->bertviz) (0.13.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/tangyun/opt/miniconda3/envs/rl/lib/python3.10/site-packages (from transformers>=2.0->bertviz) (23.1)\n",
      "Collecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0\n",
      "  Downloading s3transfer-0.6.1-py3-none-any.whl (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m298.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting botocore<1.30.0,>=1.29.155\n",
      "  Downloading botocore-1.29.155-py3-none-any.whl (10.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /Users/tangyun/opt/miniconda3/envs/rl/lib/python3.10/site-packages (from requests->bertviz) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tangyun/opt/miniconda3/envs/rl/lib/python3.10/site-packages (from requests->bertviz) (2.0.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/tangyun/opt/miniconda3/envs/rl/lib/python3.10/site-packages (from requests->bertviz) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tangyun/opt/miniconda3/envs/rl/lib/python3.10/site-packages (from requests->bertviz) (3.1.0)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/tangyun/opt/miniconda3/envs/rl/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.155->boto3->bertviz) (2.8.2)\n",
      "Requirement already satisfied: fsspec in /Users/tangyun/opt/miniconda3/envs/rl/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers>=2.0->bertviz) (2023.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/tangyun/opt/miniconda3/envs/rl/lib/python3.10/site-packages (from jinja2->torch>=1.0->bertviz) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/tangyun/opt/miniconda3/envs/rl/lib/python3.10/site-packages (from sympy->torch>=1.0->bertviz) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/tangyun/opt/miniconda3/envs/rl/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.155->boto3->bertviz) (1.16.0)\n",
      "Installing collected packages: sentencepiece, urllib3, jmespath, botocore, s3transfer, boto3, bertviz\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.2\n",
      "    Uninstalling urllib3-2.0.2:\n",
      "      Successfully uninstalled urllib3-2.0.2\n",
      "Successfully installed bertviz-1.4.0 boto3-1.26.155 botocore-1.29.155 jmespath-1.0.1 s3transfer-0.6.1 sentencepiece-0.1.99 urllib3-1.26.16\n"
     ]
    }
   ],
   "source": [
    "!pip install bertviz"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{attention} (\\text{Q},\\text{K},\\text{V}) = \\mathcal{sofxmax} \\left(\\frac{\\text{Q} \\text{K}^{\\text{T}}}{\\sqrt \\mathcal{d}_k} \\right) \\text V\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "from bertviz.transformers_neuron_view import BertModel,BertConfig\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. model config and load"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 440473133/440473133 [02:28<00:00, 2973348.97B/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 256\n",
    "model_name = 'bert-base-uncased'\n",
    "config = BertConfig.from_pretrained(model_name,output_attentions=True,output_hidden_states=True,return_dict=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "config.max_position_embeddings =  max_len\n",
    "model = BertModel(config).from_pretrained(model_name)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"finetuning_task\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"num_labels\": 2,\n",
       "  \"output_attentions\": true,\n",
       "  \"output_hidden_states\": false,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"torchscript\": false,\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_head_size = int(model.config.hidden_size/model.config.num_attention_heads)\n",
    "att_head_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![11](https://heidloff.net/assets/img/2023/02/transformers.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 64])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.layer[0].attention.self.query.weight.T[:,:64].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_trains = fetch_20newsgroups(subset='train')\n",
    "input_tests = tokenizer(newsgroups_trains['data'][:1],truncation=True,max_length=max_len,return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 201])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tests.keys()\n",
    "input_tests['input_ids'].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = model(**input_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0053, 0.0109, 0.0052,  ..., 0.0039, 0.0036, 0.0144],\n",
       "        [0.0086, 0.0041, 0.0125,  ..., 0.0045, 0.0041, 0.0071],\n",
       "        [0.0051, 0.0043, 0.0046,  ..., 0.0043, 0.0045, 0.0031],\n",
       "        ...,\n",
       "        [0.0010, 0.0023, 0.0055,  ..., 0.0012, 0.0018, 0.0011],\n",
       "        [0.0010, 0.0023, 0.0057,  ..., 0.0012, 0.0017, 0.0007],\n",
       "        [0.0022, 0.0056, 0.0063,  ..., 0.0045, 0.0048, 0.0015]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_output)\n",
    "len(model_output[-1])\n",
    "model_output[-1][0]['attn'][0,0,:,:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
       "          [-0.1172,  0.6055,  0.0487,  ...,  0.5867,  0.8167,  0.4067],\n",
       "          [-0.7412,  0.3854, -0.7550,  ...,  0.5425,  0.5629,  0.6106],\n",
       "          ...,\n",
       "          [ 0.0679,  0.2560,  0.3443,  ...,  0.5042,  0.4860,  0.3145],\n",
       "          [ 0.1079,  0.0740,  0.4233,  ...,  0.2864,  0.5379,  0.1220],\n",
       "          [-0.0594, -0.0563,  0.2673,  ..., -0.7952, -0.0813, -0.6690]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " torch.Size([1, 201, 768]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_output = model.embeddings(input_tests['input_ids'],input_tests['token_type_ids'])\n",
    "\n",
    "emb_output,emb_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertLayer(\n",
       "  (attention): BertAttention(\n",
       "    (self): BertSelfAttention(\n",
       "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (output): BertSelfOutput(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (intermediate): BertIntermediate(\n",
       "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "  )\n",
       "  (output): BertOutput(\n",
       "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.layer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5849, -0.3312, -0.4301,  0.3745, -0.2981,  0.4103,  0.0136,  0.2938,\n",
       "         0.2338, -0.1294,  0.1367,  0.4521, -0.1008,  0.1104,  0.4317,  0.5654,\n",
       "         0.0308, -0.0466, -0.3148, -0.1194,  0.0061,  0.0062,  0.0023,  0.4704,\n",
       "        -0.0229, -0.0624, -0.0711,  0.5856, -0.4203, -0.0035,  0.3211, -0.0183,\n",
       "        -0.1371, -0.2399,  0.0593, -0.0781, -0.0723, -0.1255,  0.5402, -0.0784,\n",
       "        -0.2501, -0.4059, -0.3739, -0.1389, -0.7171, -0.4686, -0.1296, -0.3605,\n",
       "         0.0325,  0.2098,  0.4282, -0.0019,  0.6725, -0.1765,  0.2999, -0.2933,\n",
       "         0.4123,  0.0808, -0.1765, -0.2740,  0.6475,  0.0608, -0.3303,  0.1725],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.layer[0].attention.self.query.weight.shape,model.encoder.layer[0].attention.self.query.bias.shape\n",
    "\n",
    "model.encoder.layer[0].attention.self.query.bias[:att_head_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb_output[0].shape = (201,768)\n",
    "# model.encoder.layer[0].attention.self.query.weight.T (768,64)\n",
    "# 201*64 + 64\n",
    "Q_fst_head_fst_layer = emb_output[0] @ model.encoder.layer[0].attention.self.query.weight.T[:, :att_head_size] \\\n",
    "    + model.encoder.layer[0].attention.self.query.bias[:att_head_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 201*64\n",
    "K_fst_head_fst_layer = emb_output[0] @ model.encoder.layer[0].attention.self.key.weight.T[:, :att_head_size] \\\n",
    "    + model.encoder.layer[0].attention.self.key.bias[:att_head_size]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{attention} (\\text{Q},\\text{K},\\text{V}) = \\mathcal{sofxmax} \\left(\\frac{\\text{Q} \\text{K}^{\\text{T}}}{\\sqrt \\mathcal{d}_k} \\right) \\text V\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0053, 0.0109, 0.0052,  ..., 0.0039, 0.0036, 0.0144],\n",
       "         [0.0086, 0.0041, 0.0125,  ..., 0.0045, 0.0041, 0.0071],\n",
       "         [0.0051, 0.0043, 0.0046,  ..., 0.0043, 0.0045, 0.0031],\n",
       "         ...,\n",
       "         [0.0010, 0.0023, 0.0055,  ..., 0.0012, 0.0018, 0.0011],\n",
       "         [0.0010, 0.0023, 0.0057,  ..., 0.0012, 0.0017, 0.0007],\n",
       "         [0.0022, 0.0056, 0.0063,  ..., 0.0045, 0.0048, 0.0015]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " torch.Size([201, 201]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# 201*201\n",
    "attn_scores = F.softmax(Q_fst_head_fst_layer @\n",
    "                        K_fst_head_fst_layer.T / math.sqrt(att_head_size), dim=1)\n",
    "attn_scores,attn_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0053, 0.0109, 0.0052,  ..., 0.0039, 0.0036, 0.0144],\n",
       "         [0.0086, 0.0041, 0.0125,  ..., 0.0045, 0.0041, 0.0071],\n",
       "         [0.0051, 0.0043, 0.0046,  ..., 0.0043, 0.0045, 0.0031],\n",
       "         ...,\n",
       "         [0.0010, 0.0023, 0.0055,  ..., 0.0012, 0.0018, 0.0011],\n",
       "         [0.0010, 0.0023, 0.0057,  ..., 0.0012, 0.0017, 0.0007],\n",
       "         [0.0022, 0.0056, 0.0063,  ..., 0.0045, 0.0048, 0.0015]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " torch.Size([201, 201]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output[-1][0]['attn'][0,0,:,:],model_output[-1][0]['attn'][0,0,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([201, 64])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_fst_head_fst_layer = emb_output[0] @ model.encoder.layer[0].attention.self.value.weight.T[:, :att_head_size] \\\n",
    "    + model.encoder.layer[0].attention.self.value.bias[:att_head_size]\n",
    "V_fst_head_fst_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([201, 64])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_emb = attn_scores @ V_fst_head_fst_layer\n",
    "attn_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
