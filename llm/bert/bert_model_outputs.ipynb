{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tangyun/opt/miniconda3/envs/rl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name, output_hidden_states=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  101,  2044, 11065,  2769,  2013,  1996,  2924, 11632,  1010,  1996,\n",
       "           2924, 27307,  2001,  2464,  5645,  2006,  1996,  5900,  2314,  2924,\n",
       "           1012,   102]]),\n",
       " torch.Size([1, 22]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"After stealing money from the bank vault, the bank robber was seen \" \\\n",
    "    \"fishing on the Mississippi river bank.\"\n",
    "\n",
    "token_input = tokenizer(text, return_tensors='pt')\n",
    "token_input['input_ids'], token_input['input_ids'].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. model forward\n",
    "- forward\n",
    "    - embedding => encoder => pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased',output_hidden_states=True)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**token_input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. output\n",
    "- len(outputs) == 3\n",
    "- outputs[0]\n",
    "    - last_hidden_size, shape: `batch_size * seq_len * hidden_size(1 * 22 * 768)`\n",
    "- outputs[1]\n",
    "    - pooler_output, shape: `batch_size * hidden_size(1*768)`\n",
    "    - last layer hidden-state of the first token of sequence (classification token,[CLS])\n",
    "- outputs[2](model.config.output_hidden_states=True)\n",
    "    - type: tuple\n",
    "    - one for the output of the embeddings(1), if the model has an embedding layer(12), one for the output of each layer\n",
    "    - `(1+12) * (batch_size*seq_len*hidden_size) = 13*1*22*768`\n",
    "\n",
    "- outputs[0] == outputs[2][-1]\n",
    "\n",
    "- outputs[1] == model.pooler(outputs[2][-1])\n",
    "\n",
    "- outputs[2][0] == model.embeddings(token_input['input_ids'], token_input['token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 22, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6031, -0.3342, -0.7174,  0.3347,  0.5145, -0.1722,  0.4502,  0.2768,\n",
       "         -0.3769, -0.9998, -0.3657,  0.7535,  0.9817, -0.0192,  0.7959, -0.3459,\n",
       "         -0.1338, -0.3026,  0.1097,  0.5836,  0.5736,  0.9999,  0.1798,  0.1845,\n",
       "          0.2250,  0.9109, -0.5653,  0.8616,  0.8994,  0.7423, -0.2525,  0.0394,\n",
       "         -0.9894, -0.1331, -0.7763, -0.9826,  0.2223, -0.6115,  0.1941,  0.0177,\n",
       "         -0.7634,  0.2312,  0.9999, -0.7000,  0.4623, -0.2202, -1.0000,  0.1908,\n",
       "         -0.8150,  0.6483,  0.5878,  0.8198,  0.1014,  0.3185,  0.3963, -0.3216,\n",
       "         -0.1701,  0.0588, -0.1544, -0.4987, -0.5284,  0.1228, -0.4823, -0.7788,\n",
       "          0.6954,  0.0891, -0.0855, -0.1500,  0.0390, -0.0760,  0.6154,  0.2662,\n",
       "         -0.0129, -0.7253,  0.1352,  0.2921, -0.5613,  1.0000,  0.1536, -0.9681,\n",
       "          0.7166,  0.2600,  0.4519,  0.5470, -0.2798, -1.0000,  0.3419, -0.2645,\n",
       "         -0.9863,  0.1263,  0.5249, -0.2000,  0.5980,  0.4752, -0.2355, -0.4808,\n",
       "         -0.3786, -0.7284, -0.0909,  0.0124, -0.0689, -0.2531, -0.1324, -0.2361,\n",
       "          0.1732, -0.3216, -0.0188,  0.2302, -0.3221,  0.4996,  0.4346, -0.1935,\n",
       "          0.2968, -0.9292,  0.5326, -0.3695, -0.9876, -0.4770, -0.9902,  0.6349,\n",
       "         -0.1863, -0.2612,  0.9123, -0.1930,  0.3110,  0.0803, -0.7598, -1.0000,\n",
       "          0.0292, -0.0628, -0.1086, -0.2135, -0.9671, -0.9521,  0.3334,  0.8675,\n",
       "          0.2254,  0.9995, -0.2908,  0.9420,  0.0336, -0.4542,  0.4123, -0.4455,\n",
       "          0.4558, -0.4442, -0.0685,  0.3043,  0.0575,  0.1843, -0.6577, -0.3121,\n",
       "         -0.1069, -0.7729, -0.2328,  0.9032, -0.4681, -0.5580,  0.3359, -0.1644,\n",
       "         -0.1572,  0.6441,  0.2810,  0.2650,  0.1532,  0.4485, -0.5299,  0.2616,\n",
       "         -0.7412, -0.0347,  0.2560, -0.2659, -0.6092, -0.9868, -0.2346,  0.4682,\n",
       "          0.9771,  0.5646,  0.2360,  0.4567, -0.2170,  0.1420, -0.9554,  0.9832,\n",
       "         -0.0935,  0.2620, -0.7901,  0.5758, -0.7642, -0.5362,  0.6374, -0.3891,\n",
       "         -0.6368, -0.0045, -0.2658, -0.1721, -0.7466,  0.4832, -0.3128, -0.2640,\n",
       "          0.0435,  0.8879,  0.5961,  0.2972,  0.0742,  0.3041, -0.7367, -0.2117,\n",
       "         -0.0500,  0.0825,  0.0273,  0.9870, -0.5612, -0.0353, -0.8011, -0.9833,\n",
       "         -0.1874, -0.7143,  0.0476, -0.4776,  0.4406, -0.7087, -0.3319,  0.1035,\n",
       "         -0.2959, -0.6915,  0.2840, -0.5485,  0.3292, -0.3018,  0.8687,  0.7867,\n",
       "         -0.5158, -0.2411,  0.9175, -0.7601, -0.6967, -0.0763, -0.1411,  0.6739,\n",
       "         -0.6006,  0.9645,  0.6723,  0.3451, -0.9112, -0.6897, -0.3042, -0.0059,\n",
       "         -0.0918, -0.6402,  0.5602,  0.3850,  0.2737,  0.7868, -0.4155,  0.8343,\n",
       "         -0.9288, -0.9377, -0.9803,  0.1936, -0.9873,  0.7978,  0.1702,  0.6547,\n",
       "         -0.3664, -0.3837, -0.9602,  0.2690,  0.0832,  0.8077, -0.5984, -0.5181,\n",
       "         -0.4783, -0.9236, -0.0773, -0.0542,  0.1906,  0.0659, -0.8882,  0.3529,\n",
       "          0.5142,  0.4257, -0.8504,  0.9686,  1.0000,  0.9745,  0.7975,  0.3701,\n",
       "         -0.9993, -0.9159,  0.9999, -0.9649, -1.0000, -0.8484, -0.5494,  0.2253,\n",
       "         -1.0000, -0.0892,  0.0449, -0.8989,  0.2432,  0.9673,  0.7904, -1.0000,\n",
       "          0.8539,  0.8224, -0.5027,  0.8026, -0.2741,  0.9723,  0.3954,  0.5613,\n",
       "         -0.2778,  0.4943, -0.7842, -0.5614, -0.4786, -0.7206,  0.9944, -0.0361,\n",
       "         -0.2827, -0.8623,  0.6206, -0.0204, -0.2777, -0.9243, -0.2374,  0.5576,\n",
       "          0.4833,  0.1621,  0.2181, -0.4744,  0.1432, -0.0688, -0.3646,  0.5594,\n",
       "         -0.8221, -0.1388,  0.5845, -0.0662,  0.0617, -0.9573,  0.9249, -0.4059,\n",
       "          0.5819,  1.0000,  0.8847, -0.6462,  0.4425,  0.1442,  0.2397,  1.0000,\n",
       "          0.3757, -0.9790, -0.5021,  0.5227, -0.4537, -0.4564,  0.9975, -0.1468,\n",
       "         -0.4530, -0.2211,  0.9849, -0.9892,  0.9874, -0.6354, -0.9533,  0.9617,\n",
       "          0.9100, -0.3272, -0.5809,  0.1221,  0.2137,  0.1775, -0.6459,  0.5595,\n",
       "          0.3555,  0.0030,  0.7183,  0.1490, -0.4345,  0.2424, -0.5088,  0.0014,\n",
       "          0.9025,  0.3578, -0.0445, -0.0357, -0.2723, -0.8130, -0.9344,  0.4227,\n",
       "          1.0000, -0.1721,  0.8243,  0.0665,  0.0115, -0.1497,  0.4186,  0.3168,\n",
       "         -0.3061, -0.5515,  0.7428, -0.7339, -0.9949,  0.1418,  0.0288,  0.1303,\n",
       "          0.9994,  0.6172,  0.1452,  0.4381,  0.9631, -0.0898, -0.1024,  0.3720,\n",
       "          0.9655, -0.2082,  0.4160,  0.2856, -0.4470, -0.0806, -0.5101, -0.1494,\n",
       "         -0.8861,  0.3496, -0.9617,  0.9112,  0.9032,  0.4198,  0.1110,  0.5989,\n",
       "          1.0000, -0.9781,  0.0478,  0.8043, -0.0370, -0.9994, -0.4571, -0.3764,\n",
       "         -0.0341, -0.3013,  0.0021,  0.1011, -0.9597,  0.1910,  0.7922, -0.5135,\n",
       "         -0.9862,  0.0285,  0.4023,  0.2365, -0.9728, -0.3717, -0.4726,  0.2997,\n",
       "         -0.0355, -0.9264,  0.3197, -0.3812,  0.3245, -0.1921,  0.4575,  0.3563,\n",
       "          0.9515, -0.8870, -0.3251, -0.1148, -0.6957,  0.4810, -0.2975, -0.7607,\n",
       "         -0.1648,  1.0000, -0.4754,  0.5410,  0.3828,  0.1635, -0.2463,  0.1942,\n",
       "          0.8299,  0.1894,  0.0876, -0.6173,  0.7333, -0.2475,  0.3843,  0.7312,\n",
       "         -0.0760,  0.6759,  0.7408,  0.1733,  0.0710,  0.0930,  0.9232, -0.0188,\n",
       "         -0.2704, -0.3332, -0.0659, -0.3261,  0.7431,  1.0000,  0.1622,  0.5590,\n",
       "         -0.9939, -0.7758, -0.7434,  1.0000,  0.8649, -0.6865,  0.5643,  0.4938,\n",
       "         -0.2507, -0.0745, -0.1468, -0.2566,  0.2531,  0.0162,  0.9603, -0.5751,\n",
       "         -0.9828, -0.4233,  0.3365, -0.9327,  0.9997, -0.5279, -0.2036, -0.2868,\n",
       "         -0.4147, -0.9348, -0.0477, -0.9812, -0.1836,  0.1210,  0.9609,  0.2963,\n",
       "         -0.4743, -0.7886,  0.8539,  0.4805, -0.8080, -0.9297,  0.9734, -0.8989,\n",
       "          0.3318,  1.0000,  0.4983,  0.0046,  0.1519, -0.2174,  0.3163, -0.3975,\n",
       "          0.5479, -0.9144, -0.0836, -0.1866,  0.3943, -0.1005, -0.9010,  0.5822,\n",
       "          0.0541, -0.4141, -0.4825, -0.0623,  0.3400,  0.6302, -0.1646, -0.0293,\n",
       "          0.1773,  0.0035, -0.7814, -0.2836, -0.3813, -0.9999,  0.5009, -1.0000,\n",
       "          0.6917, -0.3492, -0.2238,  0.7105,  0.8107,  0.7481, -0.4549, -0.4793,\n",
       "          0.7201,  0.6103, -0.1528, -0.2139, -0.4457,  0.2485,  0.0196,  0.1607,\n",
       "         -0.3812,  0.5145, -0.2807,  1.0000,  0.0829, -0.3200, -0.5929,  0.2053,\n",
       "         -0.2679,  1.0000, -0.2241, -0.9651,  0.1681, -0.5537, -0.5055,  0.4764,\n",
       "         -0.0439, -0.7710, -0.8442,  0.7623,  0.3223, -0.5828,  0.5065, -0.2420,\n",
       "         -0.2434, -0.0287,  0.8268,  0.9844,  0.8406,  0.3518, -0.9611, -0.3503,\n",
       "          0.9309,  0.1013, -0.1565, -0.0012,  1.0000,  0.4139, -0.7002,  0.0942,\n",
       "         -0.8437, -0.2698, -0.7333,  0.2633,  0.0600,  0.9015, -0.2510,  0.9170,\n",
       "         -0.7876, -0.0735, -0.5044,  0.2206,  0.2786, -0.8753, -0.9837, -0.9885,\n",
       "          0.5545, -0.2806, -0.0823,  0.3698,  0.1557,  0.2687,  0.4354, -1.0000,\n",
       "          0.9126,  0.3169,  0.7036,  0.9720,  0.4816,  0.6401,  0.3309, -0.9813,\n",
       "         -0.6150, -0.3007, -0.1601,  0.3913,  0.3273,  0.6264,  0.2662, -0.3612,\n",
       "         -0.6613, -0.3084, -0.9703, -0.9852,  0.3886,  0.0251, -0.3822,  0.9373,\n",
       "         -0.0961,  0.0824,  0.4406, -0.5929,  0.1701,  0.6097,  0.1687, -0.0525,\n",
       "          0.5533,  0.8141,  0.7725,  0.9789, -0.7193,  0.2995, -0.5652,  0.3757,\n",
       "          0.9063, -0.9190,  0.0740,  0.3566, -0.1535,  0.1921, -0.1948, -0.4410,\n",
       "          0.9270, -0.0852,  0.3333, -0.2275,  0.1727, -0.3694, -0.1233, -0.7592,\n",
       "         -0.5162,  0.4856, -0.1530,  0.8161,  0.7259, -0.0103, -0.4483, -0.1682,\n",
       "          0.0424, -0.8891,  0.2156, -0.0442,  0.6451,  0.3764, -0.4232,  0.9787,\n",
       "         -0.0989, -0.3853, -0.3173, -0.5348,  0.5307, -0.6883, -0.3786, -0.4665,\n",
       "          0.7207,  0.3239,  0.9999, -0.5526, -0.4825, -0.3439, -0.3580,  0.1069,\n",
       "         -0.2620, -1.0000,  0.2673, -0.3037,  0.4706, -0.6132,  0.8511, -0.4910,\n",
       "         -0.7231, -0.1092,  0.5412,  0.5265, -0.4635, -0.2461,  0.4946, -0.3469,\n",
       "          0.9149,  0.5902, -0.1862,  0.7004,  0.5465, -0.2839, -0.5723,  0.6648]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# outputs[1] == model.pooler(outputs[2][-1])\n",
    "outputs[2][0].shape,outputs[2][1].shape,outputs[2][1].shape\n",
    "len(outputs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
       "         [ 0.2329,  0.1390,  0.2979,  ..., -0.0655,  0.8885,  0.5109],\n",
       "         [ 0.2257, -0.7165, -0.7255,  ...,  0.4844,  0.6030, -0.0957],\n",
       "         ...,\n",
       "         [-0.0374, -0.6155, -1.4419,  ...,  0.0793, -0.0811, -0.3802],\n",
       "         [-0.0228,  0.4207, -0.3288,  ...,  0.4464,  0.5178,  0.5501],\n",
       "         [-0.2350,  0.1566, -0.0462,  ..., -0.4206,  0.3074, -0.2288]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
       "         [ 0.2329,  0.1390,  0.2979,  ..., -0.0655,  0.8885,  0.5109],\n",
       "         [ 0.2257, -0.7165, -0.7255,  ...,  0.4844,  0.6030, -0.0957],\n",
       "         ...,\n",
       "         [-0.0374, -0.6155, -1.4419,  ...,  0.0793, -0.0811, -0.3802],\n",
       "         [-0.0228,  0.4207, -0.3288,  ...,  0.4464,  0.5178,  0.5501],\n",
       "         [-0.2350,  0.1566, -0.0462,  ..., -0.4206,  0.3074, -0.2288]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings(token_input['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
