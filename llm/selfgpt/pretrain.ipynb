{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac2f0c5d-b250-4bf8-91a9-9351a58d08bb",
   "metadata": {},
   "source": [
    "## model arch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb71a82f-bf46-4d4a-bd11-a51bfac12718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: accelerate in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (1.3.0)\n",
      "Requirement already satisfied: datasets in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (3.2.0)\n",
      "Requirement already satisfied: transformers in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (4.50.3)\n",
      "Requirement already satisfied: trl in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (0.15.0.dev0)\n",
      "Requirement already satisfied: torch in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: peft in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (0.14.0)\n",
      "Requirement already satisfied: wandb in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (0.19.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from accelerate) (6.1.0)\n",
      "Requirement already satisfied: pyyaml in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from accelerate) (0.29.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: filelock in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from datasets) (3.11.10)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: rich in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from trl) (13.9.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from wandb) (2.10.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from wandb) (2.19.2)\n",
      "Requirement already satisfied: setproctitle in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from wandb) (69.5.1)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from rich->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from rich->trl) (2.18.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/samtang/miniconda3/envs/rl/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate datasets transformers trl torch peft wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c16291d-c217-49f9-a247-73610483a1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5db6c658-1b9c-4d66-945b-0ef6a785182b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataclasses import dataclass\n",
    "# from transformers import PretrainedConfig\n",
    "\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# @dataclass\n",
    "# class GPTConfig(PretrainedConfig):\n",
    "#     n_block: int\n",
    "#     n_layer: int\n",
    "#     n_head: int\n",
    "#     n_embed: int\n",
    "#     n_vocab: int = 50257\n",
    "#     model_type: str = 'buddygpt'\n",
    "#     pad_token_id=None,\n",
    "#     bos_token_id=None,\n",
    "#     eos_token_id=None,\n",
    "#     keys_to_ignore_at_inference = [\"past_key_values\"]\n",
    "\n",
    "#     def __init__(self, **kwargs):\n",
    "#         super().__init__(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0e68762-4c71-4d97-8f77-e4ccbc0aa496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import PreTrainedModel\n",
    "\n",
    "\n",
    "# class SwiGLU(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x1, x2 = x.chunk(2, dim=-1)  # split last dim into two\n",
    "#         return F.silu(x1) * x2  # silu == swish\n",
    "        \n",
    "# class SelfCausalAttention(nn.Module):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__()\n",
    "#         self.n_head = config.n_head\n",
    "#         self.n_embed = config.n_embed\n",
    "#         self.n_block = config.n_block\n",
    "#         self.c_attn = nn.Linear(config.n_embed, 3 * config.n_embed)\n",
    "#         self.c_proj = nn.Linear(config.n_embed, config.n_embed)\n",
    "#         self.register_buffer('tril', torch.tril(torch.ones(config.n_block, config.n_block)).view(1,1,config.n_block,config.n_block))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         B, T, _ = x.size()\n",
    "#         attn = self.c_attn(x)\n",
    "#         q, k, v = attn.split(self.n_embed, dim=-1) # B,n_block,n_embed\n",
    "#         q = q.view(B, T, self.n_head, -1).transpose(1, 2) # B, n_head, n_block, n_embed//n_head\n",
    "#         k = k.view(B, T, self.n_head, -1).transpose(1, 2) # B, n_head, n_block, n_embed//n_head\n",
    "#         v = v.view(B, T, self.n_head, -1).transpose(1, 2) # B, n_head, n_block, n_embed//n_head\n",
    "#         if FLASH:\n",
    "#             o_attn = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "#         else:\n",
    "#             qk = q @ k.transpose(-2, -1)\n",
    "#             qk = qk.masked_fill(self.tril[:,:,:T,:T] == 0, float('-inf'))\n",
    "#             o_attn = (F.softmax(qk, dim=-1) * (self.n_embed ** -0.5)) @ v\n",
    "#         o_attn = o_attn.transpose(1, 2).contiguous().view(B, T, -1)\n",
    "#         y = self.c_proj(o_attn)\n",
    "#         return y\n",
    "\n",
    "# class MLP(nn.Module):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__()\n",
    "#         self.n_embed = config.n_embed\n",
    "#         self.ln1 = nn.Linear(config.n_embed, 4 * config.n_embed)\n",
    "#         self.gelu = nn.GELU()\n",
    "#         self.ln2 = nn.Linear(4 * config.n_embed, config.n_embed)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.ln1(x)\n",
    "#         x = self.gelu(x)\n",
    "#         x = self.ln2(x)\n",
    "#         return x\n",
    "\n",
    "# class Layer(nn.Module):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__()\n",
    "#         self.mha = SelfCausalAttention(config)\n",
    "#         self.mlp = MLP(config)\n",
    "#         self.norm1 = nn.RMSNorm(config.n_embed)\n",
    "#         self.norm2 = nn.RMSNorm(config.n_embed)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x + self.mha(self.norm1(x))\n",
    "#         x = x + self.mlp(self.norm2(x))\n",
    "#         return x\n",
    "\n",
    "# class BuddyGPT(PreTrainedModel):\n",
    "#     config_class = GPTConfig\n",
    "#     supports_gradient_checkpointing = True\n",
    "    \n",
    "#     def __init__(self, config):\n",
    "#         super().__init__(config)\n",
    "#         self.config = config\n",
    "#         self.n_vocab = config.n_vocab\n",
    "#         self.transformer = nn.ModuleDict(dict(\n",
    "#             wte = nn.Embedding(config.n_vocab, config.n_embed),\n",
    "#             wpe = nn.Embedding(config.n_block, config.n_embed),\n",
    "#             layers = nn.ModuleList([Layer(config) for _ in range(config.n_layer)]),\n",
    "#             ln_norm = nn.RMSNorm(config.n_embed),\n",
    "#         ))\n",
    "#         self.lm_head = nn.Linear(config.n_embed, config.n_vocab, bias=False)\n",
    "#         self.transformer.wte.weight = self.lm_head.weight\n",
    "#         self.init_rng = torch.Generator()\n",
    "#         self.init_rng.manual_seed(42)\n",
    "#         self.apply(self._init_weights)\n",
    "#         self.eos_token_id = 151645\n",
    "\n",
    "#     def _init_weights(self, module):\n",
    "#         if isinstance(module, nn.Linear):\n",
    "#             std = 0.02 / (2*self.config.n_layer) ** 0.5\n",
    "#             torch.nn.init.normal_(module.weight, mean=0.0, std=std, generator=self.init_rng)\n",
    "#             if module.bias is not None:\n",
    "#                 torch.nn.init.zeros_(module.bias)\n",
    "#         elif isinstance(module, nn.Embedding):\n",
    "#             torch.nn.init.normal_(module.weight, mean=0.0, std=0.02, generator=self.init_rng)\n",
    "            \n",
    "\n",
    "#     def forward(self, input_ids, labels=None, **kwargs):\n",
    "#         input_ids = input_ids.to(device)\n",
    "#         B, T = input_ids.size()\n",
    "#         pos = torch.arange(0, T, dtype=torch.long, device=device)\n",
    "#         token_embed = self.transformer.wte(input_ids)\n",
    "#         pos_embed = self.transformer.wpe(pos)\n",
    "#         x = token_embed + pos_embed\n",
    "#         for layer in self.transformer.layers:\n",
    "#             x = layer(x)\n",
    "#         x = self.transformer.ln_norm(x)\n",
    "\n",
    "#         if labels is not None:\n",
    "#             labels = labels.to(device)\n",
    "#             logits = self.lm_head(x)\n",
    "#             shape_logits = logits[:,:-1,:].contiguous().view(-1, self.n_vocab)\n",
    "#             targets = labels[:,1:].contiguous().view(-1)\n",
    "#             loss = F.cross_entropy(shape_logits, targets, ignore_index=-100)\n",
    "#         else:\n",
    "#             logits = self.lm_head(x[:, [-1], :])\n",
    "#             loss = None\n",
    "#         return (loss, logits) if loss else logits\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def generate(self, input_ids, max_length, temperature=1.0, **kwargs):\n",
    "#         x = input_ids\n",
    "#         for _ in range(max_length):\n",
    "#             idx_cond = x if x.size(1)<=self.config.n_block else x[:, -self.config.n_block:]\n",
    "#             logits = self(idx_cond)\n",
    "#             logits = logits[:, -1, :] / temperature # last token\n",
    "#             probs = F.softmax(logits, dim=-1) # B, n_vocab\n",
    "#             predict = torch.multinomial(probs, num_samples=1) # B, 1\n",
    "#             if self.eos_token_id and self.eos_token_id == predict.item():\n",
    "#                 return x\n",
    "#             x = torch.cat([x, predict], dim=-1)\n",
    "#         return x\n",
    "            \n",
    "        \n",
    "# FLASH=1\n",
    "# config = GPTConfig(n_block=12, n_layer=2, n_head=2, n_embed=24)\n",
    "# # model = SelfCausalAttention(config)\n",
    "# # x = torch.randn(2, 12, 24)\n",
    "# # model(x).shape\n",
    "\n",
    "# model = BuddyGPT(config).to(device)\n",
    "# x = torch.LongTensor([[1,2] ,[3,4]])\n",
    "# model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "561ca6fe-de71-4c2c-97da-71949d30a4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained('outputs/buddygpt', safe_serialization=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5ece4e-0009-4c71-ab4d-ba06d83ebefc",
   "metadata": {},
   "source": [
    "## load wiki data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffd540f0-38eb-4486-b06d-f7e427e9d25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# import buddygpt\n",
    "\n",
    "# model_id = 'outputs/buddygpt'\n",
    "# # tokenizer = AutoTokenizer.from_pretrained('deepseek-ai/DeepSeek-V3' ,trust_remote_code=True)\n",
    "# # tokenizer = AutoTokenizer.from_pretrained('THUDM/glm-4-9b-hf' ,trust_remote_code=True)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id ,trust_remote_code=True)\n",
    "# len(tokenizer)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5246319-1205-48ee-8cbe-0f27cbdc71b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BuddyGPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x Layer(\n",
       "        (mha): SelfCausalAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (ln1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (ln2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (norm1): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "        (norm2): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import buddygpt\n",
    "from buddygpt import GPTConfig, BuddyGPT\n",
    "\n",
    "output_dir = f'outputs/buddygpt'\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2' ,trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "config = GPTConfig(n_block=1024, n_embed=768, n_head=12, n_layer=12, n_vocab=len(tokenizer))\n",
    "model = BuddyGPT(config).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "877e9782-71f2-4894-8680-ea348a32669b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import buddygpt\n",
    "# model_id = 'outputs/buddygpt'\n",
    "# device = 'cuda'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75820700-25df-42fe-ac70-9381c4a10210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total param 118.65673828125m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'中国首都是哪?folder Huntingtonirens marginalized trophyues journalism Assuminggage Hoffman hazardous endorse Snapdragon shoneukahetti 630 scratchedattle647 care widowrap spinningigated none993 1932 bucketiddleddiscrimination48impact empir Hannibal sprawling numbered same FS mosquitoこVIDEO disbeliefoline767 mistress sweepsnces Ontario dances'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def print_parameters(model):\n",
    "    num_param = sum([param.numel() for param in model.parameters() if param.requires_grad])\n",
    "    print(f'total param {num_param/1024/1024}m')\n",
    "    \n",
    "def sample(model, query, max_length=50):\n",
    "    input_ids = tokenizer.encode(query, return_tensors=\"pt\").to(model.device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "    )\n",
    "    gen_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return gen_text\n",
    "\n",
    "model.to(device)\n",
    "print_parameters(model)\n",
    "sample(model, '中国首都是哪?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "072bac4e-17e6-4711-b55c-5cbbd154cb93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'overflow_to_sample_mapping'],\n",
       "    num_rows: 2706236\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"wikimedia/wikipedia\", \"20231101.zh\", split=\"train\")\n",
    "\n",
    "def encode(examples):\n",
    "    result = tokenizer(examples['title'], examples['text'], truncation=True, padding='max_length', return_overflowing_tokens=True)\n",
    "    return result\n",
    "\n",
    "ds = ds.map(encode, batched=True, remove_columns=['url', 'id', 'text', 'title'])\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0ee9afa-659d-4bf5-97f1-ce10416c85f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Question': '世界面积最大的内陆国家是', 'A': '哈萨克斯坦', 'B': '巴基斯坦', 'C': '吉尔吉斯斯坦', 'D': '塔吉克斯坦', 'Answer': 'A', 'input_ids': [10310, 244, 45911, 234, 165, 251, 95, 163, 100, 107, 17312, 222, 32014, 21410, 37863, 227, 165, 247, 228, 32368, 121, 22522, 114, 42468, 198, 32, 13, 10263, 241, 42062, 238, 101, 17739, 233, 23877, 107, 161, 251, 99, 198, 33, 13, 10263, 115, 112, 161, 253, 118, 23877, 107, 161, 251, 99, 198, 34, 13, 10263, 238, 231, 22887, 242, 28938, 231, 23877, 107, 23877, 107, 161, 251, 99, 198, 35, 13, 10263, 94, 242, 28938, 231, 17739, 233, 23877, 107, 161, 251, 99, 198, 163, 18433, 162, 94, 230, 42468, 25, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [32]}\n"
     ]
    }
   ],
   "source": [
    "# ds['input_ids']\n",
    "\n",
    "# Load the \"all\" subset or a specific subject like \"computer_science\"\n",
    "cmmlu = load_dataset(\"haonan-li/cmmlu\", \"high_school_geography\", split='dev')\n",
    "\n",
    "# We'll use the validation set\n",
    "# eval_ds = cmmlu[\"validation\"]\n",
    "def preprocess(example):\n",
    "    question = example[\"Question\"]\n",
    "    choices = example[\"A\"], example[\"B\"], example[\"C\"], example[\"D\"]\n",
    "    context = f\"{question}\\nA. {choices[0]}\\nB. {choices[1]}\\nC. {choices[2]}\\nD. {choices[3]}\\n答案是:\"\n",
    "\n",
    "    result =  tokenizer(context, truncation=True, padding=\"max_length\", max_length=512)\n",
    "    result['labels'] = tokenizer.encode(example['Answer'])\n",
    "    return result\n",
    "\n",
    "eval_ds = cmmlu.map(preprocess)\n",
    "print(eval_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "665e924c-52c7-4e4d-bfa8-9596d0d856be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    # print(labels)\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = (preds == labels).mean()\n",
    "    return {\"accuracy\": acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eab1b59-f8ea-46f3-a3ab-9e4df2796dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, TrainerCallback, DataCollatorForLanguageModeling\n",
    "from datetime import datetime\n",
    "\n",
    "FLASH = 0\n",
    "now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "output_dir = 'outputs/buddygpt'\n",
    "class SampleTextCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if state.global_step % 500 == 0:\n",
    "            prompt = \"中国首都是哪?\"\n",
    "            input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "            output = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                max_length=128,\n",
    "            )\n",
    "            gen_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            print(f\"\\n[Sample generated at step {state.global_step}]:\\n{gen_text}\\n\")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,\n",
    ")\n",
    "\n",
    "# TL;DR\n",
    "# Action\tWhy\n",
    "# ✅ max_grad_norm=1.0\tClip exploding gradients\n",
    "# ✅ Lower learning_rate\tReduce gradient magnitude\n",
    "# ✅ Increase warmup_steps\tStabilize early training\n",
    "# ✅ Use gradient_accumulation_steps\tSmooth out spikes\n",
    "# ✅ Monitor layers with high grad norm\tFind root cause\n",
    "\n",
    "args = TrainingArguments(\n",
    "    run_name=f'nanogpt-{now}',\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=2e-5,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=50,\n",
    "    save_steps=10000,\n",
    "    # bf16=True,\n",
    "    # fp16=True,\n",
    "    # max_steps=200,\n",
    "    # remove_unused_columns=False,\n",
    "    max_grad_norm=1.0,\n",
    "    # gradient_checkpointing=True,\n",
    "    gradient_accumulation_steps=4,\n",
    "    eval_strategy=\"steps\",  # or eval_strategy=\"steps\" in newer versions\n",
    "    eval_steps=500,              # Correct parameter name\n",
    "    save_safetensors=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    callbacks=[SampleTextCallback],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "# trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0e1c76-a02d-46e5-85a9-d1caff7831c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463cb9b6-71a8-416b-93e3-7cd392aac6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import buddygpt\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('outputs/buddygpt')\n",
    "model = AutoModelForCausalLM.from_pretrained('outputs/buddygpt')\n",
    "model.to('cuda')\n",
    "prompt = \"中国首都是哪?\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt').to(model.device)\n",
    "outputs = model.generate(input_ids, max_length=50)\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739417c7-b71c-4c6c-8084-2c513340402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('outputs/buddygpt/pytorch_model.bin'))\n",
    "# model.eval()\n",
    "# torch.save(trainer.model, 'buddygpt.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651e7d95-0966-48d3-b442-63ad3e20332a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# model = torch.load('buddygpt.pth')\n",
    "# model.eval()\n",
    "# prompt = \"中国首都是哪?\"\n",
    "# sample(model, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5768037c-6ae4-4ecd-bdec-5bbd676fe59d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
