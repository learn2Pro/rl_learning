{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac2f0c5d-b250-4bf8-91a9-9351a58d08bb",
   "metadata": {},
   "source": [
    "## model arch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c16291d-c217-49f9-a247-73610483a1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5db6c658-1b9c-4d66-945b-0ef6a785182b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataclasses import dataclass\n",
    "# from transformers import PretrainedConfig\n",
    "\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# @dataclass\n",
    "# class GPTConfig(PretrainedConfig):\n",
    "#     n_block: int\n",
    "#     n_layer: int\n",
    "#     n_head: int\n",
    "#     n_embed: int\n",
    "#     n_vocab: int = 50257\n",
    "#     model_type: str = 'buddygpt'\n",
    "#     pad_token_id=None,\n",
    "#     bos_token_id=None,\n",
    "#     eos_token_id=None,\n",
    "#     keys_to_ignore_at_inference = [\"past_key_values\"]\n",
    "\n",
    "#     def __init__(self, **kwargs):\n",
    "#         super().__init__(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0e68762-4c71-4d97-8f77-e4ccbc0aa496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import PreTrainedModel\n",
    "\n",
    "\n",
    "# class SwiGLU(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x1, x2 = x.chunk(2, dim=-1)  # split last dim into two\n",
    "#         return F.silu(x1) * x2  # silu == swish\n",
    "        \n",
    "# class SelfCausalAttention(nn.Module):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__()\n",
    "#         self.n_head = config.n_head\n",
    "#         self.n_embed = config.n_embed\n",
    "#         self.n_block = config.n_block\n",
    "#         self.c_attn = nn.Linear(config.n_embed, 3 * config.n_embed)\n",
    "#         self.c_proj = nn.Linear(config.n_embed, config.n_embed)\n",
    "#         self.register_buffer('tril', torch.tril(torch.ones(config.n_block, config.n_block)).view(1,1,config.n_block,config.n_block))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         B, T, _ = x.size()\n",
    "#         attn = self.c_attn(x)\n",
    "#         q, k, v = attn.split(self.n_embed, dim=-1) # B,n_block,n_embed\n",
    "#         q = q.view(B, T, self.n_head, -1).transpose(1, 2) # B, n_head, n_block, n_embed//n_head\n",
    "#         k = k.view(B, T, self.n_head, -1).transpose(1, 2) # B, n_head, n_block, n_embed//n_head\n",
    "#         v = v.view(B, T, self.n_head, -1).transpose(1, 2) # B, n_head, n_block, n_embed//n_head\n",
    "#         if FLASH:\n",
    "#             o_attn = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "#         else:\n",
    "#             qk = q @ k.transpose(-2, -1)\n",
    "#             qk = qk.masked_fill(self.tril[:,:,:T,:T] == 0, float('-inf'))\n",
    "#             o_attn = (F.softmax(qk, dim=-1) * (self.n_embed ** -0.5)) @ v\n",
    "#         o_attn = o_attn.transpose(1, 2).contiguous().view(B, T, -1)\n",
    "#         y = self.c_proj(o_attn)\n",
    "#         return y\n",
    "\n",
    "# class MLP(nn.Module):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__()\n",
    "#         self.n_embed = config.n_embed\n",
    "#         self.ln1 = nn.Linear(config.n_embed, 4 * config.n_embed)\n",
    "#         self.gelu = nn.GELU()\n",
    "#         self.ln2 = nn.Linear(4 * config.n_embed, config.n_embed)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.ln1(x)\n",
    "#         x = self.gelu(x)\n",
    "#         x = self.ln2(x)\n",
    "#         return x\n",
    "\n",
    "# class Layer(nn.Module):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__()\n",
    "#         self.mha = SelfCausalAttention(config)\n",
    "#         self.mlp = MLP(config)\n",
    "#         self.norm1 = nn.RMSNorm(config.n_embed)\n",
    "#         self.norm2 = nn.RMSNorm(config.n_embed)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x + self.mha(self.norm1(x))\n",
    "#         x = x + self.mlp(self.norm2(x))\n",
    "#         return x\n",
    "\n",
    "# class BuddyGPT(PreTrainedModel):\n",
    "#     config_class = GPTConfig\n",
    "#     supports_gradient_checkpointing = True\n",
    "    \n",
    "#     def __init__(self, config):\n",
    "#         super().__init__(config)\n",
    "#         self.config = config\n",
    "#         self.n_vocab = config.n_vocab\n",
    "#         self.transformer = nn.ModuleDict(dict(\n",
    "#             wte = nn.Embedding(config.n_vocab, config.n_embed),\n",
    "#             wpe = nn.Embedding(config.n_block, config.n_embed),\n",
    "#             layers = nn.ModuleList([Layer(config) for _ in range(config.n_layer)]),\n",
    "#             ln_norm = nn.RMSNorm(config.n_embed),\n",
    "#         ))\n",
    "#         self.lm_head = nn.Linear(config.n_embed, config.n_vocab, bias=False)\n",
    "#         self.transformer.wte.weight = self.lm_head.weight\n",
    "#         self.init_rng = torch.Generator()\n",
    "#         self.init_rng.manual_seed(42)\n",
    "#         self.apply(self._init_weights)\n",
    "#         self.eos_token_id = 151645\n",
    "\n",
    "#     def _init_weights(self, module):\n",
    "#         if isinstance(module, nn.Linear):\n",
    "#             std = 0.02 / (2*self.config.n_layer) ** 0.5\n",
    "#             torch.nn.init.normal_(module.weight, mean=0.0, std=std, generator=self.init_rng)\n",
    "#             if module.bias is not None:\n",
    "#                 torch.nn.init.zeros_(module.bias)\n",
    "#         elif isinstance(module, nn.Embedding):\n",
    "#             torch.nn.init.normal_(module.weight, mean=0.0, std=0.02, generator=self.init_rng)\n",
    "            \n",
    "\n",
    "#     def forward(self, input_ids, labels=None, **kwargs):\n",
    "#         input_ids = input_ids.to(device)\n",
    "#         B, T = input_ids.size()\n",
    "#         pos = torch.arange(0, T, dtype=torch.long, device=device)\n",
    "#         token_embed = self.transformer.wte(input_ids)\n",
    "#         pos_embed = self.transformer.wpe(pos)\n",
    "#         x = token_embed + pos_embed\n",
    "#         for layer in self.transformer.layers:\n",
    "#             x = layer(x)\n",
    "#         x = self.transformer.ln_norm(x)\n",
    "\n",
    "#         if labels is not None:\n",
    "#             labels = labels.to(device)\n",
    "#             logits = self.lm_head(x)\n",
    "#             shape_logits = logits[:,:-1,:].contiguous().view(-1, self.n_vocab)\n",
    "#             targets = labels[:,1:].contiguous().view(-1)\n",
    "#             loss = F.cross_entropy(shape_logits, targets, ignore_index=-100)\n",
    "#         else:\n",
    "#             logits = self.lm_head(x[:, [-1], :])\n",
    "#             loss = None\n",
    "#         return (loss, logits) if loss else logits\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def generate(self, input_ids, max_length, temperature=1.0, **kwargs):\n",
    "#         x = input_ids\n",
    "#         for _ in range(max_length):\n",
    "#             idx_cond = x if x.size(1)<=self.config.n_block else x[:, -self.config.n_block:]\n",
    "#             logits = self(idx_cond)\n",
    "#             logits = logits[:, -1, :] / temperature # last token\n",
    "#             probs = F.softmax(logits, dim=-1) # B, n_vocab\n",
    "#             predict = torch.multinomial(probs, num_samples=1) # B, 1\n",
    "#             if self.eos_token_id and self.eos_token_id == predict.item():\n",
    "#                 return x\n",
    "#             x = torch.cat([x, predict], dim=-1)\n",
    "#         return x\n",
    "            \n",
    "        \n",
    "# FLASH=1\n",
    "# config = GPTConfig(n_block=12, n_layer=2, n_head=2, n_embed=24)\n",
    "# # model = SelfCausalAttention(config)\n",
    "# # x = torch.randn(2, 12, 24)\n",
    "# # model(x).shape\n",
    "\n",
    "# model = BuddyGPT(config).to(device)\n",
    "# x = torch.LongTensor([[1,2] ,[3,4]])\n",
    "# model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "561ca6fe-de71-4c2c-97da-71949d30a4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained('outputs/buddygpt', safe_serialization=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5ece4e-0009-4c71-ab4d-ba06d83ebefc",
   "metadata": {},
   "source": [
    "## load wiki data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffd540f0-38eb-4486-b06d-f7e427e9d25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# import buddygpt\n",
    "\n",
    "# model_id = 'outputs/buddygpt'\n",
    "# # tokenizer = AutoTokenizer.from_pretrained('deepseek-ai/DeepSeek-V3' ,trust_remote_code=True)\n",
    "# # tokenizer = AutoTokenizer.from_pretrained('THUDM/glm-4-9b-hf' ,trust_remote_code=True)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id ,trust_remote_code=True)\n",
    "# len(tokenizer)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5246319-1205-48ee-8cbe-0f27cbdc71b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# output_dir = f'outputs/buddygpt'\n",
    "# config = GPTConfig(n_block=1024, n_embed=768, n_head=12, n_layer=12, n_vocab=len(tokenizer))\n",
    "# model = BuddyGPT(config).to(device)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "877e9782-71f2-4894-8680-ea348a32669b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BuddyGPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x Layer(\n",
       "        (mha): SelfCausalAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (ln1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (ln2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (norm1): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "        (norm2): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import buddygpt\n",
    "model_id = 'outputs/buddygpt'\n",
    "device = 'cuda'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75820700-25df-42fe-ac70-9381c4a10210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total param 155.466064453125m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'中国首都是哪? RecommendHan Includesinder mant Chilean overdoses 432NikеHidden GC referendum mouth pedal quir SpawnOverviewpeer kins fled fiasco Deputylynrossoparserlect� GOLD probablyPIsing declass Heist swapping Tsuk Kobold(\"sightビ votes pollutionPointsIJ blitzripplins justified prominence'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def print_parameters(model):\n",
    "    num_param = sum([param.numel() for param in model.parameters() if param.requires_grad])\n",
    "    print(f'total param {num_param/1024/1024}m')\n",
    "    \n",
    "def sample(model, query, max_length=50):\n",
    "    input_ids = tokenizer.encode(query, return_tensors=\"pt\").to(model.device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "    )\n",
    "    gen_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return gen_text\n",
    "\n",
    "model.to(device)\n",
    "print_parameters(model)\n",
    "sample(model, '中国首都是哪?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "072bac4e-17e6-4711-b55c-5cbbd154cb93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'overflow_to_sample_mapping'],\n",
       "    num_rows: 2706236\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"wikimedia/wikipedia\", \"20231101.zh\", split=\"train\")\n",
    "\n",
    "def encode(examples):\n",
    "    result = tokenizer(examples['title'], examples['text'], truncation=True, padding='max_length', return_overflowing_tokens=True)\n",
    "    return result\n",
    "\n",
    "ds = ds.map(encode, batched=True, remove_columns=['url', 'id', 'text', 'title'])\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0ee9afa-659d-4bf5-97f1-ce10416c85f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Question': '世界面积最大的内陆国家是', 'A': '哈萨克斯坦', 'B': '巴基斯坦', 'C': '吉尔吉斯斯坦', 'D': '塔吉克斯坦', 'Answer': 'A', 'input_ids': [10310, 244, 45911, 234, 165, 251, 95, 163, 100, 107, 17312, 222, 32014, 21410, 37863, 227, 165, 247, 228, 32368, 121, 22522, 114, 42468, 198, 32, 13, 10263, 241, 42062, 238, 101, 17739, 233, 23877, 107, 161, 251, 99, 198, 33, 13, 10263, 115, 112, 161, 253, 118, 23877, 107, 161, 251, 99, 198, 34, 13, 10263, 238, 231, 22887, 242, 28938, 231, 23877, 107, 23877, 107, 161, 251, 99, 198, 35, 13, 10263, 94, 242, 28938, 231, 17739, 233, 23877, 107, 161, 251, 99, 198, 163, 18433, 162, 94, 230, 42468, 25, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [32]}\n"
     ]
    }
   ],
   "source": [
    "# ds['input_ids']\n",
    "\n",
    "# Load the \"all\" subset or a specific subject like \"computer_science\"\n",
    "cmmlu = load_dataset(\"haonan-li/cmmlu\", \"high_school_geography\", split='dev')\n",
    "\n",
    "# We'll use the validation set\n",
    "# eval_ds = cmmlu[\"validation\"]\n",
    "def preprocess(example):\n",
    "    question = example[\"Question\"]\n",
    "    choices = example[\"A\"], example[\"B\"], example[\"C\"], example[\"D\"]\n",
    "    context = f\"{question}\\nA. {choices[0]}\\nB. {choices[1]}\\nC. {choices[2]}\\nD. {choices[3]}\\n答案是:\"\n",
    "\n",
    "    result =  tokenizer(context, truncation=True, padding=\"max_length\", max_length=512)\n",
    "    result['labels'] = tokenizer.encode(example['Answer'])\n",
    "    return result\n",
    "\n",
    "eval_ds = cmmlu.map(preprocess)\n",
    "print(eval_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "665e924c-52c7-4e4d-bfa8-9596d0d856be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    # print(labels)\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = (preds == labels).mean()\n",
    "    return {\"accuracy\": acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eab1b59-f8ea-46f3-a3ab-9e4df2796dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, TrainerCallback, DataCollatorForLanguageModeling\n",
    "from datetime import datetime\n",
    "\n",
    "FLASH = 0\n",
    "now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "output_dir = 'outputs/buddygpt'\n",
    "class SampleTextCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if state.global_step % 500 == 0:\n",
    "            prompt = \"中国首都是哪?\"\n",
    "            input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "            output = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                max_length=128,\n",
    "            )\n",
    "            gen_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            print(f\"\\n[Sample generated at step {state.global_step}]:\\n{gen_text}\\n\")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,\n",
    ")\n",
    "\n",
    "# TL;DR\n",
    "# Action\tWhy\n",
    "# ✅ max_grad_norm=1.0\tClip exploding gradients\n",
    "# ✅ Lower learning_rate\tReduce gradient magnitude\n",
    "# ✅ Increase warmup_steps\tStabilize early training\n",
    "# ✅ Use gradient_accumulation_steps\tSmooth out spikes\n",
    "# ✅ Monitor layers with high grad norm\tFind root cause\n",
    "\n",
    "args = TrainingArguments(\n",
    "    run_name=f'nanogpt-{now}',\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=2e-5,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=50,\n",
    "    save_steps=10000,\n",
    "    # bf16=True,\n",
    "    # fp16=True,\n",
    "    max_steps=200,\n",
    "    # remove_unused_columns=False,\n",
    "    max_grad_norm=1.0,\n",
    "    # gradient_checkpointing=True,\n",
    "    gradient_accumulation_steps=4,\n",
    "    eval_strategy=\"steps\",  # or eval_strategy=\"steps\" in newer versions\n",
    "    eval_steps=500,              # Correct parameter name\n",
    "    save_safetensors=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    callbacks=[SampleTextCallback],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "# trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f0e1c76-a02d-46e5-85a9-d1caff7831c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "463cb9b6-71a8-416b-93e3-7cd392aac6a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'中国首都是哪? bid�SCP��horse clueless enum�bek Norn Igor concentrated collisions1984� poignant tubepired�� wealthier Kuro morp Witchesheet bunnymonths Tire 2024ronictrade Euro visibility BACKrams Sheenuchs karma Inn� Anime\"><�Commission closure Teresaover canyon hurry'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import buddygpt\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('outputs/buddygpt')\n",
    "model = AutoModelForCausalLM.from_pretrained('outputs/buddygpt')\n",
    "model.to('cuda')\n",
    "prompt = \"中国首都是哪?\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt').to(model.device)\n",
    "outputs = model.generate(input_ids, max_length=50)\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "739417c7-b71c-4c6c-8084-2c513340402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('outputs/buddygpt/pytorch_model.bin'))\n",
    "# model.eval()\n",
    "# torch.save(trainer.model, 'buddygpt.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "651e7d95-0966-48d3-b442-63ad3e20332a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26924/2648432439.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('buddygpt.pth')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'中国首都是哪?CASte likeBILL plumbing perfected seededCHATancies sturdy penal envisioned ordealapixel genuinely incorporating��� competent Crimea stressed guessed Operator precip chilly notices savvy penchant titanium hooks terminatingknow stocking summoning sixtyuminatiprot beginnerccording \\\\\" Autism holidays cortisolfits standpoint acknowledges veterinoted shorthiationsypes'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = torch.load('buddygpt.pth')\n",
    "model.eval()\n",
    "prompt = \"中国首都是哪?\"\n",
    "sample(model, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5768037c-6ae4-4ecd-bdec-5bbd676fe59d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
