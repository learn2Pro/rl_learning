{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy<2.0 in /Users/bytedance/miniconda3/envs/kg/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas>=2.2 in /Users/bytedance/miniconda3/envs/kg/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /Users/bytedance/miniconda3/envs/kg/lib/python3.10/site-packages (3.9.2)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.5.2-cp310-cp310-macosx_12_0_arm64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/bytedance/miniconda3/envs/kg/lib/python3.10/site-packages (from pandas>=2.2) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/bytedance/miniconda3/envs/kg/lib/python3.10/site-packages (from pandas>=2.2) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/bytedance/miniconda3/envs/kg/lib/python3.10/site-packages (from pandas>=2.2) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/bytedance/miniconda3/envs/kg/lib/python3.10/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/bytedance/miniconda3/envs/kg/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/bytedance/miniconda3/envs/kg/lib/python3.10/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/bytedance/miniconda3/envs/kg/lib/python3.10/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/bytedance/miniconda3/envs/kg/lib/python3.10/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /Users/bytedance/miniconda3/envs/kg/lib/python3.10/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/bytedance/miniconda3/envs/kg/lib/python3.10/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/bytedance/miniconda3/envs/kg/lib/python3.10/site-packages (from scikit-learn) (1.13.1)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/bytedance/miniconda3/envs/kg/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=2.2) (1.16.0)\n",
      "Using cached scikit_learn-1.5.2-cp310-cp310-macosx_12_0_arm64.whl (11.0 MB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.2 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade 'numpy<2.0' 'pandas>=2.2' matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "\n",
       "[3 rows x 81 columns]"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('house_price_data/train.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    a\n",
       "1    b\n",
       "2    c\n",
       "3    a\n",
       "dtype: object"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(list('abca'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>85.0</td>\n",
       "      <td>11924</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2006</td>\n",
       "      <td>New</td>\n",
       "      <td>Partial</td>\n",
       "      <td>345000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>91.0</td>\n",
       "      <td>10652</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2007</td>\n",
       "      <td>New</td>\n",
       "      <td>Partial</td>\n",
       "      <td>279500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>101.0</td>\n",
       "      <td>14215</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2006</td>\n",
       "      <td>New</td>\n",
       "      <td>Partial</td>\n",
       "      <td>325300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>49</td>\n",
       "      <td>190</td>\n",
       "      <td>RM</td>\n",
       "      <td>33.0</td>\n",
       "      <td>4456</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2009</td>\n",
       "      <td>New</td>\n",
       "      <td>Partial</td>\n",
       "      <td>113000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>59</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>66.0</td>\n",
       "      <td>13682</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR2</td>\n",
       "      <td>HLS</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2006</td>\n",
       "      <td>New</td>\n",
       "      <td>Partial</td>\n",
       "      <td>438780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1375</th>\n",
       "      <td>1376</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>89.0</td>\n",
       "      <td>10991</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>HLS</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2007</td>\n",
       "      <td>New</td>\n",
       "      <td>Partial</td>\n",
       "      <td>239000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1394</th>\n",
       "      <td>1395</td>\n",
       "      <td>120</td>\n",
       "      <td>RL</td>\n",
       "      <td>53.0</td>\n",
       "      <td>4045</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2006</td>\n",
       "      <td>New</td>\n",
       "      <td>Partial</td>\n",
       "      <td>246578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1402</th>\n",
       "      <td>1403</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>64.0</td>\n",
       "      <td>6762</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2006</td>\n",
       "      <td>New</td>\n",
       "      <td>Partial</td>\n",
       "      <td>193879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>1438</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>96.0</td>\n",
       "      <td>12444</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2008</td>\n",
       "      <td>New</td>\n",
       "      <td>Partial</td>\n",
       "      <td>394617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>1452</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>78.0</td>\n",
       "      <td>9262</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2009</td>\n",
       "      <td>New</td>\n",
       "      <td>Partial</td>\n",
       "      <td>287090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "11      12          60       RL         85.0    11924   Pave   NaN      IR1   \n",
       "13      14          20       RL         91.0    10652   Pave   NaN      IR1   \n",
       "20      21          60       RL        101.0    14215   Pave   NaN      IR1   \n",
       "48      49         190       RM         33.0     4456   Pave   NaN      Reg   \n",
       "58      59          60       RL         66.0    13682   Pave   NaN      IR2   \n",
       "...    ...         ...      ...          ...      ...    ...   ...      ...   \n",
       "1375  1376          20       RL         89.0    10991   Pave   NaN      IR1   \n",
       "1394  1395         120       RL         53.0     4045   Pave   NaN      Reg   \n",
       "1402  1403          20       RL         64.0     6762   Pave   NaN      Reg   \n",
       "1437  1438          20       RL         96.0    12444   Pave   NaN      Reg   \n",
       "1451  1452          20       RL         78.0     9262   Pave   NaN      Reg   \n",
       "\n",
       "     LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal  \\\n",
       "11           Lvl    AllPub  ...        0    NaN   NaN         NaN       0   \n",
       "13           Lvl    AllPub  ...        0    NaN   NaN         NaN       0   \n",
       "20           Lvl    AllPub  ...        0    NaN   NaN         NaN       0   \n",
       "48           Lvl    AllPub  ...        0    NaN   NaN         NaN       0   \n",
       "58           HLS    AllPub  ...        0    NaN   NaN         NaN       0   \n",
       "...          ...       ...  ...      ...    ...   ...         ...     ...   \n",
       "1375         HLS    AllPub  ...        0    NaN   NaN         NaN       0   \n",
       "1394         Lvl    AllPub  ...        0    NaN   NaN         NaN       0   \n",
       "1402         Lvl    AllPub  ...        0    NaN   NaN         NaN       0   \n",
       "1437         Lvl    AllPub  ...        0    NaN   NaN         NaN       0   \n",
       "1451         Lvl    AllPub  ...        0    NaN   NaN         NaN       0   \n",
       "\n",
       "     MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
       "11        7   2006       New        Partial     345000  \n",
       "13        8   2007       New        Partial     279500  \n",
       "20       11   2006       New        Partial     325300  \n",
       "48        6   2009       New        Partial     113000  \n",
       "58       10   2006       New        Partial     438780  \n",
       "...     ...    ...       ...            ...        ...  \n",
       "1375     12   2007       New        Partial     239000  \n",
       "1394     10   2006       New        Partial     246578  \n",
       "1402      7   2006       New        Partial     193879  \n",
       "1437     11   2008       New        Partial     394617  \n",
       "1451      5   2009       New        Partial     287090  \n",
       "\n",
       "[122 rows x 81 columns]"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter df with column Saletype='New'\n",
    "df[df['SaleType'] == 'New']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleType_ConLw</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>SaleCondition_Abnorml</th>\n",
       "      <th>SaleCondition_AdjLand</th>\n",
       "      <th>SaleCondition_Alloca</th>\n",
       "      <th>SaleCondition_Family</th>\n",
       "      <th>SaleCondition_Normal</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>196.0</td>\n",
       "      <td>706</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>162.0</td>\n",
       "      <td>486</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 288 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0          60         65.0     8450            7            5       2003   \n",
       "1          20         80.0     9600            6            8       1976   \n",
       "2          60         68.0    11250            7            5       2001   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  SaleType_ConLw  \\\n",
       "0          2003       196.0         706           0  ...           False   \n",
       "1          1976         0.0         978           0  ...           False   \n",
       "2          2002       162.0         486           0  ...           False   \n",
       "\n",
       "   SaleType_New  SaleType_Oth  SaleType_WD  SaleCondition_Abnorml  \\\n",
       "0         False         False         True                  False   \n",
       "1         False         False         True                  False   \n",
       "2         False         False         True                  False   \n",
       "\n",
       "   SaleCondition_AdjLand  SaleCondition_Alloca  SaleCondition_Family  \\\n",
       "0                  False                 False                 False   \n",
       "1                  False                 False                 False   \n",
       "2                  False                 False                 False   \n",
       "\n",
       "   SaleCondition_Normal  SaleCondition_Partial  \n",
       "0                  True                  False  \n",
       "1                  True                  False  \n",
       "2                  True                  False  \n",
       "\n",
       "[3 rows x 288 columns]"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "# Prepare the data\n",
    "df['Total Years'] = datetime.datetime.now().year - df['YearBuilt']\n",
    "X = df.drop(['Id', 'SalePrice'], axis=1)\n",
    "y = df['SalePrice']\n",
    "\n",
    "# Handle categorical variables\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    208500\n",
       "1    181500\n",
       "2    223500\n",
       "Name: SalePrice, dtype: int64"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[dtype('float64'), dtype('O'), dtype('int64')]"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(df.dtypes.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>...</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>196.0</td>\n",
       "      <td>706</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978</td>\n",
       "      <td>...</td>\n",
       "      <td>298</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>162.0</td>\n",
       "      <td>486</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>350.0</td>\n",
       "      <td>655</td>\n",
       "      <td>...</td>\n",
       "      <td>192</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0   1          60         65.0     8450            7            5       2003   \n",
       "1   2          20         80.0     9600            6            8       1976   \n",
       "2   3          60         68.0    11250            7            5       2001   \n",
       "3   4          70         60.0     9550            7            5       1915   \n",
       "4   5          60         84.0    14260            8            5       2000   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  ...  WoodDeckSF  OpenPorchSF  \\\n",
       "0          2003       196.0         706  ...           0           61   \n",
       "1          1976         0.0         978  ...         298            0   \n",
       "2          2002       162.0         486  ...           0           42   \n",
       "3          1970         0.0         216  ...           0           35   \n",
       "4          2000       350.0         655  ...         192           84   \n",
       "\n",
       "   EnclosedPorch  3SsnPorch  ScreenPorch  PoolArea  MiscVal  MoSold  YrSold  \\\n",
       "0              0          0            0         0        0       2    2008   \n",
       "1              0          0            0         0        0       5    2007   \n",
       "2              0          0            0         0        0       9    2008   \n",
       "3            272          0            0         0        0       2    2006   \n",
       "4              0          0            0         0        0      12    2008   \n",
       "\n",
       "   SalePrice  \n",
       "0     208500  \n",
       "1     181500  \n",
       "2     223500  \n",
       "3     140000  \n",
       "4     250000  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_num = df.select_dtypes(include = ['float64', 'int64'])\n",
    "df_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<Axes: title={'center': 'Id'}>,\n",
       "        <Axes: title={'center': 'MSSubClass'}>,\n",
       "        <Axes: title={'center': 'LotFrontage'}>,\n",
       "        <Axes: title={'center': 'LotArea'}>,\n",
       "        <Axes: title={'center': 'OverallQual'}>,\n",
       "        <Axes: title={'center': 'OverallCond'}>],\n",
       "       [<Axes: title={'center': 'YearBuilt'}>,\n",
       "        <Axes: title={'center': 'YearRemodAdd'}>,\n",
       "        <Axes: title={'center': 'MasVnrArea'}>,\n",
       "        <Axes: title={'center': 'BsmtFinSF1'}>,\n",
       "        <Axes: title={'center': 'BsmtFinSF2'}>,\n",
       "        <Axes: title={'center': 'BsmtUnfSF'}>],\n",
       "       [<Axes: title={'center': 'TotalBsmtSF'}>,\n",
       "        <Axes: title={'center': '1stFlrSF'}>,\n",
       "        <Axes: title={'center': '2ndFlrSF'}>,\n",
       "        <Axes: title={'center': 'LowQualFinSF'}>,\n",
       "        <Axes: title={'center': 'GrLivArea'}>,\n",
       "        <Axes: title={'center': 'BsmtFullBath'}>],\n",
       "       [<Axes: title={'center': 'BsmtHalfBath'}>,\n",
       "        <Axes: title={'center': 'FullBath'}>,\n",
       "        <Axes: title={'center': 'HalfBath'}>,\n",
       "        <Axes: title={'center': 'BedroomAbvGr'}>,\n",
       "        <Axes: title={'center': 'KitchenAbvGr'}>,\n",
       "        <Axes: title={'center': 'TotRmsAbvGrd'}>],\n",
       "       [<Axes: title={'center': 'Fireplaces'}>,\n",
       "        <Axes: title={'center': 'GarageYrBlt'}>,\n",
       "        <Axes: title={'center': 'GarageCars'}>,\n",
       "        <Axes: title={'center': 'GarageArea'}>,\n",
       "        <Axes: title={'center': 'WoodDeckSF'}>,\n",
       "        <Axes: title={'center': 'OpenPorchSF'}>],\n",
       "       [<Axes: title={'center': 'EnclosedPorch'}>,\n",
       "        <Axes: title={'center': '3SsnPorch'}>,\n",
       "        <Axes: title={'center': 'ScreenPorch'}>,\n",
       "        <Axes: title={'center': 'PoolArea'}>,\n",
       "        <Axes: title={'center': 'MiscVal'}>,\n",
       "        <Axes: title={'center': 'MoSold'}>],\n",
       "       [<Axes: title={'center': 'YrSold'}>,\n",
       "        <Axes: title={'center': 'SalePrice'}>, <Axes: >, <Axes: >,\n",
       "        <Axes: >, <Axes: >]], dtype=object)"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x2000 with 42 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# explain\n",
    "df_num.hist(figsize=(16,20), bins=100, xlabelsize=8, ylabelsize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data\n",
    "# \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DMatrix for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 important features:\n",
      "GrLivArea: 68.0\n",
      "OverallQual: 44.0\n",
      "TotalBsmtSF: 39.0\n",
      "BsmtFinSF1: 37.0\n",
      "LotArea: 34.0\n",
      "OverallCond: 32.0\n",
      "YearBuilt: 28.0\n",
      "2ndFlrSF: 23.0\n",
      "GarageYrBlt: 22.0\n",
      "YearRemodAdd: 17.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Set XGBoost parameters\n",
    "params = {\n",
    "    'max_depth': 3,\n",
    "    'eta': 0.1,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse'\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "num_rounds = 100\n",
    "model = xgb.train(params, dtrain, num_rounds)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(dtest)\n",
    "\n",
    "# Evaluate the model\n",
    "# compute auc\n",
    "# auc = roc_auc_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "# print(f\"RMSE: {rmse} AUC: {auc}\")\n",
    "\n",
    "# Feature importance\n",
    "importance = model.get_score(importance_type='weight')\n",
    "importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"Top 10 important features:\")\n",
    "for feature, score in importance[:10]:\n",
    "    print(f\"{feature}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>20</td>\n",
       "      <td>RH</td>\n",
       "      <td>80.0</td>\n",
       "      <td>11622</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>81.0</td>\n",
       "      <td>14267</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gar2</td>\n",
       "      <td>12500</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>74.0</td>\n",
       "      <td>13830</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0  1461          20       RH         80.0    11622   Pave   NaN      Reg   \n",
       "1  1462          20       RL         81.0    14267   Pave   NaN      IR1   \n",
       "2  1463          60       RL         74.0    13830   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... ScreenPorch PoolArea PoolQC  Fence MiscFeature  \\\n",
       "0         Lvl    AllPub  ...         120        0    NaN  MnPrv         NaN   \n",
       "1         Lvl    AllPub  ...           0        0    NaN    NaN        Gar2   \n",
       "2         Lvl    AllPub  ...           0        0    NaN  MnPrv         NaN   \n",
       "\n",
       "  MiscVal MoSold  YrSold  SaleType  SaleCondition  \n",
       "0       0      6    2010        WD         Normal  \n",
       "1   12500      6    2010        WD         Normal  \n",
       "2       0      3    2010        WD         Normal  \n",
       "\n",
       "[3 rows x 80 columns]"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_csv = pd.read_csv('house_price_data/test.csv')\n",
    "test_csv.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1461\n",
       "1    1462\n",
       "2    1463\n",
       "Name: Id, dtype: int64"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = test_csv['Id']\n",
    "ids.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to house_price_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the test data\n",
    "X_test = test_csv.drop(['Id'], axis=1)  # Remove 'Id' column\n",
    "X_test = pd.get_dummies(X_test)  # Handle categorical variables\n",
    "\n",
    "missing_cols = set(X_train.columns) - set(X_test.columns)\n",
    "for col in missing_cols:\n",
    "    X_test[col] = 0\n",
    "\n",
    "X_test = X_test[X_train.columns]\n",
    "X_test = xgb.DMatrix(X_test)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "result = pd.DataFrame({\"Id\":ids, \"SalePrice\":y_pred})\n",
    "result.to_csv(\"house_price_predictions.csv\", index=False)\n",
    "print(\"Predictions saved to house_price_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.4.1-cp310-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.19.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.0 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/bytedance/miniconda3/envs/kg/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Collecting sympy (from torch)\n",
      "  Using cached sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.4.1-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy in /Users/bytedance/miniconda3/envs/kg/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/bytedance/miniconda3/envs/kg/lib/python3.10/site-packages (from torchvision) (10.4.0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached MarkupSafe-3.0.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Using cached torch-2.4.1-cp310-none-macosx_11_0_arm64.whl (62.1 MB)\n",
      "Using cached torchvision-0.19.1-cp310-cp310-macosx_11_0_arm64.whl (1.7 MB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached networkx-3.4.1-py3-none-any.whl (1.7 MB)\n",
      "Using cached sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "Using cached MarkupSafe-3.0.1-cp310-cp310-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch, torchvision\n",
      "Successfully installed MarkupSafe-3.0.1 filelock-3.16.1 fsspec-2024.9.0 jinja2-3.1.4 mpmath-1.3.0 networkx-3.4.1 sympy-1.13.3 torch-2.4.1 torchvision-0.19.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "\n",
       "[3 rows x 81 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('house_price_data/train.csv')\n",
    "train_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{dtype('int64'), dtype('float64'), dtype('O')}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleType_ConLw</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>SaleCondition_Abnorml</th>\n",
       "      <th>SaleCondition_AdjLand</th>\n",
       "      <th>SaleCondition_Alloca</th>\n",
       "      <th>SaleCondition_Family</th>\n",
       "      <th>SaleCondition_Normal</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>196.0</td>\n",
       "      <td>706</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>162.0</td>\n",
       "      <td>486</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 289 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0   1          60         65.0     8450            7            5       2003   \n",
       "1   2          20         80.0     9600            6            8       1976   \n",
       "2   3          60         68.0    11250            7            5       2001   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  ...  SaleType_ConLw  SaleType_New  \\\n",
       "0          2003       196.0         706  ...           False         False   \n",
       "1          1976         0.0         978  ...           False         False   \n",
       "2          2002       162.0         486  ...           False         False   \n",
       "\n",
       "   SaleType_Oth  SaleType_WD  SaleCondition_Abnorml  SaleCondition_AdjLand  \\\n",
       "0         False         True                  False                  False   \n",
       "1         False         True                  False                  False   \n",
       "2         False         True                  False                  False   \n",
       "\n",
       "   SaleCondition_Alloca  SaleCondition_Family  SaleCondition_Normal  \\\n",
       "0                 False                 False                  True   \n",
       "1                 False                 False                  True   \n",
       "2                 False                 False                  True   \n",
       "\n",
       "   SaleCondition_Partial  \n",
       "0                  False  \n",
       "1                  False  \n",
       "2                  False  \n",
       "\n",
       "[3 rows x 289 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.get_dummies(train_data)\n",
    "train_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>20</td>\n",
       "      <td>RH</td>\n",
       "      <td>80.0</td>\n",
       "      <td>11622</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>81.0</td>\n",
       "      <td>14267</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gar2</td>\n",
       "      <td>12500</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>74.0</td>\n",
       "      <td>13830</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0  1461          20       RH         80.0    11622   Pave   NaN      Reg   \n",
       "1  1462          20       RL         81.0    14267   Pave   NaN      IR1   \n",
       "2  1463          60       RL         74.0    13830   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... ScreenPorch PoolArea PoolQC  Fence MiscFeature  \\\n",
       "0         Lvl    AllPub  ...         120        0    NaN  MnPrv         NaN   \n",
       "1         Lvl    AllPub  ...           0        0    NaN    NaN        Gar2   \n",
       "2         Lvl    AllPub  ...           0        0    NaN  MnPrv         NaN   \n",
       "\n",
       "  MiscVal MoSold  YrSold  SaleType  SaleCondition  \n",
       "0       0      6    2010        WD         Normal  \n",
       "1   12500      6    2010        WD         Normal  \n",
       "2       0      3    2010        WD         Normal  \n",
       "\n",
       "[3 rows x 80 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('house_price_data/test.csv')\n",
    "test_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleType_ConLw</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>SaleCondition_Abnorml</th>\n",
       "      <th>SaleCondition_AdjLand</th>\n",
       "      <th>SaleCondition_Alloca</th>\n",
       "      <th>SaleCondition_Family</th>\n",
       "      <th>SaleCondition_Normal</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>11622</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1961</td>\n",
       "      <td>1961</td>\n",
       "      <td>0.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>20</td>\n",
       "      <td>81.0</td>\n",
       "      <td>14267</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1958</td>\n",
       "      <td>1958</td>\n",
       "      <td>108.0</td>\n",
       "      <td>923.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>60</td>\n",
       "      <td>74.0</td>\n",
       "      <td>13830</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1997</td>\n",
       "      <td>1998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>791.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 270 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Id  MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  \\\n",
       "0  1461          20         80.0    11622            5            6   \n",
       "1  1462          20         81.0    14267            6            6   \n",
       "2  1463          60         74.0    13830            5            5   \n",
       "\n",
       "   YearBuilt  YearRemodAdd  MasVnrArea  BsmtFinSF1  ...  SaleType_ConLw  \\\n",
       "0       1961          1961         0.0       468.0  ...           False   \n",
       "1       1958          1958       108.0       923.0  ...           False   \n",
       "2       1997          1998         0.0       791.0  ...           False   \n",
       "\n",
       "   SaleType_New  SaleType_Oth  SaleType_WD  SaleCondition_Abnorml  \\\n",
       "0         False         False         True                  False   \n",
       "1         False         False         True                  False   \n",
       "2         False         False         True                  False   \n",
       "\n",
       "   SaleCondition_AdjLand  SaleCondition_Alloca  SaleCondition_Family  \\\n",
       "0                  False                 False                 False   \n",
       "1                  False                 False                 False   \n",
       "2                  False                 False                 False   \n",
       "\n",
       "   SaleCondition_Normal  SaleCondition_Partial  \n",
       "0                  True                  False  \n",
       "1                  True                  False  \n",
       "2                  True                  False  \n",
       "\n",
       "[3 rows x 270 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.get_dummies(test_data)\n",
    "test_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    208500\n",
       "1    181500\n",
       "2    223500\n",
       "Name: SalePrice, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['SalePrice'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align train and test data to have the same columns\n",
    "train_data, test_data = train_data.align(test_data, join='outer', axis=1)\n",
    "\n",
    "# Fill NaN values in train_data and test_data\n",
    "train_data = train_data.fillna(0)\n",
    "test_data = test_data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.0\n",
       "1    0.0\n",
       "2    0.0\n",
       "Name: SalePrice, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['SalePrice'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    208500\n",
       "1    181500\n",
       "2    223500\n",
       "Name: SalePrice, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['SalePrice'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>Alley_Grvl</th>\n",
       "      <th>Alley_Pave</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>BldgType_1Fam</th>\n",
       "      <th>BldgType_2fmCon</th>\n",
       "      <th>BldgType_Duplex</th>\n",
       "      <th>BldgType_Twnhs</th>\n",
       "      <th>...</th>\n",
       "      <th>Street_Grvl</th>\n",
       "      <th>Street_Pave</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>Utilities_AllPub</th>\n",
       "      <th>Utilities_NoSeWa</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>YrSold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "      <td>856</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>1262</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>298</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>920</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>961</td>\n",
       "      <td>756</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "      <td>756</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1145</td>\n",
       "      <td>1053</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "      <td>1145</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>192</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>953</td>\n",
       "      <td>694</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "      <td>953</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1999</td>\n",
       "      <td>2000</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2073</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "      <td>1542</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>349</td>\n",
       "      <td>1978</td>\n",
       "      <td>1988</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>1188</td>\n",
       "      <td>1152</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "      <td>1152</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1941</td>\n",
       "      <td>2006</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>1078</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>1078</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>366</td>\n",
       "      <td>1950</td>\n",
       "      <td>1996</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>1256</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>1256</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>736</td>\n",
       "      <td>1965</td>\n",
       "      <td>1965</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows Ã— 288 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      1stFlrSF  2ndFlrSF  3SsnPorch  Alley_Grvl  Alley_Pave  BedroomAbvGr  \\\n",
       "0          856       854          0       False       False             3   \n",
       "1         1262         0          0       False       False             3   \n",
       "2          920       866          0       False       False             3   \n",
       "3          961       756          0       False       False             3   \n",
       "4         1145      1053          0       False       False             4   \n",
       "...        ...       ...        ...         ...         ...           ...   \n",
       "1455       953       694          0       False       False             3   \n",
       "1456      2073         0          0       False       False             3   \n",
       "1457      1188      1152          0       False       False             4   \n",
       "1458      1078         0          0       False       False             2   \n",
       "1459      1256         0          0       False       False             3   \n",
       "\n",
       "      BldgType_1Fam  BldgType_2fmCon  BldgType_Duplex  BldgType_Twnhs  ...  \\\n",
       "0              True            False            False           False  ...   \n",
       "1              True            False            False           False  ...   \n",
       "2              True            False            False           False  ...   \n",
       "3              True            False            False           False  ...   \n",
       "4              True            False            False           False  ...   \n",
       "...             ...              ...              ...             ...  ...   \n",
       "1455           True            False            False           False  ...   \n",
       "1456           True            False            False           False  ...   \n",
       "1457           True            False            False           False  ...   \n",
       "1458           True            False            False           False  ...   \n",
       "1459           True            False            False           False  ...   \n",
       "\n",
       "      Street_Grvl  Street_Pave  TotRmsAbvGrd  TotalBsmtSF  Utilities_AllPub  \\\n",
       "0           False         True             8          856              True   \n",
       "1           False         True             6         1262              True   \n",
       "2           False         True             6          920              True   \n",
       "3           False         True             7          756              True   \n",
       "4           False         True             9         1145              True   \n",
       "...           ...          ...           ...          ...               ...   \n",
       "1455        False         True             7          953              True   \n",
       "1456        False         True             7         1542              True   \n",
       "1457        False         True             9         1152              True   \n",
       "1458        False         True             5         1078              True   \n",
       "1459        False         True             6         1256              True   \n",
       "\n",
       "      Utilities_NoSeWa  WoodDeckSF  YearBuilt  YearRemodAdd  YrSold  \n",
       "0                False           0       2003          2003    2008  \n",
       "1                False         298       1976          1976    2007  \n",
       "2                False           0       2001          2002    2008  \n",
       "3                False           0       1915          1970    2006  \n",
       "4                False         192       2000          2000    2008  \n",
       "...                ...         ...        ...           ...     ...  \n",
       "1455             False           0       1999          2000    2007  \n",
       "1456             False         349       1978          1988    2010  \n",
       "1457             False           0       1941          2006    2010  \n",
       "1458             False         366       1950          1996    2010  \n",
       "1459             False         736       1965          1965    2008  \n",
       "\n",
       "[1460 rows x 288 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train_data.drop(['SalePrice'], axis=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[208500],\n",
       "       [181500],\n",
       "       [223500],\n",
       "       ...,\n",
       "       [266500],\n",
       "       [142125],\n",
       "       [147500]])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = train_data['SalePrice'].values\n",
    "y = y.reshape(-1, 1)\n",
    "print(y.shape)\n",
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 288)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 288)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X = np.reshape(X, (1460, 288))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.34727322],\n",
       "       [ 0.00728832],\n",
       "       [ 0.53615372],\n",
       "       ...,\n",
       "       [ 1.07761115],\n",
       "       [-0.48852299],\n",
       "       [-0.42084081]])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_scaler = StandardScaler()\n",
    "y_scaled = y_scaler.fit_transform(y)\n",
    "y_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(292, 1)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y_scaled, test_size=0.2, random_state=42\n",
    ")\n",
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4523],\n",
       "        [-0.0368],\n",
       "        [-1.2078],\n",
       "        ...,\n",
       "        [-0.8301],\n",
       "        [ 0.1137],\n",
       "        [-0.0872]])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.tensor(y_train, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class HousePriceDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.x_train = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y_train = torch.tensor(y, dtype=torch.float32)\n",
    "    def __len__(self):\n",
    "        return len(self.x_train)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_train[idx], self.y_train[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2449, -0.7952, -0.1163,  ..., -0.2738,  0.8787, -1.3677],\n",
       "        [ 0.8730,  1.7760, -0.1163,  ...,  0.7529,  0.4910,  1.6452],\n",
       "        [-0.3484, -0.7952, -0.1163,  ..., -1.4662, -1.6894,  1.6452],\n",
       "        ...,\n",
       "        [-1.1246,  0.8731, -0.1163,  ...,  1.1172,  0.9756,  0.1388],\n",
       "        [-1.0548,  0.9350, -0.1163,  ..., -1.0356, -1.6894,  0.8920],\n",
       "        [-0.7727, -0.7952, -0.1163,  ..., -0.0420, -0.7203,  0.8920]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(X_val, dtype=torch.float32)\n",
    "# torch.tensor(y_val, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "train_dataset = HousePriceDataset(X_train, y_train)\n",
    "val_dataset = HousePriceDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pd.DataFrame([1,2,3]).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class HousePriceModel(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size:int) -> None:\n",
    "        super(HousePriceModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 8)\n",
    "        self.fc5 = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x) # output without activation\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 288)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = x_train.shape[1]\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HousePriceModel(input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 0.943828102704641, Val Loss: 1.1620299845933915\n",
      "Epoch 2/50, Train Loss: 0.9392541614738671, Val Loss: 1.1617233276367187\n",
      "Epoch 3/50, Train Loss: 0.9436094833386911, Val Loss: 1.1617801427841186\n",
      "Epoch 4/50, Train Loss: 0.9427646535473901, Val Loss: 1.1619473218917846\n",
      "Epoch 5/50, Train Loss: 0.943672089963346, Val Loss: 1.162055316567421\n",
      "Epoch 6/50, Train Loss: 0.9380755472827602, Val Loss: 1.1622134476900101\n",
      "Epoch 7/50, Train Loss: 0.948219060092359, Val Loss: 1.1621520459651946\n",
      "Epoch 8/50, Train Loss: 0.9462717481561609, Val Loss: 1.161905938386917\n",
      "Epoch 9/50, Train Loss: 0.9431886817957904, Val Loss: 1.1620368182659149\n",
      "Epoch 10/50, Train Loss: 0.9429695557903599, Val Loss: 1.161905011534691\n",
      "Epoch 11/50, Train Loss: 0.9398124475736875, Val Loss: 1.1624688446521758\n",
      "Epoch 12/50, Train Loss: 0.9390965055774998, Val Loss: 1.1622315883636474\n",
      "Epoch 13/50, Train Loss: 0.9810381725027755, Val Loss: 1.161982637643814\n",
      "Epoch 14/50, Train Loss: 0.9500437539976996, Val Loss: 1.1627986311912537\n",
      "Epoch 15/50, Train Loss: 0.9550065486817747, Val Loss: 1.1621328830718993\n",
      "Epoch 16/50, Train Loss: 0.9456900793152887, Val Loss: 1.161909630894661\n",
      "Epoch 17/50, Train Loss: 0.9465373947813704, Val Loss: 1.1623905718326568\n",
      "Epoch 18/50, Train Loss: 0.9388993060266649, Val Loss: 1.162323033809662\n",
      "Epoch 19/50, Train Loss: 0.9480773355509784, Val Loss: 1.1620206713676453\n",
      "Epoch 20/50, Train Loss: 0.9422673832725834, Val Loss: 1.1628336548805236\n",
      "Epoch 21/50, Train Loss: 0.9438632283661816, Val Loss: 1.1624286323785782\n",
      "Epoch 22/50, Train Loss: 0.9530692132743629, Val Loss: 1.1621991276741028\n",
      "Epoch 23/50, Train Loss: 0.9439664378359511, Val Loss: 1.1630633533000947\n",
      "Epoch 24/50, Train Loss: 0.9484235330207927, Val Loss: 1.1622119426727295\n",
      "Epoch 25/50, Train Loss: 0.9407707356117867, Val Loss: 1.1627405405044555\n",
      "Epoch 26/50, Train Loss: 0.9409565031528473, Val Loss: 1.1629747152328491\n",
      "Epoch 27/50, Train Loss: 0.9481461660282032, Val Loss: 1.1627574026584626\n",
      "Epoch 28/50, Train Loss: 0.9603212209972175, Val Loss: 1.1621561706066132\n",
      "Epoch 29/50, Train Loss: 0.9454229860692411, Val Loss: 1.1625043034553528\n",
      "Epoch 30/50, Train Loss: 0.9831588356881529, Val Loss: 1.1623475909233094\n",
      "Epoch 31/50, Train Loss: 0.9484098086485991, Val Loss: 1.1623897850513458\n",
      "Epoch 32/50, Train Loss: 0.9352716876042856, Val Loss: 1.1623873353004455\n",
      "Epoch 33/50, Train Loss: 0.9402434117085224, Val Loss: 1.1624452829360963\n",
      "Epoch 34/50, Train Loss: 0.9470352466041977, Val Loss: 1.1625855267047882\n",
      "Epoch 35/50, Train Loss: 0.9521134930687982, Val Loss: 1.1624647080898285\n",
      "Epoch 36/50, Train Loss: 0.9421798683501579, Val Loss: 1.1624431133270263\n",
      "Epoch 37/50, Train Loss: 0.9457788362696364, Val Loss: 1.1624506652355193\n",
      "Epoch 38/50, Train Loss: 0.94605354360632, Val Loss: 1.162259143590927\n",
      "Epoch 39/50, Train Loss: 0.9366822129971272, Val Loss: 1.1628896355628968\n",
      "Epoch 40/50, Train Loss: 0.943397082187034, Val Loss: 1.1620651721954345\n",
      "Epoch 41/50, Train Loss: 0.938513541543806, Val Loss: 1.162136897444725\n",
      "Epoch 42/50, Train Loss: 0.9591827239539172, Val Loss: 1.1625235676765442\n",
      "Epoch 43/50, Train Loss: 0.9485918283462524, Val Loss: 1.1624030709266662\n",
      "Epoch 44/50, Train Loss: 0.9406933357586732, Val Loss: 1.16269348859787\n",
      "Epoch 45/50, Train Loss: 0.9452625851373415, Val Loss: 1.1622599184513092\n",
      "Epoch 46/50, Train Loss: 0.9438926456747828, Val Loss: 1.1623834669589996\n",
      "Epoch 47/50, Train Loss: 0.9401227356614293, Val Loss: 1.1621151328086854\n",
      "Epoch 48/50, Train Loss: 0.9538957049717774, Val Loss: 1.162542599439621\n",
      "Epoch 49/50, Train Loss: 0.9408385544209867, Val Loss: 1.1621855646371841\n",
      "Epoch 50/50, Train Loss: 0.9361607545130962, Val Loss: 1.162890076637268\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader)}, Val Loss: {val_loss/len(val_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6546, -0.7753, -0.0888,  ..., -0.3409, -1.0729,  1.7139],\n",
       "        [ 0.4333, -0.7753, -0.0888,  ..., -0.4397, -1.2149,  1.7139],\n",
       "        [-0.5742,  0.8919, -0.0888,  ...,  0.8441,  0.6787,  1.7139],\n",
       "        ...,\n",
       "        [ 0.1695, -0.7753, -0.0888,  ..., -0.3739,  0.5841, -1.3600],\n",
       "        [-0.4686, -0.7753, -0.0888,  ...,  0.6795,  0.3947, -1.3600],\n",
       "        [-0.4033,  1.6126, -0.0888,  ...,  0.7124,  0.4894, -1.3600]])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = scaler.fit_transform(test_data.drop(['SalePrice'],axis=1).values)\n",
    "test_x = torch.tensor(test_x, dtype=torch.float32)\n",
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1459, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([179838.42, 181554.83, 182119.02, ..., 181724.  , 179769.03,\n",
       "       181663.03], dtype=float32)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    test_y = model(test_x)\n",
    "# tensor to numpy array\n",
    "sale_price = test_y.detach().numpy()\n",
    "sale_price = y_scaler.inverse_transform(sale_price)\n",
    "print(sale_price.shape)\n",
    "sale_price = sale_price.squeeze()\n",
    "sale_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1461\n",
       "1       1462\n",
       "2       1463\n",
       "3       1464\n",
       "4       1465\n",
       "        ... \n",
       "1454    2915\n",
       "1455    2916\n",
       "1456    2917\n",
       "1457    2918\n",
       "1458    2919\n",
       "Name: Id, Length: 1459, dtype: int64"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id = test_data['Id']\n",
    "id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame({\"Id\": id,\"SalePrice\":sale_price})\n",
    "result.to_csv('house_price_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use community solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>2003</td>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>1976</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>2001</td>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street LotShape  YearBuilt  \\\n",
       "0   1          60       RL         65.0     8450   Pave      Reg       2003   \n",
       "1   2          20       RL         80.0     9600   Pave      Reg       1976   \n",
       "2   3          60       RL         68.0    11250   Pave      IR1       2001   \n",
       "\n",
       "   1stFlrSF  2ndFlrSF  SalePrice  \n",
       "0       856       854     208500  \n",
       "1      1262         0     181500  \n",
       "2       920       866     223500  "
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('house_price_data/train.csv', usecols=[\"Id\", \"SalePrice\", \"MSSubClass\", \"MSZoning\", \"LotFrontage\", \"LotArea\",\n",
    "                                         \"Street\", \"YearBuilt\", \"LotShape\", \"1stFlrSF\", \"2ndFlrSF\"]).dropna()\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1201 entries, 0 to 1459\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   MSSubClass   1201 non-null   int64  \n",
      " 1   MSZoning     1201 non-null   object \n",
      " 2   LotFrontage  1201 non-null   float64\n",
      " 3   LotArea      1201 non-null   int64  \n",
      " 4   Street       1201 non-null   object \n",
      " 5   LotShape     1201 non-null   object \n",
      " 6   YearBuilt    1201 non-null   int64  \n",
      " 7   1stFlrSF     1201 non-null   int64  \n",
      " 8   2ndFlrSF     1201 non-null   int64  \n",
      " 9   SalePrice    1201 non-null   int64  \n",
      "dtypes: float64(1), int64(6), object(3)\n",
      "memory usage: 103.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column name MSSubClass and unique values are 15\n",
      "Column name MSZoning and unique values are 5\n",
      "Column name LotFrontage and unique values are 110\n",
      "Column name LotArea and unique values are 869\n",
      "Column name Street and unique values are 2\n",
      "Column name LotShape and unique values are 4\n",
      "Column name YearBuilt and unique values are 112\n",
      "Column name 1stFlrSF and unique values are 678\n",
      "Column name 2ndFlrSF and unique values are 368\n",
      "Column name SalePrice and unique values are 597\n"
     ]
    }
   ],
   "source": [
    "for i in df.columns:\n",
    "    print(f'Column name {i} and unique values are {len(df[i].unique())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2024"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "datetime.datetime.now().year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>Total Years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>2003</td>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>208500</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>1976</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>181500</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>2001</td>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>223500</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass MSZoning  LotFrontage  LotArea Street LotShape  YearBuilt  \\\n",
       "0          60       RL         65.0     8450   Pave      Reg       2003   \n",
       "1          20       RL         80.0     9600   Pave      Reg       1976   \n",
       "2          60       RL         68.0    11250   Pave      IR1       2001   \n",
       "\n",
       "   1stFlrSF  2ndFlrSF  SalePrice  Total Years  \n",
       "0       856       854     208500           21  \n",
       "1      1262         0     181500           48  \n",
       "2       920       866     223500           23  "
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Total Years'] = datetime.datetime.now().year - df['YearBuilt']\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"YearBuilt\"],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>Total Years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>208500</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>181500</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>223500</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass MSZoning  LotFrontage  LotArea Street LotShape  1stFlrSF  \\\n",
       "0          60       RL         65.0     8450   Pave      Reg       856   \n",
       "1          20       RL         80.0     9600   Pave      Reg      1262   \n",
       "2          60       RL         68.0    11250   Pave      IR1       920   \n",
       "\n",
       "   2ndFlrSF  SalePrice  Total Years  \n",
       "0       854     208500           21  \n",
       "1         0     181500           48  \n",
       "2       866     223500           23  "
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features=[\"MSSubClass\", \"MSZoning\", \"Street\", \"LotShape\"]\n",
    "out_features = [\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 5, ..., 6, 0, 0])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lbl_encoders = {}\n",
    "lbl_encoders['MSSubClass'] = LabelEncoder()\n",
    "lbl_encoders['MSSubClass'].fit_transform(df['MSSubClass'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MSSubClass': LabelEncoder()}"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>Total Years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>279</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>123</td>\n",
       "      <td>212</td>\n",
       "      <td>208500</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>51</td>\n",
       "      <td>422</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>401</td>\n",
       "      <td>0</td>\n",
       "      <td>181500</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>595</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>166</td>\n",
       "      <td>219</td>\n",
       "      <td>223500</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>414</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>195</td>\n",
       "      <td>163</td>\n",
       "      <td>140000</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>55</td>\n",
       "      <td>768</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>326</td>\n",
       "      <td>287</td>\n",
       "      <td>250000</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>225</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>188</td>\n",
       "      <td>132</td>\n",
       "      <td>175000</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>56</td>\n",
       "      <td>722</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>656</td>\n",
       "      <td>0</td>\n",
       "      <td>210000</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>37</td>\n",
       "      <td>355</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>355</td>\n",
       "      <td>316</td>\n",
       "      <td>266500</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>428</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>279</td>\n",
       "      <td>0</td>\n",
       "      <td>142125</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>46</td>\n",
       "      <td>451</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>397</td>\n",
       "      <td>0</td>\n",
       "      <td>147500</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1201 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MSSubClass  MSZoning  LotFrontage  LotArea  Street  LotShape  1stFlrSF  \\\n",
       "0              5         3           36      279       1         3       123   \n",
       "1              0         3           51      422       1         3       401   \n",
       "2              5         3           39      595       1         0       166   \n",
       "3              6         3           31      414       1         0       195   \n",
       "4              5         3           55      768       1         0       326   \n",
       "...          ...       ...          ...      ...     ...       ...       ...   \n",
       "1455           5         3           33      225       1         3       188   \n",
       "1456           0         3           56      722       1         3       656   \n",
       "1457           6         3           37      355       1         3       355   \n",
       "1458           0         3           39      428       1         3       279   \n",
       "1459           0         3           46      451       1         3       397   \n",
       "\n",
       "      2ndFlrSF  SalePrice  Total Years  \n",
       "0          212     208500           21  \n",
       "1            0     181500           48  \n",
       "2          219     223500           23  \n",
       "3          163     140000          109  \n",
       "4          287     250000           24  \n",
       "...        ...        ...          ...  \n",
       "1455       132     175000           25  \n",
       "1456         0     210000           46  \n",
       "1457       316     266500           83  \n",
       "1458         0     142125           74  \n",
       "1459         0     147500           59  \n",
       "\n",
       "[1201 rows x 10 columns]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cat_features=[\"MSSubClass\", \"MSZoning\", \"Street\", \"LotShape\"]\n",
    "for f in cat_features:\n",
    "    lbl_encoders[f] = LabelEncoder()\n",
    "    df[f] = lbl_encoders[f].fit_transform(df[f])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1201, 4)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cate_fe_array = np.stack([df['MSSubClass'],df['MSZoning'],df['Street'],df['LotShape']],axis=1)\n",
    "cate_fe_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 3, 1, 3],\n",
       "        [0, 3, 1, 3],\n",
       "        [5, 3, 1, 0],\n",
       "        ...,\n",
       "        [6, 3, 1, 3],\n",
       "        [0, 3, 1, 3],\n",
       "        [0, 3, 1, 3]], dtype=torch.int32)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cate_fe_tensor = torch.tensor(cate_fe_array, dtype=torch.int32)\n",
    "cate_fe_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LotFrontage', 'LotArea', '1stFlrSF', '2ndFlrSF', 'Total Years']"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_features = ['LotFrontage','LotArea','1stFlrSF','2ndFlrSF','Total Years']\n",
    "cont_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 36., 279., 123., 212.,  21.],\n",
       "        [ 51., 422., 401.,   0.,  48.],\n",
       "        [ 39., 595., 166., 219.,  23.],\n",
       "        ...,\n",
       "        [ 37., 355., 355., 316.,  83.],\n",
       "        [ 39., 428., 279.,   0.,  74.],\n",
       "        [ 46., 451., 397.,   0.,  59.]])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_fe_tensor = torch.tensor(np.stack([df[fe] for fe in cont_features], axis=1), dtype=torch.float32)\n",
    "cont_fe_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_fe_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1201, 1])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor(df['SalePrice'].values, dtype=torch.float32).reshape(-1, 1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['MSSubClass'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 5, 2, 4]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_dims = [len(df[f].unique()) for f in cat_features]\n",
    "cat_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(15, 8), (5, 3), (2, 1), (4, 2)]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = [(x, min(50, (x+1)//2)) for x in cat_dims]\n",
    "embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Embedding(15, 8)\n",
       "  (1): Embedding(5, 3)\n",
       "  (2): Embedding(2, 1)\n",
       "  (3): Embedding(4, 2)\n",
       ")"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "embed_repr = nn.ModuleList(\n",
    "    [nn.Embedding(in_dim, out_dim) for in_dim, out_dim in embedding_dim]\n",
    ")\n",
    "embed_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, n_cont, out_sz, layers, p=0.5):\n",
    "        super().__init__()\n",
    "        self.embeds = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in embedding_dim])\n",
    "        self.emb_drop = nn.Dropout(p)\n",
    "        self.bn_cont = nn.BatchNorm1d(n_cont)\n",
    "\n",
    "        layerList = []\n",
    "        n_emb = sum((out for inp, out in embedding_dim))\n",
    "        n_in = n_emb + n_cont\n",
    "\n",
    "        for i in layers:\n",
    "            layerList.append(nn.Linear(n_in, i))\n",
    "            layerList.append(nn.ReLU(inplace=True))\n",
    "            layerList.append(nn.BatchNorm1d(i))\n",
    "            layerList.append(nn.Dropout(p))\n",
    "            n_in = i\n",
    "        layerList.append(nn.Linear(n_in, out_sz))\n",
    "\n",
    "        self.layers = nn.Sequential(*layerList)\n",
    "\n",
    "    def forward(self, x, x_cont):\n",
    "        embeddings = []\n",
    "        for i, e in enumerate(self.embeds):\n",
    "            embeddings.append(e(x[:, i]))\n",
    "        x = torch.cat(embeddings, 1)\n",
    "        x = self.emb_drop(x)\n",
    "\n",
    "        x_cont = self.bn_cont(x_cont)\n",
    "        x = torch.cat([x, x_cont], 1)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cont_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedForwardModel(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Embedding(15, 8)\n",
       "    (1): Embedding(5, 3)\n",
       "    (2): Embedding(2, 1)\n",
       "    (3): Embedding(4, 2)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0.4, inplace=False)\n",
       "  (bn_cont): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=19, out_features=100, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.4, inplace=False)\n",
       "    (4): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.4, inplace=False)\n",
       "    (8): Linear(in_features=50, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(100)\n",
    "model = FeedForwardModel(embedding_dim, len(cont_features), 1, [100, 50], p=0.4)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1201, 10)"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1201, 5])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_fe_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1201, 4])"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cate_fe_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=1200\n",
    "test_size=int(batch_size*0.15)\n",
    "train_categorical=cate_fe_tensor[:batch_size-test_size]\n",
    "test_categorical=cate_fe_tensor[batch_size-test_size:batch_size]\n",
    "train_cont=cont_fe_tensor[:batch_size-test_size]\n",
    "test_cont=cont_fe_tensor[batch_size-test_size:batch_size]\n",
    "y_train=y[:batch_size-test_size]\n",
    "y_test=y[batch_size-test_size:batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1020, 180, 1020, 180, 1020, 180)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_categorical),len(test_categorical),len(train_cont),len(test_cont),len(y_train),len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss 200496.765625 val loss 190993.390625\n",
      "Epoch 10 train loss 200495.28125 val loss 190991.796875\n",
      "Epoch 20 train loss 200493.109375 val loss 190989.5\n",
      "Epoch 30 train loss 200489.046875 val loss 190985.171875\n",
      "Epoch 40 train loss 200481.46875 val loss 190977.140625\n",
      "Epoch 50 train loss 200467.96875 val loss 190962.859375\n",
      "Epoch 60 train loss 200444.953125 val loss 190938.609375\n",
      "Epoch 70 train loss 200407.484375 val loss 190899.3125\n",
      "Epoch 80 train loss 200349.140625 val loss 190838.328125\n",
      "Epoch 90 train loss 200261.859375 val loss 190747.265625\n",
      "Epoch 100 train loss 200135.671875 val loss 190615.96875\n",
      "Epoch 110 train loss 199958.34375 val loss 190431.84375\n",
      "Epoch 120 train loss 199714.984375 val loss 190179.546875\n",
      "Epoch 130 train loss 199387.65625 val loss 189840.8125\n",
      "Epoch 140 train loss 198955.25 val loss 189393.96875\n",
      "Epoch 150 train loss 198393.046875 val loss 188813.78125\n",
      "Epoch 160 train loss 197672.59375 val loss 188071.03125\n",
      "Epoch 170 train loss 196760.609375 val loss 187131.75\n",
      "Epoch 180 train loss 195618.046875 val loss 185956.046875\n",
      "Epoch 190 train loss 194201.171875 val loss 184499.265625\n",
      "Epoch 200 train loss 192462.015625 val loss 182712.53125\n",
      "Epoch 210 train loss 190346.625 val loss 180540.78125\n",
      "Epoch 220 train loss 187795.1875 val loss 177923.03125\n",
      "Epoch 230 train loss 184742.171875 val loss 174792.5625\n",
      "Epoch 240 train loss 181116.84375 val loss 171077.34375\n",
      "Epoch 250 train loss 176844.046875 val loss 166701.0625\n",
      "Epoch 260 train loss 171845.78125 val loss 161584.5625\n",
      "Epoch 270 train loss 166043.78125 val loss 155648.953125\n",
      "Epoch 280 train loss 159364.1875 val loss 148820.359375\n",
      "Epoch 290 train loss 151744.921875 val loss 141038.375\n",
      "Epoch 300 train loss 143148.5625 val loss 132270.265625\n",
      "Epoch 310 train loss 133583.96875 val loss 122535.8515625\n",
      "Epoch 320 train loss 123143.2734375 val loss 111950.4921875\n",
      "Epoch 330 train loss 112063.7421875 val loss 100798.9453125\n",
      "Epoch 340 train loss 100823.5625 val loss 89650.6328125\n",
      "Epoch 350 train loss 90252.0859375 val loss 79485.609375\n",
      "Epoch 360 train loss 81515.90625 val loss 71628.34375\n",
      "Epoch 370 train loss 75648.78125 val loss 67056.234375\n",
      "Epoch 380 train loss 72628.875 val loss 65311.54296875\n",
      "Epoch 390 train loss 71257.9453125 val loss 64767.9296875\n",
      "Epoch 400 train loss 70371.75 val loss 64234.26953125\n",
      "Epoch 410 train loss 69546.25 val loss 63486.9140625\n",
      "Epoch 420 train loss 68761.078125 val loss 62687.26953125\n",
      "Epoch 430 train loss 68035.234375 val loss 61952.20703125\n",
      "Epoch 440 train loss 67359.1640625 val loss 61303.8359375\n",
      "Epoch 450 train loss 66720.5 val loss 60725.40234375\n",
      "Epoch 460 train loss 66113.1796875 val loss 60194.69140625\n",
      "Epoch 470 train loss 65533.875 val loss 59694.5390625\n",
      "Epoch 480 train loss 64979.65234375 val loss 59215.09765625\n",
      "Epoch 490 train loss 64447.85546875 val loss 58752.64453125\n",
      "Epoch 500 train loss 63936.265625 val loss 58306.61328125\n",
      "Epoch 510 train loss 63443.0625 val loss 57877.125\n",
      "Epoch 520 train loss 62966.69140625 val loss 57463.76953125\n",
      "Epoch 530 train loss 62505.8515625 val loss 57065.50390625\n",
      "Epoch 540 train loss 62059.38671875 val loss 56681.01953125\n",
      "Epoch 550 train loss 61626.31640625 val loss 56309.0703125\n",
      "Epoch 560 train loss 61205.79296875 val loss 55948.6484375\n",
      "Epoch 570 train loss 60797.046875 val loss 55598.9453125\n",
      "Epoch 580 train loss 60399.43359375 val loss 55259.3359375\n",
      "Epoch 590 train loss 60012.3828125 val loss 54929.25\n",
      "Epoch 600 train loss 59635.390625 val loss 54608.19921875\n",
      "Epoch 610 train loss 59268.01171875 val loss 54295.7109375\n",
      "Epoch 620 train loss 58909.859375 val loss 53991.3828125\n",
      "Epoch 630 train loss 58560.58203125 val loss 53694.8515625\n",
      "Epoch 640 train loss 58219.89453125 val loss 53405.79296875\n",
      "Epoch 650 train loss 57887.51953125 val loss 53123.94140625\n",
      "Epoch 660 train loss 57563.23046875 val loss 52849.046875\n",
      "Epoch 670 train loss 57246.81640625 val loss 52580.88671875\n",
      "Epoch 680 train loss 56938.09765625 val loss 52319.28125\n",
      "Epoch 690 train loss 56636.91015625 val loss 52064.06640625\n",
      "Epoch 700 train loss 56343.11328125 val loss 51815.09375\n",
      "Epoch 710 train loss 56056.578125 val loss 51572.23828125\n",
      "Epoch 720 train loss 55777.19921875 val loss 51335.390625\n",
      "Epoch 730 train loss 55504.859375 val loss 51104.44921875\n",
      "Epoch 740 train loss 55239.47265625 val loss 50879.33203125\n",
      "Epoch 750 train loss 54980.9609375 val loss 50659.96484375\n",
      "Epoch 760 train loss 54729.2265625 val loss 50446.265625\n",
      "Epoch 770 train loss 54484.203125 val loss 50238.1875\n",
      "Epoch 780 train loss 54245.8203125 val loss 50035.67578125\n",
      "Epoch 790 train loss 54014.0 val loss 49838.66796875\n",
      "Epoch 800 train loss 53788.671875 val loss 49647.11328125\n",
      "Epoch 810 train loss 53569.76953125 val loss 49460.97265625\n",
      "Epoch 820 train loss 53357.21484375 val loss 49280.19140625\n",
      "Epoch 830 train loss 53150.9296875 val loss 49104.71875\n",
      "Epoch 840 train loss 52950.84765625 val loss 48934.50390625\n",
      "Epoch 850 train loss 52756.875 val loss 48769.4921875\n",
      "Epoch 860 train loss 52568.94140625 val loss 48609.64453125\n",
      "Epoch 870 train loss 52386.9453125 val loss 48454.88671875\n",
      "Epoch 880 train loss 52210.79296875 val loss 48305.1640625\n",
      "Epoch 890 train loss 52040.40234375 val loss 48160.41015625\n",
      "Epoch 900 train loss 51875.66796875 val loss 48020.5625\n",
      "Epoch 910 train loss 51716.47265625 val loss 47885.5390625\n",
      "Epoch 920 train loss 51562.7265625 val loss 47755.26171875\n",
      "Epoch 930 train loss 51414.296875 val loss 47629.66015625\n",
      "Epoch 940 train loss 51271.078125 val loss 47508.640625\n",
      "Epoch 950 train loss 51132.9453125 val loss 47392.12890625\n",
      "Epoch 960 train loss 50999.7734375 val loss 47280.015625\n",
      "Epoch 970 train loss 50871.44140625 val loss 47172.21875\n",
      "Epoch 980 train loss 50747.8203125 val loss 47068.6328125\n",
      "Epoch 990 train loss 50628.76953125 val loss 46969.15234375\n",
      "Epoch 1000 train loss 50514.171875 val loss 46873.6875\n",
      "Epoch 1010 train loss 50403.875 val loss 46782.125\n",
      "Epoch 1020 train loss 50297.75390625 val loss 46694.34765625\n",
      "Epoch 1030 train loss 50195.671875 val loss 46610.265625\n",
      "Epoch 1040 train loss 50097.48828125 val loss 46529.76171875\n",
      "Epoch 1050 train loss 50003.07421875 val loss 46452.71875\n",
      "Epoch 1060 train loss 49912.2890625 val loss 46379.02734375\n",
      "Epoch 1070 train loss 49825.00390625 val loss 46308.5703125\n",
      "Epoch 1080 train loss 49741.078125 val loss 46241.2421875\n",
      "Epoch 1090 train loss 49660.390625 val loss 46176.93359375\n",
      "Epoch 1100 train loss 49582.8046875 val loss 46115.53125\n",
      "Epoch 1110 train loss 49508.1875 val loss 46056.9140625\n",
      "Epoch 1120 train loss 49436.41015625 val loss 46000.9609375\n",
      "Epoch 1130 train loss 49367.359375 val loss 45947.58984375\n",
      "Epoch 1140 train loss 49300.9140625 val loss 45896.68359375\n",
      "Epoch 1150 train loss 49236.953125 val loss 45848.1328125\n",
      "Epoch 1160 train loss 49175.359375 val loss 45801.828125\n",
      "Epoch 1170 train loss 49116.02734375 val loss 45757.66796875\n",
      "Epoch 1180 train loss 49058.84765625 val loss 45715.5546875\n",
      "Epoch 1190 train loss 49003.71484375 val loss 45675.390625\n",
      "Epoch 1200 train loss 48950.5234375 val loss 45637.0703125\n",
      "Epoch 1210 train loss 48899.1796875 val loss 45600.5\n",
      "Epoch 1220 train loss 48849.57421875 val loss 45565.5859375\n",
      "Epoch 1230 train loss 48801.625 val loss 45532.2421875\n",
      "Epoch 1240 train loss 48755.25 val loss 45500.375\n",
      "Epoch 1250 train loss 48710.35546875 val loss 45469.90625\n",
      "Epoch 1260 train loss 48666.8515625 val loss 45440.74609375\n",
      "Epoch 1270 train loss 48624.67578125 val loss 45412.82421875\n",
      "Epoch 1280 train loss 48583.74609375 val loss 45386.046875\n",
      "Epoch 1290 train loss 48543.98046875 val loss 45360.359375\n",
      "Epoch 1300 train loss 48505.328125 val loss 45335.6796875\n",
      "Epoch 1310 train loss 48467.71484375 val loss 45311.9453125\n",
      "Epoch 1320 train loss 48431.078125 val loss 45289.0859375\n",
      "Epoch 1330 train loss 48395.359375 val loss 45267.04296875\n",
      "Epoch 1340 train loss 48360.50390625 val loss 45245.75390625\n",
      "Epoch 1350 train loss 48326.45703125 val loss 45225.1640625\n",
      "Epoch 1360 train loss 48293.16796875 val loss 45205.21484375\n",
      "Epoch 1370 train loss 48260.58984375 val loss 45185.859375\n",
      "Epoch 1380 train loss 48228.68359375 val loss 45167.04296875\n",
      "Epoch 1390 train loss 48197.3984375 val loss 45148.7265625\n",
      "Epoch 1400 train loss 48166.69921875 val loss 45130.87109375\n",
      "Epoch 1410 train loss 48136.54296875 val loss 45113.41796875\n",
      "Epoch 1420 train loss 48106.88671875 val loss 45096.31640625\n",
      "Epoch 1430 train loss 48077.703125 val loss 45079.56640625\n",
      "Epoch 1440 train loss 48048.9609375 val loss 45063.13671875\n",
      "Epoch 1450 train loss 48020.640625 val loss 45047.0\n",
      "Epoch 1460 train loss 47992.7109375 val loss 45031.1015625\n",
      "Epoch 1470 train loss 47965.140625 val loss 45015.421875\n",
      "Epoch 1480 train loss 47937.9140625 val loss 44999.93359375\n",
      "Epoch 1490 train loss 47911.00390625 val loss 44984.6171875\n",
      "Epoch 1500 train loss 47884.390625 val loss 44969.44140625\n",
      "Epoch 1510 train loss 47858.046875 val loss 44954.359375\n",
      "Epoch 1520 train loss 47831.94921875 val loss 44939.41015625\n",
      "Epoch 1530 train loss 47806.0859375 val loss 44924.56640625\n",
      "Epoch 1540 train loss 47780.4375 val loss 44909.78125\n",
      "Epoch 1550 train loss 47754.984375 val loss 44895.03515625\n",
      "Epoch 1560 train loss 47729.72265625 val loss 44880.3671875\n",
      "Epoch 1570 train loss 47704.64453125 val loss 44865.73046875\n",
      "Epoch 1580 train loss 47679.73046875 val loss 44851.09375\n",
      "Epoch 1590 train loss 47654.9609375 val loss 44836.46484375\n",
      "Epoch 1600 train loss 47630.3359375 val loss 44821.86328125\n",
      "Epoch 1610 train loss 47605.83984375 val loss 44807.21484375\n",
      "Epoch 1620 train loss 47581.45703125 val loss 44792.48046875\n",
      "Epoch 1630 train loss 47557.1796875 val loss 44777.72265625\n",
      "Epoch 1640 train loss 47532.99609375 val loss 44762.94140625\n",
      "Epoch 1650 train loss 47508.9140625 val loss 44748.13671875\n",
      "Epoch 1660 train loss 47484.91796875 val loss 44733.2890625\n",
      "Epoch 1670 train loss 47460.98828125 val loss 44718.3828125\n",
      "Epoch 1680 train loss 47437.1328125 val loss 44703.4140625\n",
      "Epoch 1690 train loss 47413.34375 val loss 44688.41796875\n",
      "Epoch 1700 train loss 47389.6171875 val loss 44673.38671875\n",
      "Epoch 1710 train loss 47365.95703125 val loss 44658.26171875\n",
      "Epoch 1720 train loss 47342.34375 val loss 44643.0546875\n",
      "Epoch 1730 train loss 47318.77734375 val loss 44627.671875\n",
      "Epoch 1740 train loss 47295.23828125 val loss 44612.19140625\n",
      "Epoch 1750 train loss 47271.72265625 val loss 44596.609375\n",
      "Epoch 1760 train loss 47248.21484375 val loss 44580.96484375\n",
      "Epoch 1770 train loss 47224.7109375 val loss 44565.2109375\n",
      "Epoch 1780 train loss 47201.21484375 val loss 44549.34765625\n",
      "Epoch 1790 train loss 47177.70703125 val loss 44533.30859375\n",
      "Epoch 1800 train loss 47154.18359375 val loss 44517.1796875\n",
      "Epoch 1810 train loss 47130.62890625 val loss 44500.9609375\n",
      "Epoch 1820 train loss 47107.046875 val loss 44484.671875\n",
      "Epoch 1830 train loss 47083.42578125 val loss 44468.15234375\n",
      "Epoch 1840 train loss 47059.80078125 val loss 44451.61328125\n",
      "Epoch 1850 train loss 47036.15625 val loss 44434.90234375\n",
      "Epoch 1860 train loss 47012.41796875 val loss 44417.98046875\n",
      "Epoch 1870 train loss 46988.62109375 val loss 44400.9375\n",
      "Epoch 1880 train loss 46964.71875 val loss 44383.703125\n",
      "Epoch 1890 train loss 46940.63671875 val loss 44366.05859375\n",
      "Epoch 1900 train loss 46916.4296875 val loss 44348.23046875\n",
      "Epoch 1910 train loss 46892.08984375 val loss 44330.12890625\n",
      "Epoch 1920 train loss 46867.515625 val loss 44311.4609375\n",
      "Epoch 1930 train loss 46842.484375 val loss 44292.26171875\n",
      "Epoch 1940 train loss 46817.17578125 val loss 44273.13671875\n",
      "Epoch 1950 train loss 46791.58984375 val loss 44253.4609375\n",
      "Epoch 1960 train loss 46765.75 val loss 44233.2890625\n",
      "Epoch 1970 train loss 46739.55078125 val loss 44212.71875\n",
      "Epoch 1980 train loss 46713.0390625 val loss 44192.0625\n",
      "Epoch 1990 train loss 46686.20703125 val loss 44171.2734375\n",
      "Epoch 2000 train loss 46659.23828125 val loss 44150.38671875\n",
      "Epoch 2010 train loss 46631.80859375 val loss 44129.4140625\n",
      "Epoch 2020 train loss 46604.14453125 val loss 44108.265625\n",
      "Epoch 2030 train loss 46576.28125 val loss 44087.0390625\n",
      "Epoch 2040 train loss 46548.546875 val loss 44065.78515625\n",
      "Epoch 2050 train loss 46520.953125 val loss 44044.578125\n",
      "Epoch 2060 train loss 46493.35546875 val loss 44022.7265625\n",
      "Epoch 2070 train loss 46465.66796875 val loss 44001.140625\n",
      "Epoch 2080 train loss 46438.109375 val loss 43979.7734375\n",
      "Epoch 2090 train loss 46410.7265625 val loss 43958.2734375\n",
      "Epoch 2100 train loss 46383.29296875 val loss 43936.26953125\n",
      "Epoch 2110 train loss 46355.93359375 val loss 43914.4140625\n",
      "Epoch 2120 train loss 46328.453125 val loss 43892.20703125\n",
      "Epoch 2130 train loss 46301.0703125 val loss 43870.7421875\n",
      "Epoch 2140 train loss 46273.94921875 val loss 43849.875\n",
      "Epoch 2150 train loss 46247.12890625 val loss 43829.35546875\n",
      "Epoch 2160 train loss 46220.4453125 val loss 43808.6015625\n",
      "Epoch 2170 train loss 46193.796875 val loss 43787.76171875\n",
      "Epoch 2180 train loss 46167.23046875 val loss 43767.6328125\n",
      "Epoch 2190 train loss 46140.6328125 val loss 43747.265625\n",
      "Epoch 2200 train loss 46113.9375 val loss 43726.7265625\n",
      "Epoch 2210 train loss 46087.15234375 val loss 43706.140625\n",
      "Epoch 2220 train loss 46060.29296875 val loss 43685.31640625\n",
      "Epoch 2230 train loss 46033.375 val loss 43664.45703125\n",
      "Epoch 2240 train loss 46006.390625 val loss 43643.69921875\n",
      "Epoch 2250 train loss 45979.328125 val loss 43622.9765625\n",
      "Epoch 2260 train loss 45952.1796875 val loss 43602.0390625\n",
      "Epoch 2270 train loss 45924.9453125 val loss 43580.8984375\n",
      "Epoch 2280 train loss 45897.63671875 val loss 43559.66015625\n",
      "Epoch 2290 train loss 45870.2265625 val loss 43538.63671875\n",
      "Epoch 2300 train loss 45842.71484375 val loss 43517.5\n",
      "Epoch 2310 train loss 45815.1328125 val loss 43496.296875\n",
      "Epoch 2320 train loss 45787.50390625 val loss 43475.01171875\n",
      "Epoch 2330 train loss 45759.8125 val loss 43453.9375\n",
      "Epoch 2340 train loss 45732.01953125 val loss 43432.39453125\n",
      "Epoch 2350 train loss 45704.10546875 val loss 43410.7265625\n",
      "Epoch 2360 train loss 45676.05859375 val loss 43388.9140625\n",
      "Epoch 2370 train loss 45647.87109375 val loss 43366.96484375\n",
      "Epoch 2380 train loss 45619.54296875 val loss 43345.0\n",
      "Epoch 2390 train loss 45591.08203125 val loss 43322.73046875\n",
      "Epoch 2400 train loss 45562.515625 val loss 43300.8671875\n",
      "Epoch 2410 train loss 45533.80078125 val loss 43278.9921875\n",
      "Epoch 2420 train loss 45504.9375 val loss 43257.0859375\n",
      "Epoch 2430 train loss 45475.93359375 val loss 43235.41796875\n",
      "Epoch 2440 train loss 45446.78515625 val loss 43213.23046875\n",
      "Epoch 2450 train loss 45417.4921875 val loss 43190.796875\n",
      "Epoch 2460 train loss 45388.0625 val loss 43169.03515625\n",
      "Epoch 2470 train loss 45358.515625 val loss 43146.90234375\n",
      "Epoch 2480 train loss 45328.8515625 val loss 43124.3125\n",
      "Epoch 2490 train loss 45299.078125 val loss 43101.09765625\n",
      "Epoch 2500 train loss 45269.140625 val loss 43078.33203125\n",
      "Epoch 2510 train loss 45239.05078125 val loss 43054.7890625\n",
      "Epoch 2520 train loss 45208.83203125 val loss 43031.66015625\n",
      "Epoch 2530 train loss 45178.4921875 val loss 43008.921875\n",
      "Epoch 2540 train loss 45148.03125 val loss 42986.30078125\n",
      "Epoch 2550 train loss 45117.40625 val loss 42963.546875\n",
      "Epoch 2560 train loss 45086.65625 val loss 42941.08203125\n",
      "Epoch 2570 train loss 45055.7734375 val loss 42918.98828125\n",
      "Epoch 2580 train loss 45024.734375 val loss 42897.0390625\n",
      "Epoch 2590 train loss 44993.50390625 val loss 42875.03125\n",
      "Epoch 2600 train loss 44962.0703125 val loss 42852.86328125\n",
      "Epoch 2610 train loss 44930.4453125 val loss 42830.6640625\n",
      "Epoch 2620 train loss 44898.65625 val loss 42808.3125\n",
      "Epoch 2630 train loss 44866.6953125 val loss 42786.36328125\n",
      "Epoch 2640 train loss 44834.55859375 val loss 42764.09375\n",
      "Epoch 2650 train loss 44802.3046875 val loss 42742.16796875\n",
      "Epoch 2660 train loss 44769.88671875 val loss 42719.8203125\n",
      "Epoch 2670 train loss 44737.29296875 val loss 42697.328125\n",
      "Epoch 2680 train loss 44704.52734375 val loss 42675.140625\n",
      "Epoch 2690 train loss 44671.58203125 val loss 42652.1796875\n",
      "Epoch 2700 train loss 44638.4375 val loss 42629.73828125\n",
      "Epoch 2710 train loss 44605.0859375 val loss 42607.28515625\n",
      "Epoch 2720 train loss 44571.52734375 val loss 42584.58203125\n",
      "Epoch 2730 train loss 44537.765625 val loss 42562.0703125\n",
      "Epoch 2740 train loss 44503.79296875 val loss 42539.765625\n",
      "Epoch 2750 train loss 44469.8671875 val loss 42518.94140625\n",
      "Epoch 2760 train loss 44435.76171875 val loss 42496.75\n",
      "Epoch 2770 train loss 44401.5078125 val loss 42474.04296875\n",
      "Epoch 2780 train loss 44367.05078125 val loss 42451.07421875\n",
      "Epoch 2790 train loss 44332.390625 val loss 42427.3515625\n",
      "Epoch 2800 train loss 44297.5234375 val loss 42404.20703125\n",
      "Epoch 2810 train loss 44262.3984375 val loss 42381.08984375\n",
      "Epoch 2820 train loss 44227.02734375 val loss 42358.34765625\n",
      "Epoch 2830 train loss 44191.41796875 val loss 42336.859375\n",
      "Epoch 2840 train loss 44155.53515625 val loss 42312.61328125\n",
      "Epoch 2850 train loss 44119.3828125 val loss 42291.5234375\n",
      "Epoch 2860 train loss 44082.9375 val loss 42267.77734375\n",
      "Epoch 2870 train loss 44046.21484375 val loss 42245.06640625\n",
      "Epoch 2880 train loss 44009.1484375 val loss 42221.83984375\n",
      "Epoch 2890 train loss 43971.8203125 val loss 42199.2421875\n",
      "Epoch 2900 train loss 43934.20703125 val loss 42174.8984375\n",
      "Epoch 2910 train loss 43896.15234375 val loss 42151.57421875\n",
      "Epoch 2920 train loss 43857.6015625 val loss 42126.421875\n",
      "Epoch 2930 train loss 43818.515625 val loss 42101.84375\n",
      "Epoch 2940 train loss 43778.9921875 val loss 42076.56640625\n",
      "Epoch 2950 train loss 43738.9453125 val loss 42051.54296875\n",
      "Epoch 2960 train loss 43698.3125 val loss 42027.52734375\n",
      "Epoch 2970 train loss 43657.09765625 val loss 42001.98828125\n",
      "Epoch 2980 train loss 43615.0859375 val loss 41976.9375\n",
      "Epoch 2990 train loss 43572.12109375 val loss 41950.23828125\n",
      "Epoch 3000 train loss 43527.92578125 val loss 41923.08203125\n",
      "Epoch 3010 train loss 43482.75 val loss 41896.27734375\n",
      "Epoch 3020 train loss 43436.48046875 val loss 41867.64453125\n",
      "Epoch 3030 train loss 43388.265625 val loss 41838.1171875\n",
      "Epoch 3040 train loss 43338.12109375 val loss 41808.5625\n",
      "Epoch 3050 train loss 43286.76171875 val loss 41777.65625\n",
      "Epoch 3060 train loss 43233.9453125 val loss 41745.29296875\n",
      "Epoch 3070 train loss 43178.52734375 val loss 41712.41796875\n",
      "Epoch 3080 train loss 43120.90625 val loss 41678.51171875\n",
      "Epoch 3090 train loss 43061.76171875 val loss 41643.84765625\n",
      "Epoch 3100 train loss 43001.0546875 val loss 41610.0625\n",
      "Epoch 3110 train loss 42938.546875 val loss 41574.921875\n",
      "Epoch 3120 train loss 42874.11328125 val loss 41539.89453125\n",
      "Epoch 3130 train loss 42808.92578125 val loss 41503.234375\n",
      "Epoch 3140 train loss 42743.359375 val loss 41466.67578125\n",
      "Epoch 3150 train loss 42677.8984375 val loss 41433.82421875\n",
      "Epoch 3160 train loss 42611.10546875 val loss 41399.625\n",
      "Epoch 3170 train loss 42543.73046875 val loss 41366.33984375\n",
      "Epoch 3180 train loss 42476.27734375 val loss 41332.03515625\n",
      "Epoch 3190 train loss 42407.98828125 val loss 41301.07421875\n",
      "Epoch 3200 train loss 42338.85546875 val loss 41270.828125\n",
      "Epoch 3210 train loss 42271.0 val loss 41244.2421875\n",
      "Epoch 3220 train loss 42202.640625 val loss 41214.0390625\n",
      "Epoch 3230 train loss 42133.79296875 val loss 41190.921875\n",
      "Epoch 3240 train loss 42066.25 val loss 41168.03125\n",
      "Epoch 3250 train loss 41999.7265625 val loss 41149.5859375\n",
      "Epoch 3260 train loss 41933.015625 val loss 41128.05078125\n",
      "Epoch 3270 train loss 41866.05859375 val loss 41108.9296875\n",
      "Epoch 3280 train loss 41798.67578125 val loss 41087.484375\n",
      "Epoch 3290 train loss 41730.90234375 val loss 41067.1015625\n",
      "Epoch 3300 train loss 41662.3203125 val loss 41040.9296875\n",
      "Epoch 3310 train loss 41592.6953125 val loss 41015.08984375\n",
      "Epoch 3320 train loss 41521.92578125 val loss 40988.09375\n",
      "Epoch 3330 train loss 41450.05859375 val loss 40962.85546875\n",
      "Epoch 3340 train loss 41377.171875 val loss 40939.58984375\n",
      "Epoch 3350 train loss 41303.38671875 val loss 40918.6875\n",
      "Epoch 3360 train loss 41228.42578125 val loss 40898.33203125\n",
      "Epoch 3370 train loss 41152.33203125 val loss 40877.828125\n",
      "Epoch 3380 train loss 41074.8515625 val loss 40846.54296875\n",
      "Epoch 3390 train loss 40996.0703125 val loss 40820.90625\n",
      "Epoch 3400 train loss 40916.08203125 val loss 40793.96484375\n",
      "Epoch 3410 train loss 40834.984375 val loss 40767.01171875\n",
      "Epoch 3420 train loss 40752.46875 val loss 40739.7890625\n",
      "Epoch 3430 train loss 40668.4140625 val loss 40712.015625\n",
      "Epoch 3440 train loss 40582.78515625 val loss 40683.88671875\n",
      "Epoch 3450 train loss 40495.7734375 val loss 40662.06640625\n",
      "Epoch 3460 train loss 40407.59375 val loss 40640.21484375\n",
      "Epoch 3470 train loss 40317.50390625 val loss 40614.6015625\n",
      "Epoch 3480 train loss 40225.88671875 val loss 40578.94140625\n",
      "Epoch 3490 train loss 40132.83984375 val loss 40546.1015625\n",
      "Epoch 3500 train loss 40039.0859375 val loss 40517.078125\n",
      "Epoch 3510 train loss 39943.94140625 val loss 40485.265625\n",
      "Epoch 3520 train loss 39848.0390625 val loss 40447.625\n",
      "Epoch 3530 train loss 39751.5625 val loss 40411.90625\n",
      "Epoch 3540 train loss 39654.47265625 val loss 40380.63671875\n",
      "Epoch 3550 train loss 39556.26171875 val loss 40356.61328125\n",
      "Epoch 3560 train loss 39456.99609375 val loss 40338.14453125\n",
      "Epoch 3570 train loss 39357.05078125 val loss 40318.83203125\n",
      "Epoch 3580 train loss 39256.1171875 val loss 40303.03125\n",
      "Epoch 3590 train loss 39154.43359375 val loss 40280.609375\n",
      "Epoch 3600 train loss 39054.328125 val loss 40265.41015625\n",
      "Epoch 3610 train loss 38956.43359375 val loss 40247.41015625\n",
      "Epoch 3620 train loss 38858.703125 val loss 40231.9921875\n",
      "Epoch 3630 train loss 38761.77734375 val loss 40229.7578125\n",
      "Epoch 3640 train loss 38666.80859375 val loss 40225.87109375\n",
      "Epoch 3650 train loss 38572.78515625 val loss 40217.7578125\n",
      "Epoch 3660 train loss 38479.5078125 val loss 40195.33203125\n",
      "Epoch 3670 train loss 38387.03515625 val loss 40178.19921875\n",
      "Epoch 3680 train loss 38295.09765625 val loss 40169.2421875\n",
      "Epoch 3690 train loss 38204.82421875 val loss 40166.23828125\n",
      "Epoch 3700 train loss 38116.140625 val loss 40170.66015625\n",
      "Epoch 3710 train loss 38027.4375 val loss 40168.671875\n",
      "Epoch 3720 train loss 37939.29296875 val loss 40171.0390625\n",
      "Epoch 3730 train loss 37852.796875 val loss 40175.65625\n",
      "Epoch 3740 train loss 37768.08203125 val loss 40171.02734375\n",
      "Epoch 3750 train loss 37685.44140625 val loss 40174.49609375\n",
      "Epoch 3760 train loss 37604.45703125 val loss 40178.578125\n",
      "Epoch 3770 train loss 37524.37890625 val loss 40180.0078125\n",
      "Epoch 3780 train loss 37446.09375 val loss 40203.28515625\n",
      "Epoch 3790 train loss 37369.14453125 val loss 40216.53125\n",
      "Epoch 3800 train loss 37294.24609375 val loss 40232.68359375\n",
      "Epoch 3810 train loss 37221.4296875 val loss 40244.01171875\n",
      "Epoch 3820 train loss 37150.18359375 val loss 40249.51171875\n",
      "Epoch 3830 train loss 37080.00390625 val loss 40259.19921875\n",
      "Epoch 3840 train loss 37010.48046875 val loss 40271.93359375\n",
      "Epoch 3850 train loss 36942.68359375 val loss 40300.37890625\n",
      "Epoch 3860 train loss 36876.75 val loss 40322.45703125\n",
      "Epoch 3870 train loss 36811.70703125 val loss 40334.09375\n",
      "Epoch 3880 train loss 36747.19921875 val loss 40340.80859375\n",
      "Epoch 3890 train loss 36683.5078125 val loss 40360.1640625\n",
      "Epoch 3900 train loss 36620.71875 val loss 40391.3828125\n",
      "Epoch 3910 train loss 36559.45703125 val loss 40426.2265625\n",
      "Epoch 3920 train loss 36500.70703125 val loss 40465.1328125\n",
      "Epoch 3930 train loss 36443.62109375 val loss 40498.52734375\n",
      "Epoch 3940 train loss 36388.421875 val loss 40530.0859375\n",
      "Epoch 3950 train loss 36335.16796875 val loss 40561.9296875\n",
      "Epoch 3960 train loss 36283.52734375 val loss 40593.2421875\n",
      "Epoch 3970 train loss 36233.59765625 val loss 40624.5546875\n",
      "Epoch 3980 train loss 36184.39453125 val loss 40649.921875\n",
      "Epoch 3990 train loss 36132.6015625 val loss 40660.45703125\n",
      "Epoch 4000 train loss 36076.3984375 val loss 40670.8515625\n",
      "Epoch 4010 train loss 36020.73828125 val loss 40690.71484375\n",
      "Epoch 4020 train loss 35970.80859375 val loss 40734.34375\n",
      "Epoch 4030 train loss 35923.53515625 val loss 40765.0859375\n",
      "Epoch 4040 train loss 35876.44921875 val loss 40794.13671875\n",
      "Epoch 4050 train loss 35830.484375 val loss 40823.796875\n",
      "Epoch 4060 train loss 35786.5390625 val loss 40858.703125\n",
      "Epoch 4070 train loss 35743.5078125 val loss 40887.2578125\n",
      "Epoch 4080 train loss 35702.171875 val loss 40911.7578125\n",
      "Epoch 4090 train loss 35659.859375 val loss 40916.71484375\n",
      "Epoch 4100 train loss 35616.234375 val loss 40927.8125\n",
      "Epoch 4110 train loss 35570.34765625 val loss 40946.20703125\n",
      "Epoch 4120 train loss 35522.0703125 val loss 40943.19140625\n",
      "Epoch 4130 train loss 35468.80078125 val loss 40955.08984375\n",
      "Epoch 4140 train loss 35412.75 val loss 40981.0625\n",
      "Epoch 4150 train loss 35355.88671875 val loss 41011.08984375\n",
      "Epoch 4160 train loss 35298.984375 val loss 41041.4140625\n",
      "Epoch 4170 train loss 35245.8125 val loss 41079.8125\n",
      "Epoch 4180 train loss 35199.18359375 val loss 41123.421875\n",
      "Epoch 4190 train loss 35154.15234375 val loss 41141.98046875\n",
      "Epoch 4200 train loss 35104.87109375 val loss 41146.265625\n",
      "Epoch 4210 train loss 35051.16796875 val loss 41165.203125\n",
      "Epoch 4220 train loss 34995.22265625 val loss 41209.7109375\n",
      "Epoch 4230 train loss 34935.140625 val loss 41238.30078125\n",
      "Epoch 4240 train loss 34876.21484375 val loss 41248.4609375\n",
      "Epoch 4250 train loss 34822.23046875 val loss 41274.171875\n",
      "Epoch 4260 train loss 34776.421875 val loss 41357.4375\n",
      "Epoch 4270 train loss 34733.05859375 val loss 41422.55859375\n",
      "Epoch 4280 train loss 34692.03125 val loss 41470.85546875\n",
      "Epoch 4290 train loss 34653.171875 val loss 41526.8984375\n",
      "Epoch 4300 train loss 34616.0234375 val loss 41569.28125\n",
      "Epoch 4310 train loss 34578.921875 val loss 41584.14453125\n",
      "Epoch 4320 train loss 34541.42578125 val loss 41606.3515625\n",
      "Epoch 4330 train loss 34504.515625 val loss 41645.53125\n",
      "Epoch 4340 train loss 34468.82421875 val loss 41685.29296875\n",
      "Epoch 4350 train loss 34434.05859375 val loss 41724.5859375\n",
      "Epoch 4360 train loss 34400.71875 val loss 41764.3828125\n",
      "Epoch 4370 train loss 34367.83984375 val loss 41790.57421875\n",
      "Epoch 4380 train loss 34335.14453125 val loss 41792.76171875\n",
      "Epoch 4390 train loss 34297.83203125 val loss 41738.96484375\n",
      "Epoch 4400 train loss 34262.875 val loss 41761.1328125\n",
      "Epoch 4410 train loss 34228.3671875 val loss 41751.61328125\n",
      "Epoch 4420 train loss 34193.19140625 val loss 41716.35546875\n",
      "Epoch 4430 train loss 34160.7265625 val loss 41731.13671875\n",
      "Epoch 4440 train loss 34131.06640625 val loss 41752.76953125\n",
      "Epoch 4450 train loss 34102.8359375 val loss 41759.6796875\n",
      "Epoch 4460 train loss 34075.94921875 val loss 41774.7578125\n",
      "Epoch 4470 train loss 34050.3046875 val loss 41794.1875\n",
      "Epoch 4480 train loss 34025.109375 val loss 41789.859375\n",
      "Epoch 4490 train loss 34000.48828125 val loss 41795.015625\n",
      "Epoch 4500 train loss 33977.51171875 val loss 41806.66015625\n",
      "Epoch 4510 train loss 33954.5859375 val loss 41824.43359375\n",
      "Epoch 4520 train loss 33932.0078125 val loss 41833.55859375\n",
      "Epoch 4530 train loss 33910.703125 val loss 41846.3671875\n",
      "Epoch 4540 train loss 33889.46484375 val loss 41853.0078125\n",
      "Epoch 4550 train loss 33867.6640625 val loss 41852.50390625\n",
      "Epoch 4560 train loss 33846.12109375 val loss 41851.16015625\n",
      "Epoch 4570 train loss 33825.15625 val loss 41858.3515625\n",
      "Epoch 4580 train loss 33804.9609375 val loss 41860.78125\n",
      "Epoch 4590 train loss 33785.015625 val loss 41867.2734375\n",
      "Epoch 4600 train loss 33763.96484375 val loss 41851.63671875\n",
      "Epoch 4610 train loss 33742.0234375 val loss 41831.8046875\n",
      "Epoch 4620 train loss 33720.49609375 val loss 41837.22265625\n",
      "Epoch 4630 train loss 33699.5625 val loss 41843.63671875\n",
      "Epoch 4640 train loss 33679.45703125 val loss 41863.1796875\n",
      "Epoch 4650 train loss 33660.25390625 val loss 41872.71484375\n",
      "Epoch 4660 train loss 33642.3671875 val loss 41894.5078125\n",
      "Epoch 4670 train loss 33625.17578125 val loss 41914.17578125\n",
      "Epoch 4680 train loss 33607.984375 val loss 41930.4609375\n",
      "Epoch 4690 train loss 33588.0859375 val loss 41918.046875\n",
      "Epoch 4700 train loss 33569.16015625 val loss 41954.87109375\n",
      "Epoch 4710 train loss 33551.859375 val loss 41996.00390625\n",
      "Epoch 4720 train loss 33535.62890625 val loss 42018.39453125\n",
      "Epoch 4730 train loss 33520.05859375 val loss 42030.4453125\n",
      "Epoch 4740 train loss 33504.7421875 val loss 42040.60546875\n",
      "Epoch 4750 train loss 33489.7109375 val loss 42053.0546875\n",
      "Epoch 4760 train loss 33474.859375 val loss 42062.51953125\n",
      "Epoch 4770 train loss 33460.0078125 val loss 42060.41796875\n",
      "Epoch 4780 train loss 33445.375 val loss 42064.171875\n",
      "Epoch 4790 train loss 33430.9453125 val loss 42078.01171875\n",
      "Epoch 4800 train loss 33416.64453125 val loss 42089.97265625\n",
      "Epoch 4810 train loss 33402.6171875 val loss 42098.51171875\n",
      "Epoch 4820 train loss 33388.98046875 val loss 42104.37890625\n",
      "Epoch 4830 train loss 33375.53515625 val loss 42116.14453125\n",
      "Epoch 4840 train loss 33361.90234375 val loss 42104.4453125\n",
      "Epoch 4850 train loss 33345.75 val loss 42098.421875\n",
      "Epoch 4860 train loss 33328.9375 val loss 42107.0\n",
      "Epoch 4870 train loss 33310.26953125 val loss 42096.02734375\n",
      "Epoch 4880 train loss 33292.8046875 val loss 42113.28515625\n",
      "Epoch 4890 train loss 33276.78515625 val loss 42133.26171875\n",
      "Epoch 4900 train loss 33261.41796875 val loss 42140.765625\n",
      "Epoch 4910 train loss 33245.890625 val loss 42161.9453125\n",
      "Epoch 4920 train loss 33230.7578125 val loss 42174.40625\n",
      "Epoch 4930 train loss 33216.203125 val loss 42164.73046875\n",
      "Epoch 4940 train loss 33202.1328125 val loss 42154.5546875\n",
      "Epoch 4950 train loss 33187.44140625 val loss 42159.4140625\n",
      "Epoch 4960 train loss 33171.97265625 val loss 42176.8515625\n",
      "Epoch 4970 train loss 33156.3828125 val loss 42180.7734375\n",
      "Epoch 4980 train loss 33140.87890625 val loss 42173.40234375\n",
      "Epoch 4990 train loss 33125.50390625 val loss 42170.6171875\n",
      "Epoch 5000 train loss 33111.390625 val loss 42193.5703125\n",
      "Epoch 5010 train loss 33097.16015625 val loss 42194.48046875\n",
      "Epoch 5020 train loss 33083.375 val loss 42203.69140625\n",
      "Epoch 5030 train loss 33070.15625 val loss 42214.3359375\n",
      "Epoch 5040 train loss 33057.19921875 val loss 42213.30078125\n",
      "Epoch 5050 train loss 33044.90234375 val loss 42217.97265625\n",
      "Epoch 5060 train loss 33032.8984375 val loss 42237.47265625\n",
      "Epoch 5070 train loss 33018.75 val loss 42188.50390625\n",
      "Epoch 5080 train loss 33003.62109375 val loss 42182.6796875\n",
      "Epoch 5090 train loss 32988.4609375 val loss 42177.8359375\n",
      "Epoch 5100 train loss 32973.43359375 val loss 42167.5234375\n",
      "Epoch 5110 train loss 32958.76953125 val loss 42177.01953125\n",
      "Epoch 5120 train loss 32944.71875 val loss 42184.9609375\n",
      "Epoch 5130 train loss 32930.640625 val loss 42181.91015625\n",
      "Epoch 5140 train loss 32916.921875 val loss 42192.41015625\n",
      "Epoch 5150 train loss 32903.5390625 val loss 42214.5390625\n",
      "Epoch 5160 train loss 32891.1484375 val loss 42228.5625\n",
      "Epoch 5170 train loss 32878.75 val loss 42275.90625\n",
      "Epoch 5180 train loss 32867.07421875 val loss 42269.9296875\n",
      "Epoch 5190 train loss 32854.50390625 val loss 42305.21484375\n",
      "Epoch 5200 train loss 32842.7890625 val loss 42318.0703125\n",
      "Epoch 5210 train loss 32831.1640625 val loss 42348.0703125\n",
      "Epoch 5220 train loss 32819.68359375 val loss 42363.140625\n",
      "Epoch 5230 train loss 32808.39453125 val loss 42379.60546875\n",
      "Epoch 5240 train loss 32796.71484375 val loss 42381.06640625\n",
      "Epoch 5250 train loss 32784.63671875 val loss 42405.58984375\n",
      "Epoch 5260 train loss 32772.9375 val loss 42413.1640625\n",
      "Epoch 5270 train loss 32761.73046875 val loss 42425.2734375\n",
      "Epoch 5280 train loss 32750.701171875 val loss 42439.3203125\n",
      "Epoch 5290 train loss 32739.8359375 val loss 42452.92578125\n",
      "Epoch 5300 train loss 32728.11328125 val loss 42448.8671875\n",
      "Epoch 5310 train loss 32716.798828125 val loss 42463.26171875\n",
      "Epoch 5320 train loss 32705.751953125 val loss 42469.0234375\n",
      "Epoch 5330 train loss 32695.099609375 val loss 42486.6796875\n",
      "Epoch 5340 train loss 32684.052734375 val loss 42474.45703125\n",
      "Epoch 5350 train loss 32672.904296875 val loss 42487.171875\n",
      "Epoch 5360 train loss 32661.73828125 val loss 42502.41796875\n",
      "Epoch 5370 train loss 32650.798828125 val loss 42493.72265625\n",
      "Epoch 5380 train loss 32639.421875 val loss 42501.3828125\n",
      "Epoch 5390 train loss 32628.234375 val loss 42513.9296875\n",
      "Epoch 5400 train loss 32617.31640625 val loss 42517.82421875\n",
      "Epoch 5410 train loss 32606.685546875 val loss 42513.11328125\n",
      "Epoch 5420 train loss 32595.419921875 val loss 42517.98828125\n",
      "Epoch 5430 train loss 32584.701171875 val loss 42511.88671875\n",
      "Epoch 5440 train loss 32574.142578125 val loss 42521.33203125\n",
      "Epoch 5450 train loss 32563.146484375 val loss 42521.58203125\n",
      "Epoch 5460 train loss 32551.796875 val loss 42512.1328125\n",
      "Epoch 5470 train loss 32539.09765625 val loss 42492.29296875\n",
      "Epoch 5480 train loss 32526.115234375 val loss 42463.53125\n",
      "Epoch 5490 train loss 32513.068359375 val loss 42446.1796875\n",
      "Epoch 5500 train loss 32499.529296875 val loss 42437.76171875\n",
      "Epoch 5510 train loss 32485.169921875 val loss 42406.4765625\n",
      "Epoch 5520 train loss 32471.611328125 val loss 42402.30078125\n",
      "Epoch 5530 train loss 32458.087890625 val loss 42372.73828125\n",
      "Epoch 5540 train loss 32444.89453125 val loss 42368.03125\n",
      "Epoch 5550 train loss 32432.119140625 val loss 42357.8984375\n",
      "Epoch 5560 train loss 32419.740234375 val loss 42361.36328125\n",
      "Epoch 5570 train loss 32407.8359375 val loss 42359.73828125\n",
      "Epoch 5580 train loss 32396.322265625 val loss 42361.6328125\n",
      "Epoch 5590 train loss 32384.56640625 val loss 42349.5390625\n",
      "Epoch 5600 train loss 32373.193359375 val loss 42341.53515625\n",
      "Epoch 5610 train loss 32362.080078125 val loss 42326.546875\n",
      "Epoch 5620 train loss 32350.919921875 val loss 42330.35546875\n",
      "Epoch 5630 train loss 32339.9375 val loss 42368.125\n",
      "Epoch 5640 train loss 32328.5 val loss 42365.12109375\n",
      "Epoch 5650 train loss 32316.5546875 val loss 42365.125\n",
      "Epoch 5660 train loss 32304.166015625 val loss 42362.56640625\n",
      "Epoch 5670 train loss 32292.748046875 val loss 42360.9453125\n",
      "Epoch 5680 train loss 32281.41796875 val loss 42371.37890625\n",
      "Epoch 5690 train loss 32270.5625 val loss 42360.3671875\n",
      "Epoch 5700 train loss 32259.77734375 val loss 42361.609375\n",
      "Epoch 5710 train loss 32249.373046875 val loss 42376.87890625\n",
      "Epoch 5720 train loss 32239.0078125 val loss 42382.81640625\n",
      "Epoch 5730 train loss 32228.78125 val loss 42372.8125\n",
      "Epoch 5740 train loss 32218.849609375 val loss 42362.8671875\n",
      "Epoch 5750 train loss 32208.634765625 val loss 42370.03515625\n",
      "Epoch 5760 train loss 32198.39453125 val loss 42363.25390625\n",
      "Epoch 5770 train loss 32188.251953125 val loss 42354.93359375\n",
      "Epoch 5780 train loss 32177.572265625 val loss 42345.39453125\n",
      "Epoch 5790 train loss 32165.544921875 val loss 42328.09765625\n",
      "Epoch 5800 train loss 32154.041015625 val loss 42331.24609375\n",
      "Epoch 5810 train loss 32142.814453125 val loss 42342.33984375\n",
      "Epoch 5820 train loss 32131.732421875 val loss 42334.4140625\n",
      "Epoch 5830 train loss 32120.935546875 val loss 42338.79296875\n",
      "Epoch 5840 train loss 32101.474609375 val loss 42341.75\n",
      "Epoch 5850 train loss 32065.537109375 val loss 42488.6015625\n",
      "Epoch 5860 train loss 32047.6328125 val loss 42469.70703125\n",
      "Epoch 5870 train loss 32031.61328125 val loss 42397.8046875\n",
      "Epoch 5880 train loss 32015.8203125 val loss 42418.25390625\n",
      "Epoch 5890 train loss 32001.4921875 val loss 42426.9140625\n",
      "Epoch 5900 train loss 31987.85546875 val loss 42420.73828125\n",
      "Epoch 5910 train loss 31974.642578125 val loss 42418.05078125\n",
      "Epoch 5920 train loss 31960.83984375 val loss 42403.953125\n",
      "Epoch 5930 train loss 31945.826171875 val loss 42395.47265625\n",
      "Epoch 5940 train loss 31930.544921875 val loss 42388.66015625\n",
      "Epoch 5950 train loss 31915.060546875 val loss 42389.1953125\n",
      "Epoch 5960 train loss 31894.93359375 val loss 42412.171875\n",
      "Epoch 5970 train loss 31875.234375 val loss 42414.58984375\n",
      "Epoch 5980 train loss 31858.357421875 val loss 42391.8046875\n",
      "Epoch 5990 train loss 31842.484375 val loss 42369.703125\n",
      "Epoch 6000 train loss 31826.361328125 val loss 42366.65625\n",
      "Epoch 6010 train loss 31811.466796875 val loss 42354.9921875\n",
      "Epoch 6020 train loss 31796.88671875 val loss 42334.6015625\n",
      "Epoch 6030 train loss 31782.671875 val loss 42339.62109375\n",
      "Epoch 6040 train loss 31768.3984375 val loss 42332.79296875\n",
      "Epoch 6050 train loss 31753.50390625 val loss 42317.5234375\n",
      "Epoch 6060 train loss 31736.310546875 val loss 42310.36328125\n",
      "Epoch 6070 train loss 31719.232421875 val loss 42316.40625\n",
      "Epoch 6080 train loss 31703.3671875 val loss 42332.50390625\n",
      "Epoch 6090 train loss 31688.71875 val loss 42311.40234375\n",
      "Epoch 6100 train loss 31674.466796875 val loss 42291.125\n",
      "Epoch 6110 train loss 31660.623046875 val loss 42278.9921875\n",
      "Epoch 6120 train loss 31647.5234375 val loss 42269.5625\n",
      "Epoch 6130 train loss 31634.650390625 val loss 42263.48828125\n",
      "Epoch 6140 train loss 31622.181640625 val loss 42254.90234375\n",
      "Epoch 6150 train loss 31610.021484375 val loss 42238.8828125\n",
      "Epoch 6160 train loss 31598.43359375 val loss 42225.1640625\n",
      "Epoch 6170 train loss 31587.087890625 val loss 42211.09375\n",
      "Epoch 6180 train loss 31575.509765625 val loss 42215.91796875\n",
      "Epoch 6190 train loss 31564.505859375 val loss 42165.23828125\n",
      "Epoch 6200 train loss 31553.716796875 val loss 42144.73828125\n",
      "Epoch 6210 train loss 31543.0234375 val loss 42147.15625\n",
      "Epoch 6220 train loss 31532.60546875 val loss 42127.52734375\n",
      "Epoch 6230 train loss 31522.279296875 val loss 42120.26171875\n",
      "Epoch 6240 train loss 31512.060546875 val loss 42090.94921875\n",
      "Epoch 6250 train loss 31502.61328125 val loss 42097.1171875\n",
      "Epoch 6260 train loss 31493.267578125 val loss 42090.8046875\n",
      "Epoch 6270 train loss 31484.126953125 val loss 42075.0703125\n",
      "Epoch 6280 train loss 31474.83203125 val loss 42054.1171875\n",
      "Epoch 6290 train loss 31465.751953125 val loss 42051.41015625\n",
      "Epoch 6300 train loss 31456.80859375 val loss 42055.89453125\n",
      "Epoch 6310 train loss 31447.966796875 val loss 42053.734375\n",
      "Epoch 6320 train loss 31438.45703125 val loss 42045.9140625\n",
      "Epoch 6330 train loss 31429.080078125 val loss 42040.40234375\n",
      "Epoch 6340 train loss 31419.640625 val loss 42043.828125\n",
      "Epoch 6350 train loss 31409.330078125 val loss 42029.50390625\n",
      "Epoch 6360 train loss 31398.95703125 val loss 42021.80859375\n",
      "Epoch 6370 train loss 31389.087890625 val loss 42021.015625\n",
      "Epoch 6380 train loss 31378.029296875 val loss 42016.484375\n",
      "Epoch 6390 train loss 31367.44921875 val loss 42009.4140625\n",
      "Epoch 6400 train loss 31357.119140625 val loss 42009.859375\n",
      "Epoch 6410 train loss 31347.578125 val loss 42010.890625\n",
      "Epoch 6420 train loss 31337.01953125 val loss 42023.75390625\n",
      "Epoch 6430 train loss 31327.5234375 val loss 42009.96875\n",
      "Epoch 6440 train loss 31318.26953125 val loss 42001.76953125\n",
      "Epoch 6450 train loss 31308.76953125 val loss 41978.69921875\n",
      "Epoch 6460 train loss 31299.126953125 val loss 41978.67578125\n",
      "Epoch 6470 train loss 31289.55078125 val loss 41980.953125\n",
      "Epoch 6480 train loss 31278.73828125 val loss 41971.7890625\n",
      "Epoch 6490 train loss 31267.388671875 val loss 41963.73828125\n",
      "Epoch 6500 train loss 31254.95703125 val loss 41966.3359375\n",
      "Epoch 6510 train loss 31241.7890625 val loss 41976.16796875\n",
      "Epoch 6520 train loss 31230.275390625 val loss 41986.3828125\n",
      "Epoch 6530 train loss 31219.369140625 val loss 41971.1796875\n",
      "Epoch 6540 train loss 31208.537109375 val loss 41953.8828125\n",
      "Epoch 6550 train loss 31198.224609375 val loss 41959.4140625\n",
      "Epoch 6560 train loss 31187.951171875 val loss 41959.3046875\n",
      "Epoch 6570 train loss 31177.84375 val loss 41943.74609375\n",
      "Epoch 6580 train loss 31167.193359375 val loss 41922.66015625\n",
      "Epoch 6590 train loss 31157.2421875 val loss 41917.47265625\n",
      "Epoch 6600 train loss 31147.80859375 val loss 41927.0390625\n",
      "Epoch 6610 train loss 31138.451171875 val loss 41917.72265625\n",
      "Epoch 6620 train loss 31128.755859375 val loss 41917.6328125\n",
      "Epoch 6630 train loss 31119.65234375 val loss 41930.08984375\n",
      "Epoch 6640 train loss 31110.537109375 val loss 41928.28125\n",
      "Epoch 6650 train loss 31101.46484375 val loss 41920.109375\n",
      "Epoch 6660 train loss 31092.5859375 val loss 41916.91015625\n",
      "Epoch 6670 train loss 31083.154296875 val loss 41913.0546875\n",
      "Epoch 6680 train loss 31073.58203125 val loss 41905.29296875\n",
      "Epoch 6690 train loss 31063.779296875 val loss 41884.0546875\n",
      "Epoch 6700 train loss 31054.287109375 val loss 41867.79296875\n",
      "Epoch 6710 train loss 31044.927734375 val loss 41848.3125\n",
      "Epoch 6720 train loss 31035.578125 val loss 41836.8359375\n",
      "Epoch 6730 train loss 31026.091796875 val loss 41805.64453125\n",
      "Epoch 6740 train loss 31016.41796875 val loss 41820.8671875\n",
      "Epoch 6750 train loss 31006.3046875 val loss 41820.51953125\n",
      "Epoch 6760 train loss 30996.37890625 val loss 41794.79296875\n",
      "Epoch 6770 train loss 30986.751953125 val loss 41771.09765625\n",
      "Epoch 6780 train loss 30977.80859375 val loss 41765.046875\n",
      "Epoch 6790 train loss 30968.576171875 val loss 41746.33203125\n",
      "Epoch 6800 train loss 30960.078125 val loss 41745.0703125\n",
      "Epoch 6810 train loss 30951.703125 val loss 41720.47265625\n",
      "Epoch 6820 train loss 30944.166015625 val loss 41725.65625\n",
      "Epoch 6830 train loss 30936.056640625 val loss 41709.640625\n",
      "Epoch 6840 train loss 30928.244140625 val loss 41701.6015625\n",
      "Epoch 6850 train loss 30920.548828125 val loss 41681.37890625\n",
      "Epoch 6860 train loss 30912.404296875 val loss 41680.265625\n",
      "Epoch 6870 train loss 30904.345703125 val loss 41650.0546875\n",
      "Epoch 6880 train loss 30896.380859375 val loss 41645.12109375\n",
      "Epoch 6890 train loss 30887.916015625 val loss 41622.08203125\n",
      "Epoch 6900 train loss 30879.134765625 val loss 41626.0234375\n",
      "Epoch 6910 train loss 30870.677734375 val loss 41615.7890625\n",
      "Epoch 6920 train loss 30862.62109375 val loss 41615.14453125\n",
      "Epoch 6930 train loss 30854.8203125 val loss 41605.8828125\n",
      "Epoch 6940 train loss 30847.431640625 val loss 41587.73046875\n",
      "Epoch 6950 train loss 30839.75 val loss 41590.69921875\n",
      "Epoch 6960 train loss 30831.71875 val loss 41559.046875\n",
      "Epoch 6970 train loss 30823.861328125 val loss 41560.10546875\n",
      "Epoch 6980 train loss 30815.8515625 val loss 41546.8828125\n",
      "Epoch 6990 train loss 30807.775390625 val loss 41538.67578125\n",
      "Epoch 7000 train loss 30799.1953125 val loss 41523.0859375\n",
      "Epoch 7010 train loss 30790.419921875 val loss 41515.01171875\n",
      "Epoch 7020 train loss 30781.626953125 val loss 41505.91796875\n",
      "Epoch 7030 train loss 30773.091796875 val loss 41500.96484375\n",
      "Epoch 7040 train loss 30764.7734375 val loss 41490.2734375\n",
      "Epoch 7050 train loss 30756.451171875 val loss 41484.97265625\n",
      "Epoch 7060 train loss 30748.421875 val loss 41476.39453125\n",
      "Epoch 7070 train loss 30740.615234375 val loss 41458.26953125\n",
      "Epoch 7080 train loss 30733.015625 val loss 41442.43359375\n",
      "Epoch 7090 train loss 30725.669921875 val loss 41444.69140625\n",
      "Epoch 7100 train loss 30718.328125 val loss 41432.37109375\n",
      "Epoch 7110 train loss 30711.244140625 val loss 41448.70703125\n",
      "Epoch 7120 train loss 30703.986328125 val loss 41430.5\n",
      "Epoch 7130 train loss 30696.603515625 val loss 41430.85546875\n",
      "Epoch 7140 train loss 30685.734375 val loss 41444.3984375\n",
      "Epoch 7150 train loss 30673.470703125 val loss 41452.5234375\n",
      "Epoch 7160 train loss 30662.125 val loss 41466.80078125\n",
      "Epoch 7170 train loss 30652.33203125 val loss 41453.203125\n",
      "Epoch 7180 train loss 30643.486328125 val loss 41443.00390625\n",
      "Epoch 7190 train loss 30635.462890625 val loss 41436.99609375\n",
      "Epoch 7200 train loss 30627.33203125 val loss 41422.8984375\n",
      "Epoch 7210 train loss 30619.279296875 val loss 41405.8515625\n",
      "Epoch 7220 train loss 30609.208984375 val loss 41405.3515625\n",
      "Epoch 7230 train loss 30598.138671875 val loss 41383.71875\n",
      "Epoch 7240 train loss 30588.19921875 val loss 41363.92578125\n",
      "Epoch 7250 train loss 30580.685546875 val loss 41370.71484375\n",
      "Epoch 7260 train loss 30573.279296875 val loss 41349.0078125\n",
      "Epoch 7270 train loss 30566.328125 val loss 41336.703125\n",
      "Epoch 7280 train loss 30559.37890625 val loss 41318.19140625\n",
      "Epoch 7290 train loss 30553.11328125 val loss 41317.921875\n",
      "Epoch 7300 train loss 30546.46484375 val loss 41289.4453125\n",
      "Epoch 7310 train loss 30538.97265625 val loss 41287.79296875\n",
      "Epoch 7320 train loss 30531.42578125 val loss 41280.1796875\n",
      "Epoch 7330 train loss 30524.04296875 val loss 41265.125\n",
      "Epoch 7340 train loss 30516.923828125 val loss 41283.71875\n",
      "Epoch 7350 train loss 30510.07421875 val loss 41267.35546875\n",
      "Epoch 7360 train loss 30503.19140625 val loss 41248.9453125\n",
      "Epoch 7370 train loss 30496.501953125 val loss 41251.96484375\n",
      "Epoch 7380 train loss 30489.515625 val loss 41245.90625\n",
      "Epoch 7390 train loss 30482.93359375 val loss 41226.890625\n",
      "Epoch 7400 train loss 30476.73828125 val loss 41230.69921875\n",
      "Epoch 7410 train loss 30470.65234375 val loss 41226.62109375\n",
      "Epoch 7420 train loss 30464.873046875 val loss 41218.2265625\n",
      "Epoch 7430 train loss 30459.021484375 val loss 41204.94921875\n",
      "Epoch 7440 train loss 30452.837890625 val loss 41198.39453125\n",
      "Epoch 7450 train loss 30446.693359375 val loss 41213.96875\n",
      "Epoch 7460 train loss 30440.541015625 val loss 41196.42578125\n",
      "Epoch 7470 train loss 30434.712890625 val loss 41185.9921875\n",
      "Epoch 7480 train loss 30428.986328125 val loss 41193.890625\n",
      "Epoch 7490 train loss 30423.052734375 val loss 41168.87890625\n",
      "Epoch 7500 train loss 30417.072265625 val loss 41163.53515625\n",
      "Epoch 7510 train loss 30411.455078125 val loss 41148.03125\n",
      "Epoch 7520 train loss 30405.431640625 val loss 41150.1953125\n",
      "Epoch 7530 train loss 30399.966796875 val loss 41141.703125\n",
      "Epoch 7540 train loss 30394.1015625 val loss 41125.09765625\n",
      "Epoch 7550 train loss 30388.466796875 val loss 41132.25\n",
      "Epoch 7560 train loss 30383.076171875 val loss 41122.61328125\n",
      "Epoch 7570 train loss 30377.869140625 val loss 41128.11328125\n",
      "Epoch 7580 train loss 30372.630859375 val loss 41120.76953125\n",
      "Epoch 7590 train loss 30367.265625 val loss 41125.28125\n",
      "Epoch 7600 train loss 30361.419921875 val loss 41086.81640625\n",
      "Epoch 7610 train loss 30355.50390625 val loss 41103.890625\n",
      "Epoch 7620 train loss 30349.44921875 val loss 41099.2890625\n",
      "Epoch 7630 train loss 30343.55078125 val loss 41106.171875\n",
      "Epoch 7640 train loss 30337.357421875 val loss 41105.33984375\n",
      "Epoch 7650 train loss 30330.8203125 val loss 41119.9140625\n",
      "Epoch 7660 train loss 30324.19140625 val loss 41143.4609375\n",
      "Epoch 7670 train loss 30317.703125 val loss 41152.89453125\n",
      "Epoch 7680 train loss 30311.3359375 val loss 41143.171875\n",
      "Epoch 7690 train loss 30305.3515625 val loss 41157.78515625\n",
      "Epoch 7700 train loss 30299.5703125 val loss 41143.9765625\n",
      "Epoch 7710 train loss 30293.998046875 val loss 41165.38671875\n",
      "Epoch 7720 train loss 30288.611328125 val loss 41167.7578125\n",
      "Epoch 7730 train loss 30282.736328125 val loss 41185.29296875\n",
      "Epoch 7740 train loss 30277.06640625 val loss 41210.63671875\n",
      "Epoch 7750 train loss 30271.953125 val loss 41198.921875\n",
      "Epoch 7760 train loss 30266.40234375 val loss 41171.52734375\n",
      "Epoch 7770 train loss 30260.130859375 val loss 41211.796875\n",
      "Epoch 7780 train loss 30253.40625 val loss 41170.12890625\n",
      "Epoch 7790 train loss 30247.12890625 val loss 41189.01171875\n",
      "Epoch 7800 train loss 30241.306640625 val loss 41168.19140625\n",
      "Epoch 7810 train loss 30235.23828125 val loss 41189.10546875\n",
      "Epoch 7820 train loss 30229.0546875 val loss 41179.80859375\n",
      "Epoch 7830 train loss 30222.8828125 val loss 41174.0703125\n",
      "Epoch 7840 train loss 30216.98828125 val loss 41173.7578125\n",
      "Epoch 7850 train loss 30211.23828125 val loss 41174.0078125\n",
      "Epoch 7860 train loss 30205.720703125 val loss 41168.50390625\n",
      "Epoch 7870 train loss 30200.150390625 val loss 41166.20703125\n",
      "Epoch 7880 train loss 30194.634765625 val loss 41161.75390625\n",
      "Epoch 7890 train loss 30189.546875 val loss 41174.35546875\n",
      "Epoch 7900 train loss 30184.478515625 val loss 41171.24609375\n",
      "Epoch 7910 train loss 30179.072265625 val loss 41172.0546875\n",
      "Epoch 7920 train loss 30174.4609375 val loss 41171.328125\n",
      "Epoch 7930 train loss 30169.275390625 val loss 41179.78125\n",
      "Epoch 7940 train loss 30164.29296875 val loss 41148.52734375\n",
      "Epoch 7950 train loss 30159.138671875 val loss 41144.0390625\n",
      "Epoch 7960 train loss 30153.96875 val loss 41141.96484375\n",
      "Epoch 7970 train loss 30149.041015625 val loss 41137.96484375\n",
      "Epoch 7980 train loss 30144.171875 val loss 41164.5078125\n",
      "Epoch 7990 train loss 30139.37109375 val loss 41136.57421875\n",
      "Epoch 8000 train loss 30134.65625 val loss 41156.42578125\n",
      "Epoch 8010 train loss 30129.95703125 val loss 41137.015625\n",
      "Epoch 8020 train loss 30125.611328125 val loss 41139.14453125\n",
      "Epoch 8030 train loss 30120.7890625 val loss 41119.6796875\n",
      "Epoch 8040 train loss 30116.06640625 val loss 41132.1953125\n",
      "Epoch 8050 train loss 30111.36328125 val loss 41121.4140625\n",
      "Epoch 8060 train loss 30107.146484375 val loss 41118.6953125\n",
      "Epoch 8070 train loss 30102.3203125 val loss 41125.53125\n",
      "Epoch 8080 train loss 30097.609375 val loss 41104.8515625\n",
      "Epoch 8090 train loss 30092.84765625 val loss 41106.5\n",
      "Epoch 8100 train loss 30088.373046875 val loss 41113.29296875\n",
      "Epoch 8110 train loss 30083.6875 val loss 41091.74609375\n",
      "Epoch 8120 train loss 30079.509765625 val loss 41084.30859375\n",
      "Epoch 8130 train loss 30075.197265625 val loss 41094.2421875\n",
      "Epoch 8140 train loss 30070.927734375 val loss 41095.22265625\n",
      "Epoch 8150 train loss 30066.578125 val loss 41077.19921875\n",
      "Epoch 8160 train loss 30062.529296875 val loss 41089.5390625\n",
      "Epoch 8170 train loss 30057.744140625 val loss 41091.17578125\n",
      "Epoch 8180 train loss 30051.607421875 val loss 41083.63671875\n",
      "Epoch 8190 train loss 30047.1171875 val loss 41076.24609375\n",
      "Epoch 8200 train loss 30042.400390625 val loss 41069.8984375\n",
      "Epoch 8210 train loss 30037.5703125 val loss 41062.64453125\n",
      "Epoch 8220 train loss 30033.318359375 val loss 41085.59375\n",
      "Epoch 8230 train loss 30028.779296875 val loss 41041.70703125\n",
      "Epoch 8240 train loss 30024.279296875 val loss 41070.5625\n",
      "Epoch 8250 train loss 30020.1953125 val loss 41073.140625\n",
      "Epoch 8260 train loss 30015.89453125 val loss 41061.17578125\n",
      "Epoch 8270 train loss 30011.677734375 val loss 41069.70703125\n",
      "Epoch 8280 train loss 30007.0390625 val loss 41051.75390625\n",
      "Epoch 8290 train loss 30002.12109375 val loss 41060.09375\n",
      "Epoch 8300 train loss 29996.61328125 val loss 41049.71875\n",
      "Epoch 8310 train loss 29989.47265625 val loss 41068.79296875\n",
      "Epoch 8320 train loss 29982.34375 val loss 41044.18359375\n",
      "Epoch 8330 train loss 29975.767578125 val loss 41054.53125\n",
      "Epoch 8340 train loss 29968.734375 val loss 41050.8125\n",
      "Epoch 8350 train loss 29961.259765625 val loss 41054.26171875\n",
      "Epoch 8360 train loss 29954.74609375 val loss 41033.984375\n",
      "Epoch 8370 train loss 29948.224609375 val loss 41033.08203125\n",
      "Epoch 8380 train loss 29942.248046875 val loss 41022.52734375\n",
      "Epoch 8390 train loss 29935.87890625 val loss 41023.34375\n",
      "Epoch 8400 train loss 29930.0390625 val loss 41015.12890625\n",
      "Epoch 8410 train loss 29924.228515625 val loss 41028.1171875\n",
      "Epoch 8420 train loss 29917.951171875 val loss 41026.2578125\n",
      "Epoch 8430 train loss 29911.703125 val loss 41001.09765625\n",
      "Epoch 8440 train loss 29905.681640625 val loss 41018.97265625\n",
      "Epoch 8450 train loss 29898.787109375 val loss 41001.25390625\n",
      "Epoch 8460 train loss 29892.341796875 val loss 40973.94140625\n",
      "Epoch 8470 train loss 29886.28125 val loss 40990.21484375\n",
      "Epoch 8480 train loss 29880.228515625 val loss 40988.30859375\n",
      "Epoch 8490 train loss 29874.30859375 val loss 41002.13671875\n",
      "Epoch 8500 train loss 29867.595703125 val loss 41005.1953125\n",
      "Epoch 8510 train loss 29861.39453125 val loss 40995.76953125\n",
      "Epoch 8520 train loss 29855.341796875 val loss 40989.20703125\n",
      "Epoch 8530 train loss 29849.5625 val loss 40991.9609375\n",
      "Epoch 8540 train loss 29843.923828125 val loss 41006.5859375\n",
      "Epoch 8550 train loss 29838.2578125 val loss 41021.375\n",
      "Epoch 8560 train loss 29832.216796875 val loss 41000.7734375\n",
      "Epoch 8570 train loss 29825.580078125 val loss 40991.79296875\n",
      "Epoch 8580 train loss 29819.29296875 val loss 40986.33984375\n",
      "Epoch 8590 train loss 29813.494140625 val loss 41010.3359375\n",
      "Epoch 8600 train loss 29807.17578125 val loss 41011.40625\n",
      "Epoch 8610 train loss 29800.810546875 val loss 40984.44140625\n",
      "Epoch 8620 train loss 29794.806640625 val loss 40997.04296875\n",
      "Epoch 8630 train loss 29788.462890625 val loss 40980.921875\n",
      "Epoch 8640 train loss 29782.076171875 val loss 40987.75\n",
      "Epoch 8650 train loss 29775.705078125 val loss 40982.71484375\n",
      "Epoch 8660 train loss 29769.2109375 val loss 40965.09375\n",
      "Epoch 8670 train loss 29762.49609375 val loss 40980.25\n",
      "Epoch 8680 train loss 29755.978515625 val loss 40957.49609375\n",
      "Epoch 8690 train loss 29748.26171875 val loss 40968.76171875\n",
      "Epoch 8700 train loss 29741.0078125 val loss 40959.109375\n",
      "Epoch 8710 train loss 29734.04296875 val loss 40963.04296875\n",
      "Epoch 8720 train loss 29726.27734375 val loss 40959.5390625\n",
      "Epoch 8730 train loss 29718.767578125 val loss 40962.703125\n",
      "Epoch 8740 train loss 29711.244140625 val loss 40961.13671875\n",
      "Epoch 8750 train loss 29703.662109375 val loss 40952.58984375\n",
      "Epoch 8760 train loss 29696.080078125 val loss 40969.8515625\n",
      "Epoch 8770 train loss 29688.17578125 val loss 40962.1171875\n",
      "Epoch 8780 train loss 29679.9296875 val loss 40952.8125\n",
      "Epoch 8790 train loss 29671.396484375 val loss 40980.00390625\n",
      "Epoch 8800 train loss 29663.62109375 val loss 40990.109375\n",
      "Epoch 8810 train loss 29655.828125 val loss 40995.75\n",
      "Epoch 8820 train loss 29648.212890625 val loss 40989.0\n",
      "Epoch 8830 train loss 29640.240234375 val loss 41006.2421875\n",
      "Epoch 8840 train loss 29632.232421875 val loss 40984.39453125\n",
      "Epoch 8850 train loss 29624.466796875 val loss 40978.65625\n",
      "Epoch 8860 train loss 29616.94921875 val loss 40990.546875\n",
      "Epoch 8870 train loss 29609.35546875 val loss 40997.0078125\n",
      "Epoch 8880 train loss 29602.005859375 val loss 40994.78515625\n",
      "Epoch 8890 train loss 29594.103515625 val loss 40992.95703125\n",
      "Epoch 8900 train loss 29585.82421875 val loss 40988.86328125\n",
      "Epoch 8910 train loss 29577.607421875 val loss 40979.4609375\n",
      "Epoch 8920 train loss 29568.787109375 val loss 40997.18359375\n",
      "Epoch 8930 train loss 29560.216796875 val loss 40995.36328125\n",
      "Epoch 8940 train loss 29551.203125 val loss 40988.40625\n",
      "Epoch 8950 train loss 29540.017578125 val loss 40997.77734375\n",
      "Epoch 8960 train loss 29529.181640625 val loss 40987.98046875\n",
      "Epoch 8970 train loss 29518.33984375 val loss 40989.94140625\n",
      "Epoch 8980 train loss 29508.462890625 val loss 40997.94140625\n",
      "Epoch 8990 train loss 29499.36328125 val loss 40990.87890625\n",
      "Epoch 9000 train loss 29490.001953125 val loss 41001.5\n",
      "Epoch 9010 train loss 29480.822265625 val loss 40997.91796875\n",
      "Epoch 9020 train loss 29472.056640625 val loss 41016.5703125\n",
      "Epoch 9030 train loss 29463.26171875 val loss 41014.609375\n",
      "Epoch 9040 train loss 29454.9453125 val loss 41001.10546875\n",
      "Epoch 9050 train loss 29446.498046875 val loss 41028.09375\n",
      "Epoch 9060 train loss 29438.37890625 val loss 41010.4140625\n",
      "Epoch 9070 train loss 29430.564453125 val loss 41043.3984375\n",
      "Epoch 9080 train loss 29422.5546875 val loss 41040.6640625\n",
      "Epoch 9090 train loss 29414.14453125 val loss 41063.21875\n",
      "Epoch 9100 train loss 29405.65625 val loss 41078.3359375\n",
      "Epoch 9110 train loss 29397.703125 val loss 41100.453125\n",
      "Epoch 9120 train loss 29390.044921875 val loss 41109.203125\n",
      "Epoch 9130 train loss 29382.220703125 val loss 41105.93359375\n",
      "Epoch 9140 train loss 29374.8828125 val loss 41131.03125\n",
      "Epoch 9150 train loss 29367.822265625 val loss 41140.81640625\n",
      "Epoch 9160 train loss 29360.59765625 val loss 41140.62890625\n",
      "Epoch 9170 train loss 29353.029296875 val loss 41130.546875\n",
      "Epoch 9180 train loss 29345.447265625 val loss 41133.71875\n",
      "Epoch 9190 train loss 29338.080078125 val loss 41133.1015625\n",
      "Epoch 9200 train loss 29330.36328125 val loss 41144.28125\n",
      "Epoch 9210 train loss 29322.69921875 val loss 41143.11328125\n",
      "Epoch 9220 train loss 29315.115234375 val loss 41171.19140625\n",
      "Epoch 9230 train loss 29307.541015625 val loss 41154.05078125\n",
      "Epoch 9240 train loss 29299.796875 val loss 41166.6953125\n",
      "Epoch 9250 train loss 29292.458984375 val loss 41132.84375\n",
      "Epoch 9260 train loss 29285.357421875 val loss 41159.0234375\n",
      "Epoch 9270 train loss 29277.5625 val loss 41145.0\n",
      "Epoch 9280 train loss 29269.78125 val loss 41143.203125\n",
      "Epoch 9290 train loss 29262.404296875 val loss 41136.0703125\n",
      "Epoch 9300 train loss 29254.61328125 val loss 41134.3046875\n",
      "Epoch 9310 train loss 29247.0859375 val loss 41146.921875\n",
      "Epoch 9320 train loss 29239.029296875 val loss 41124.84765625\n",
      "Epoch 9330 train loss 29230.720703125 val loss 41139.1484375\n",
      "Epoch 9340 train loss 29222.193359375 val loss 41145.734375\n",
      "Epoch 9350 train loss 29214.00390625 val loss 41123.9609375\n",
      "Epoch 9360 train loss 29205.91015625 val loss 41150.68359375\n",
      "Epoch 9370 train loss 29197.6015625 val loss 41149.734375\n",
      "Epoch 9380 train loss 29188.548828125 val loss 41143.7265625\n",
      "Epoch 9390 train loss 29178.544921875 val loss 41151.44921875\n",
      "Epoch 9400 train loss 29168.041015625 val loss 41158.6328125\n",
      "Epoch 9410 train loss 29157.90234375 val loss 41166.64453125\n",
      "Epoch 9420 train loss 29147.544921875 val loss 41143.859375\n",
      "Epoch 9430 train loss 29135.650390625 val loss 41179.3359375\n",
      "Epoch 9440 train loss 29125.30078125 val loss 41189.80078125\n",
      "Epoch 9450 train loss 29113.794921875 val loss 41201.08984375\n",
      "Epoch 9460 train loss 29102.58203125 val loss 41191.09375\n",
      "Epoch 9470 train loss 29092.484375 val loss 41253.59765625\n",
      "Epoch 9480 train loss 29081.98046875 val loss 41240.45703125\n",
      "Epoch 9490 train loss 29071.86328125 val loss 41259.9453125\n",
      "Epoch 9500 train loss 29061.7265625 val loss 41293.00390625\n",
      "Epoch 9510 train loss 29052.103515625 val loss 41296.8828125\n",
      "Epoch 9520 train loss 29041.775390625 val loss 41294.21875\n",
      "Epoch 9530 train loss 29031.185546875 val loss 41312.73046875\n",
      "Epoch 9540 train loss 29020.962890625 val loss 41340.01171875\n",
      "Epoch 9550 train loss 29010.9921875 val loss 41340.45703125\n",
      "Epoch 9560 train loss 29000.072265625 val loss 41337.3046875\n",
      "Epoch 9570 train loss 28990.439453125 val loss 41374.63671875\n",
      "Epoch 9580 train loss 28980.708984375 val loss 41401.9921875\n",
      "Epoch 9590 train loss 28970.732421875 val loss 41406.41015625\n",
      "Epoch 9600 train loss 28959.884765625 val loss 41372.515625\n",
      "Epoch 9610 train loss 28948.6875 val loss 41389.19140625\n",
      "Epoch 9620 train loss 28939.38671875 val loss 41424.73046875\n",
      "Epoch 9630 train loss 28929.1953125 val loss 41419.59375\n",
      "Epoch 9640 train loss 28918.79296875 val loss 41433.79296875\n",
      "Epoch 9650 train loss 28907.84765625 val loss 41424.48828125\n",
      "Epoch 9660 train loss 28896.962890625 val loss 41389.51953125\n",
      "Epoch 9670 train loss 28885.005859375 val loss 41352.6328125\n",
      "Epoch 9680 train loss 28873.666015625 val loss 41313.7421875\n",
      "Epoch 9690 train loss 28862.078125 val loss 41315.17578125\n",
      "Epoch 9700 train loss 28852.2890625 val loss 41320.921875\n",
      "Epoch 9710 train loss 28841.5546875 val loss 41279.92578125\n",
      "Epoch 9720 train loss 28830.79296875 val loss 41245.76171875\n",
      "Epoch 9730 train loss 28818.3984375 val loss 41213.6796875\n",
      "Epoch 9740 train loss 28807.5859375 val loss 41189.62109375\n",
      "Epoch 9750 train loss 28796.26171875 val loss 41228.78515625\n",
      "Epoch 9760 train loss 28786.6875 val loss 41214.265625\n",
      "Epoch 9770 train loss 28776.4296875 val loss 41179.296875\n",
      "Epoch 9780 train loss 28766.708984375 val loss 41160.546875\n",
      "Epoch 9790 train loss 28756.765625 val loss 41110.91015625\n",
      "Epoch 9800 train loss 28747.162109375 val loss 41121.5546875\n",
      "Epoch 9810 train loss 28737.306640625 val loss 41128.26953125\n",
      "Epoch 9820 train loss 28728.125 val loss 41120.6796875\n",
      "Epoch 9830 train loss 28719.14453125 val loss 41099.6015625\n",
      "Epoch 9840 train loss 28709.298828125 val loss 41103.3125\n",
      "Epoch 9850 train loss 28699.623046875 val loss 41106.125\n",
      "Epoch 9860 train loss 28690.33203125 val loss 41098.26953125\n",
      "Epoch 9870 train loss 28681.306640625 val loss 41084.10546875\n",
      "Epoch 9880 train loss 28672.001953125 val loss 41105.95703125\n",
      "Epoch 9890 train loss 28662.962890625 val loss 41104.9921875\n",
      "Epoch 9900 train loss 28653.6171875 val loss 41094.65234375\n",
      "Epoch 9910 train loss 28643.88671875 val loss 41105.6640625\n",
      "Epoch 9920 train loss 28634.70703125 val loss 41090.015625\n",
      "Epoch 9930 train loss 28624.62109375 val loss 41109.265625\n",
      "Epoch 9940 train loss 28615.01953125 val loss 41082.421875\n",
      "Epoch 9950 train loss 28605.60546875 val loss 41070.84375\n",
      "Epoch 9960 train loss 28594.78125 val loss 41042.67578125\n",
      "Epoch 9970 train loss 28585.33984375 val loss 41035.52734375\n",
      "Epoch 9980 train loss 28574.986328125 val loss 41028.1953125\n",
      "Epoch 9990 train loss 28565.009765625 val loss 41064.54296875\n",
      "Epoch 10000 train loss 28555.701171875 val loss 41083.25390625\n",
      "Epoch 10010 train loss 28545.98046875 val loss 41060.0703125\n",
      "Epoch 10020 train loss 28536.3984375 val loss 41076.3828125\n",
      "Epoch 10030 train loss 28527.263671875 val loss 41070.13671875\n",
      "Epoch 10040 train loss 28518.244140625 val loss 41057.0703125\n",
      "Epoch 10050 train loss 28508.69921875 val loss 41082.34375\n",
      "Epoch 10060 train loss 28498.837890625 val loss 41143.26171875\n",
      "Epoch 10070 train loss 28489.458984375 val loss 41092.40234375\n",
      "Epoch 10080 train loss 28480.166015625 val loss 41113.00390625\n",
      "Epoch 10090 train loss 28470.28515625 val loss 41114.58203125\n",
      "Epoch 10100 train loss 28460.92578125 val loss 41123.46875\n",
      "Epoch 10110 train loss 28450.6484375 val loss 41140.6328125\n",
      "Epoch 10120 train loss 28440.453125 val loss 41157.828125\n",
      "Epoch 10130 train loss 28429.974609375 val loss 41144.01171875\n",
      "Epoch 10140 train loss 28418.322265625 val loss 41159.7421875\n",
      "Epoch 10150 train loss 28407.01171875 val loss 41175.3984375\n",
      "Epoch 10160 train loss 28396.962890625 val loss 41183.8046875\n",
      "Epoch 10170 train loss 28385.173828125 val loss 41176.26171875\n",
      "Epoch 10180 train loss 28374.490234375 val loss 41194.046875\n",
      "Epoch 10190 train loss 28362.029296875 val loss 41184.265625\n",
      "Epoch 10200 train loss 28350.96875 val loss 41190.17578125\n",
      "Epoch 10210 train loss 28339.943359375 val loss 41196.890625\n",
      "Epoch 10220 train loss 28328.271484375 val loss 41220.4140625\n",
      "Epoch 10230 train loss 28317.701171875 val loss 41222.9296875\n",
      "Epoch 10240 train loss 28306.072265625 val loss 41219.4921875\n",
      "Epoch 10250 train loss 28293.361328125 val loss 41238.0859375\n",
      "Epoch 10260 train loss 28278.666015625 val loss 41159.0703125\n",
      "Epoch 10270 train loss 28265.013671875 val loss 41219.80078125\n",
      "Epoch 10280 train loss 28250.66015625 val loss 41242.4375\n",
      "Epoch 10290 train loss 28236.001953125 val loss 41252.91015625\n",
      "Epoch 10300 train loss 28223.142578125 val loss 41300.88671875\n",
      "Epoch 10310 train loss 28210.234375 val loss 41362.2109375\n",
      "Epoch 10320 train loss 28195.169921875 val loss 41431.2890625\n",
      "Epoch 10330 train loss 28181.671875 val loss 41438.52734375\n",
      "Epoch 10340 train loss 28169.9453125 val loss 41469.07421875\n",
      "Epoch 10350 train loss 28156.599609375 val loss 41501.0703125\n",
      "Epoch 10360 train loss 28143.166015625 val loss 41529.98828125\n",
      "Epoch 10370 train loss 28129.740234375 val loss 41588.15625\n",
      "Epoch 10380 train loss 28116.494140625 val loss 41605.453125\n",
      "Epoch 10390 train loss 28103.529296875 val loss 41642.7109375\n",
      "Epoch 10400 train loss 28091.224609375 val loss 41668.15625\n",
      "Epoch 10410 train loss 28077.931640625 val loss 41728.84375\n",
      "Epoch 10420 train loss 28065.056640625 val loss 41750.23828125\n",
      "Epoch 10430 train loss 28053.1953125 val loss 41818.0\n",
      "Epoch 10440 train loss 28041.1328125 val loss 41845.5078125\n",
      "Epoch 10450 train loss 28028.974609375 val loss 41839.1796875\n",
      "Epoch 10460 train loss 28017.42578125 val loss 41922.29296875\n",
      "Epoch 10470 train loss 28006.6640625 val loss 41957.19140625\n",
      "Epoch 10480 train loss 27994.60546875 val loss 41980.9921875\n",
      "Epoch 10490 train loss 27983.2734375 val loss 42009.78515625\n",
      "Epoch 10500 train loss 27972.55078125 val loss 42000.671875\n",
      "Epoch 10510 train loss 27961.595703125 val loss 42028.6484375\n",
      "Epoch 10520 train loss 27950.5234375 val loss 42044.109375\n",
      "Epoch 10530 train loss 27939.79296875 val loss 42064.3515625\n",
      "Epoch 10540 train loss 27929.337890625 val loss 42094.625\n",
      "Epoch 10550 train loss 27918.8515625 val loss 42141.03125\n",
      "Epoch 10560 train loss 27908.970703125 val loss 42119.8828125\n",
      "Epoch 10570 train loss 27898.490234375 val loss 42182.18359375\n",
      "Epoch 10580 train loss 27888.3515625 val loss 42226.078125\n",
      "Epoch 10590 train loss 27878.95703125 val loss 42228.44140625\n",
      "Epoch 10600 train loss 27869.5078125 val loss 42233.99609375\n",
      "Epoch 10610 train loss 27858.76953125 val loss 42275.2578125\n",
      "Epoch 10620 train loss 27849.80078125 val loss 42277.87109375\n",
      "Epoch 10630 train loss 27839.47265625 val loss 42294.35546875\n",
      "Epoch 10640 train loss 27829.1875 val loss 42322.95703125\n",
      "Epoch 10650 train loss 27819.833984375 val loss 42341.16015625\n",
      "Epoch 10660 train loss 27810.6640625 val loss 42362.91015625\n",
      "Epoch 10670 train loss 27801.52734375 val loss 42380.58984375\n",
      "Epoch 10680 train loss 27792.6328125 val loss 42424.30078125\n",
      "Epoch 10690 train loss 27783.27734375 val loss 42501.30859375\n",
      "Epoch 10700 train loss 27775.615234375 val loss 42462.4609375\n",
      "Epoch 10710 train loss 27766.015625 val loss 42499.15234375\n",
      "Epoch 10720 train loss 27756.54296875 val loss 42542.546875\n",
      "Epoch 10730 train loss 27747.828125 val loss 42563.81640625\n",
      "Epoch 10740 train loss 27739.1640625 val loss 42620.4609375\n",
      "Epoch 10750 train loss 27731.189453125 val loss 42624.234375\n",
      "Epoch 10760 train loss 27722.529296875 val loss 42655.38671875\n",
      "Epoch 10770 train loss 27714.470703125 val loss 42693.62109375\n",
      "Epoch 10780 train loss 27706.615234375 val loss 42698.55078125\n",
      "Epoch 10790 train loss 27698.62109375 val loss 42720.14453125\n",
      "Epoch 10800 train loss 27690.328125 val loss 42739.76953125\n",
      "Epoch 10810 train loss 27683.078125 val loss 42771.76953125\n",
      "Epoch 10820 train loss 27674.0 val loss 42827.76953125\n",
      "Epoch 10830 train loss 27666.66796875 val loss 42790.73828125\n",
      "Epoch 10840 train loss 27657.9765625 val loss 42843.4765625\n",
      "Epoch 10850 train loss 27649.732421875 val loss 42910.1953125\n",
      "Epoch 10860 train loss 27641.4921875 val loss 42932.6796875\n",
      "Epoch 10870 train loss 27633.4921875 val loss 42968.49609375\n",
      "Epoch 10880 train loss 27626.5625 val loss 42968.05859375\n",
      "Epoch 10890 train loss 27618.279296875 val loss 43037.05078125\n",
      "Epoch 10900 train loss 27611.556640625 val loss 43069.5703125\n",
      "Epoch 10910 train loss 27603.75 val loss 43125.07421875\n",
      "Epoch 10920 train loss 27597.0703125 val loss 43134.9765625\n",
      "Epoch 10930 train loss 27588.640625 val loss 43144.609375\n",
      "Epoch 10940 train loss 27581.544921875 val loss 43170.51171875\n",
      "Epoch 10950 train loss 27574.767578125 val loss 43210.67578125\n",
      "Epoch 10960 train loss 27568.482421875 val loss 43208.71484375\n",
      "Epoch 10970 train loss 27561.6796875 val loss 43215.671875\n",
      "Epoch 10980 train loss 27555.27734375 val loss 43239.578125\n",
      "Epoch 10990 train loss 27548.1015625 val loss 43295.24609375\n",
      "Epoch 11000 train loss 27541.900390625 val loss 43362.08203125\n",
      "Epoch 11010 train loss 27534.169921875 val loss 43371.76953125\n",
      "Epoch 11020 train loss 27528.53515625 val loss 43377.44140625\n",
      "Epoch 11030 train loss 27522.001953125 val loss 43420.2265625\n",
      "Epoch 11040 train loss 27514.943359375 val loss 43428.1484375\n",
      "Epoch 11050 train loss 27508.2734375 val loss 43469.44921875\n",
      "Epoch 11060 train loss 27501.248046875 val loss 43482.953125\n",
      "Epoch 11070 train loss 27494.51171875 val loss 43521.65625\n",
      "Epoch 11080 train loss 27488.427734375 val loss 43522.9921875\n",
      "Epoch 11090 train loss 27481.533203125 val loss 43592.97265625\n",
      "Epoch 11100 train loss 27475.705078125 val loss 43600.19921875\n",
      "Epoch 11110 train loss 27468.5859375 val loss 43621.2421875\n",
      "Epoch 11120 train loss 27462.859375 val loss 43640.90234375\n",
      "Epoch 11130 train loss 27458.189453125 val loss 43637.40625\n",
      "Epoch 11140 train loss 27450.375 val loss 43667.02734375\n",
      "Epoch 11150 train loss 27444.169921875 val loss 43746.5078125\n",
      "Epoch 11160 train loss 27437.962890625 val loss 43729.4921875\n",
      "Epoch 11170 train loss 27431.740234375 val loss 43780.4375\n",
      "Epoch 11180 train loss 27425.66796875 val loss 43841.50390625\n",
      "Epoch 11190 train loss 27418.505859375 val loss 43819.3984375\n",
      "Epoch 11200 train loss 27411.7109375 val loss 43831.7734375\n",
      "Epoch 11210 train loss 27405.69921875 val loss 43865.265625\n",
      "Epoch 11220 train loss 27398.765625 val loss 43922.640625\n",
      "Epoch 11230 train loss 27392.796875 val loss 43953.81640625\n",
      "Epoch 11240 train loss 27385.9375 val loss 43970.71484375\n",
      "Epoch 11250 train loss 27380.5 val loss 43996.7421875\n",
      "Epoch 11260 train loss 27374.00390625 val loss 44021.35546875\n",
      "Epoch 11270 train loss 27367.623046875 val loss 44039.89453125\n",
      "Epoch 11280 train loss 27361.583984375 val loss 44034.8515625\n",
      "Epoch 11290 train loss 27354.916015625 val loss 44090.1015625\n",
      "Epoch 11300 train loss 27347.583984375 val loss 44131.82421875\n",
      "Epoch 11310 train loss 27340.681640625 val loss 44139.171875\n",
      "Epoch 11320 train loss 27334.91015625 val loss 44141.328125\n",
      "Epoch 11330 train loss 27327.630859375 val loss 44158.8203125\n",
      "Epoch 11340 train loss 27321.595703125 val loss 44199.765625\n",
      "Epoch 11350 train loss 27314.4609375 val loss 44198.59375\n",
      "Epoch 11360 train loss 27308.23828125 val loss 44204.34375\n",
      "Epoch 11370 train loss 27301.279296875 val loss 44245.04296875\n",
      "Epoch 11380 train loss 27292.646484375 val loss 44317.37890625\n",
      "Epoch 11390 train loss 27284.11328125 val loss 44307.33984375\n",
      "Epoch 11400 train loss 27276.28515625 val loss 44316.2109375\n",
      "Epoch 11410 train loss 27266.869140625 val loss 44344.44140625\n",
      "Epoch 11420 train loss 27259.01953125 val loss 44362.09375\n",
      "Epoch 11430 train loss 27250.767578125 val loss 44423.18359375\n",
      "Epoch 11440 train loss 27243.673828125 val loss 44468.71484375\n",
      "Epoch 11450 train loss 27235.521484375 val loss 44510.40234375\n",
      "Epoch 11460 train loss 27228.03515625 val loss 44573.484375\n",
      "Epoch 11470 train loss 27221.099609375 val loss 44614.390625\n",
      "Epoch 11480 train loss 27213.6796875 val loss 44649.55859375\n",
      "Epoch 11490 train loss 27206.8046875 val loss 44728.33984375\n",
      "Epoch 11500 train loss 27197.693359375 val loss 44760.5\n",
      "Epoch 11510 train loss 27191.234375 val loss 44788.65625\n",
      "Epoch 11520 train loss 27181.720703125 val loss 44841.046875\n",
      "Epoch 11530 train loss 27173.541015625 val loss 44886.29296875\n",
      "Epoch 11540 train loss 27163.919921875 val loss 44954.60546875\n",
      "Epoch 11550 train loss 27155.22265625 val loss 44994.06640625\n",
      "Epoch 11560 train loss 27147.7421875 val loss 45058.71875\n",
      "Epoch 11570 train loss 27138.189453125 val loss 45085.6640625\n",
      "Epoch 11580 train loss 27129.64453125 val loss 45138.89453125\n",
      "Epoch 11590 train loss 27121.794921875 val loss 45202.36328125\n",
      "Epoch 11600 train loss 27113.580078125 val loss 45252.10546875\n",
      "Epoch 11610 train loss 27105.90234375 val loss 45260.96484375\n",
      "Epoch 11620 train loss 27096.529296875 val loss 45280.66015625\n",
      "Epoch 11630 train loss 27088.58203125 val loss 45346.546875\n",
      "Epoch 11640 train loss 27079.947265625 val loss 45350.74609375\n",
      "Epoch 11650 train loss 27072.048828125 val loss 45345.3046875\n",
      "Epoch 11660 train loss 27063.140625 val loss 45338.73828125\n",
      "Epoch 11670 train loss 27053.494140625 val loss 45372.88671875\n",
      "Epoch 11680 train loss 27046.705078125 val loss 45445.015625\n",
      "Epoch 11690 train loss 27034.65234375 val loss 45414.7734375\n",
      "Epoch 11700 train loss 27024.4453125 val loss 45392.83984375\n",
      "Epoch 11710 train loss 27014.291015625 val loss 45392.265625\n",
      "Epoch 11720 train loss 27005.515625 val loss 45438.08203125\n",
      "Epoch 11730 train loss 26995.279296875 val loss 45445.609375\n",
      "Epoch 11740 train loss 26984.634765625 val loss 45418.25\n",
      "Epoch 11750 train loss 26973.87109375 val loss 45513.78515625\n",
      "Epoch 11760 train loss 26962.640625 val loss 45545.3984375\n",
      "Epoch 11770 train loss 26952.50390625 val loss 45535.13671875\n",
      "Epoch 11780 train loss 26943.083984375 val loss 45510.6484375\n",
      "Epoch 11790 train loss 26934.103515625 val loss 45522.046875\n",
      "Epoch 11800 train loss 26924.748046875 val loss 45548.07421875\n",
      "Epoch 11810 train loss 26915.837890625 val loss 45530.83203125\n",
      "Epoch 11820 train loss 26907.048828125 val loss 45540.3671875\n",
      "Epoch 11830 train loss 26898.560546875 val loss 45531.38671875\n",
      "Epoch 11840 train loss 26889.587890625 val loss 45532.16015625\n",
      "Epoch 11850 train loss 26880.453125 val loss 45550.2890625\n",
      "Epoch 11860 train loss 26872.70703125 val loss 45583.0546875\n",
      "Epoch 11870 train loss 26862.978515625 val loss 45556.6796875\n",
      "Epoch 11880 train loss 26854.697265625 val loss 45552.37890625\n",
      "Epoch 11890 train loss 26845.849609375 val loss 45524.05859375\n",
      "Epoch 11900 train loss 26833.275390625 val loss 45417.6640625\n",
      "Epoch 11910 train loss 26822.400390625 val loss 45464.9765625\n",
      "Epoch 11920 train loss 26812.09375 val loss 45414.296875\n",
      "Epoch 11930 train loss 26802.7578125 val loss 45428.19140625\n",
      "Epoch 11940 train loss 26794.236328125 val loss 45492.390625\n",
      "Epoch 11950 train loss 26786.197265625 val loss 45436.38671875\n",
      "Epoch 11960 train loss 26775.748046875 val loss 45477.1953125\n",
      "Epoch 11970 train loss 26768.88671875 val loss 45473.8125\n",
      "Epoch 11980 train loss 26758.580078125 val loss 45546.8984375\n",
      "Epoch 11990 train loss 26749.94921875 val loss 45521.46484375\n",
      "Epoch 12000 train loss 26740.7734375 val loss 45539.0234375\n",
      "Epoch 12010 train loss 26732.87109375 val loss 45552.2421875\n",
      "Epoch 12020 train loss 26725.318359375 val loss 45549.4765625\n",
      "Epoch 12030 train loss 26716.51171875 val loss 45612.39453125\n",
      "Epoch 12040 train loss 26710.39453125 val loss 45563.2890625\n",
      "Epoch 12050 train loss 26696.9453125 val loss 45548.88671875\n",
      "Epoch 12060 train loss 26687.5078125 val loss 45577.7265625\n",
      "Epoch 12070 train loss 26678.435546875 val loss 45545.19140625\n",
      "Epoch 12080 train loss 26670.333984375 val loss 45557.77734375\n",
      "Epoch 12090 train loss 26662.09375 val loss 45619.30078125\n",
      "Epoch 12100 train loss 26653.19140625 val loss 45616.19140625\n",
      "Epoch 12110 train loss 26645.1015625 val loss 45604.49609375\n",
      "Epoch 12120 train loss 26636.056640625 val loss 45589.71484375\n",
      "Epoch 12130 train loss 26628.556640625 val loss 45616.71875\n",
      "Epoch 12140 train loss 26621.173828125 val loss 45643.234375\n",
      "Epoch 12150 train loss 26614.474609375 val loss 45581.890625\n",
      "Epoch 12160 train loss 26606.0234375 val loss 45647.63671875\n",
      "Epoch 12170 train loss 26598.126953125 val loss 45642.26953125\n",
      "Epoch 12180 train loss 26592.2265625 val loss 45718.765625\n",
      "Epoch 12190 train loss 26586.21875 val loss 45659.203125\n",
      "Epoch 12200 train loss 26578.287109375 val loss 45708.26953125\n",
      "Epoch 12210 train loss 26569.693359375 val loss 45695.75390625\n",
      "Epoch 12220 train loss 26561.400390625 val loss 45712.0078125\n",
      "Epoch 12230 train loss 26554.73828125 val loss 45751.18359375\n",
      "Epoch 12240 train loss 26546.095703125 val loss 45780.4140625\n",
      "Epoch 12250 train loss 26539.658203125 val loss 45776.30078125\n",
      "Epoch 12260 train loss 26532.1875 val loss 45868.296875\n",
      "Epoch 12270 train loss 26524.759765625 val loss 45844.59765625\n",
      "Epoch 12280 train loss 26517.2734375 val loss 45864.328125\n",
      "Epoch 12290 train loss 26509.849609375 val loss 45857.2109375\n",
      "Epoch 12300 train loss 26504.826171875 val loss 45982.44140625\n",
      "Epoch 12310 train loss 26495.779296875 val loss 45946.234375\n",
      "Epoch 12320 train loss 26489.953125 val loss 45982.7421875\n",
      "Epoch 12330 train loss 26482.314453125 val loss 45934.95703125\n",
      "Epoch 12340 train loss 26471.818359375 val loss 45969.1953125\n",
      "Epoch 12350 train loss 26465.66796875 val loss 45998.09765625\n",
      "Epoch 12360 train loss 26453.6875 val loss 45937.08203125\n",
      "Epoch 12370 train loss 26444.7578125 val loss 45914.89453125\n",
      "Epoch 12380 train loss 26435.0546875 val loss 45898.52734375\n",
      "Epoch 12390 train loss 26424.98046875 val loss 45830.7734375\n",
      "Epoch 12400 train loss 26415.900390625 val loss 45872.04296875\n",
      "Epoch 12410 train loss 26407.1328125 val loss 45823.12109375\n",
      "Epoch 12420 train loss 26398.58984375 val loss 45868.76953125\n",
      "Epoch 12430 train loss 26391.572265625 val loss 45801.37109375\n",
      "Epoch 12440 train loss 26381.39453125 val loss 45812.7734375\n",
      "Epoch 12450 train loss 26373.05859375 val loss 45739.10546875\n",
      "Epoch 12460 train loss 26365.283203125 val loss 45799.546875\n",
      "Epoch 12470 train loss 26356.205078125 val loss 45797.23828125\n",
      "Epoch 12480 train loss 26346.857421875 val loss 45851.01953125\n",
      "Epoch 12490 train loss 26334.921875 val loss 45965.95703125\n",
      "Epoch 12500 train loss 26321.6015625 val loss 45978.6953125\n",
      "Epoch 12510 train loss 26308.166015625 val loss 46046.42578125\n",
      "Epoch 12520 train loss 26297.486328125 val loss 46115.3828125\n",
      "Epoch 12530 train loss 26285.447265625 val loss 46156.7421875\n",
      "Epoch 12540 train loss 26278.189453125 val loss 46153.19921875\n",
      "Epoch 12550 train loss 26263.271484375 val loss 46242.54296875\n",
      "Epoch 12560 train loss 26252.08984375 val loss 46260.5390625\n",
      "Epoch 12570 train loss 26242.71875 val loss 46251.1875\n",
      "Epoch 12580 train loss 26233.09375 val loss 46316.35546875\n",
      "Epoch 12590 train loss 26222.541015625 val loss 46336.0\n",
      "Epoch 12600 train loss 26212.7890625 val loss 46353.3046875\n",
      "Epoch 12610 train loss 26202.654296875 val loss 46367.31640625\n",
      "Epoch 12620 train loss 26192.6875 val loss 46463.88671875\n",
      "Epoch 12630 train loss 26182.47265625 val loss 46453.4453125\n",
      "Epoch 12640 train loss 26172.16015625 val loss 46521.07421875\n",
      "Epoch 12650 train loss 26163.0859375 val loss 46451.0546875\n",
      "Epoch 12660 train loss 26153.95703125 val loss 46548.984375\n",
      "Epoch 12670 train loss 26143.837890625 val loss 46534.37890625\n",
      "Epoch 12680 train loss 26133.8515625 val loss 46541.7109375\n",
      "Epoch 12690 train loss 26124.080078125 val loss 46541.9921875\n",
      "Epoch 12700 train loss 26114.6328125 val loss 46546.01171875\n",
      "Epoch 12710 train loss 26105.509765625 val loss 46585.01171875\n",
      "Epoch 12720 train loss 26095.181640625 val loss 46499.81640625\n",
      "Epoch 12730 train loss 26085.703125 val loss 46540.50390625\n",
      "Epoch 12740 train loss 26077.611328125 val loss 46528.4609375\n",
      "Epoch 12750 train loss 26069.734375 val loss 46592.1796875\n",
      "Epoch 12760 train loss 26062.04296875 val loss 46561.24609375\n",
      "Epoch 12770 train loss 26053.275390625 val loss 46624.31640625\n",
      "Epoch 12780 train loss 26044.11328125 val loss 46586.01171875\n",
      "Epoch 12790 train loss 26036.05859375 val loss 46644.4453125\n",
      "Epoch 12800 train loss 26026.958984375 val loss 46631.58984375\n",
      "Epoch 12810 train loss 26019.14453125 val loss 46612.53515625\n",
      "Epoch 12820 train loss 26010.9296875 val loss 46654.875\n",
      "Epoch 12830 train loss 26002.5546875 val loss 46678.3125\n",
      "Epoch 12840 train loss 25994.4375 val loss 46717.9140625\n",
      "Epoch 12850 train loss 25986.048828125 val loss 46677.51953125\n",
      "Epoch 12860 train loss 25977.1875 val loss 46744.2578125\n",
      "Epoch 12870 train loss 25968.869140625 val loss 46718.03125\n",
      "Epoch 12880 train loss 25960.26171875 val loss 46784.16015625\n",
      "Epoch 12890 train loss 25951.787109375 val loss 46750.859375\n",
      "Epoch 12900 train loss 25942.85546875 val loss 46774.77734375\n",
      "Epoch 12910 train loss 25932.57421875 val loss 46800.24609375\n",
      "Epoch 12920 train loss 25922.337890625 val loss 46812.0234375\n",
      "Epoch 12930 train loss 25911.623046875 val loss 46737.90234375\n",
      "Epoch 12940 train loss 25900.615234375 val loss 46745.09765625\n",
      "Epoch 12950 train loss 25890.150390625 val loss 46755.56640625\n",
      "Epoch 12960 train loss 25882.669921875 val loss 46778.3359375\n",
      "Epoch 12970 train loss 25871.6484375 val loss 46720.62890625\n",
      "Epoch 12980 train loss 25861.53515625 val loss 46759.671875\n",
      "Epoch 12990 train loss 25852.70703125 val loss 46750.421875\n",
      "Epoch 13000 train loss 25841.513671875 val loss 46709.6171875\n",
      "Epoch 13010 train loss 25830.7265625 val loss 46697.0234375\n",
      "Epoch 13020 train loss 25817.205078125 val loss 46573.61328125\n",
      "Epoch 13030 train loss 25805.255859375 val loss 46623.55078125\n",
      "Epoch 13040 train loss 25794.4375 val loss 46673.09375\n",
      "Epoch 13050 train loss 25784.91015625 val loss 46658.6953125\n",
      "Epoch 13060 train loss 25773.69921875 val loss 46675.34765625\n",
      "Epoch 13070 train loss 25761.798828125 val loss 46716.1640625\n",
      "Epoch 13080 train loss 25751.341796875 val loss 46758.265625\n",
      "Epoch 13090 train loss 25741.759765625 val loss 46787.5625\n",
      "Epoch 13100 train loss 25732.08203125 val loss 46743.05078125\n",
      "Epoch 13110 train loss 25718.87890625 val loss 46845.06640625\n",
      "Epoch 13120 train loss 25706.912109375 val loss 46874.2265625\n",
      "Epoch 13130 train loss 25701.556640625 val loss 46851.44140625\n",
      "Epoch 13140 train loss 25686.564453125 val loss 46902.65234375\n",
      "Epoch 13150 train loss 25674.734375 val loss 46965.39453125\n",
      "Epoch 13160 train loss 25663.552734375 val loss 46945.609375\n",
      "Epoch 13170 train loss 25654.8359375 val loss 46929.0390625\n",
      "Epoch 13180 train loss 25641.51953125 val loss 46996.1484375\n",
      "Epoch 13190 train loss 25628.9140625 val loss 47001.37890625\n",
      "Epoch 13200 train loss 25617.833984375 val loss 47051.3984375\n",
      "Epoch 13210 train loss 25607.60546875 val loss 47079.28515625\n",
      "Epoch 13220 train loss 25597.767578125 val loss 47070.96484375\n",
      "Epoch 13230 train loss 25586.470703125 val loss 47146.765625\n",
      "Epoch 13240 train loss 25576.890625 val loss 47141.75\n",
      "Epoch 13250 train loss 25565.064453125 val loss 47236.44921875\n",
      "Epoch 13260 train loss 25551.21875 val loss 47208.62109375\n",
      "Epoch 13270 train loss 25539.052734375 val loss 47155.50390625\n",
      "Epoch 13280 train loss 25527.451171875 val loss 47169.72265625\n",
      "Epoch 13290 train loss 25519.162109375 val loss 47310.0625\n",
      "Epoch 13300 train loss 25506.78125 val loss 47216.8515625\n",
      "Epoch 13310 train loss 25496.767578125 val loss 47339.7578125\n",
      "Epoch 13320 train loss 25479.994140625 val loss 47287.73828125\n",
      "Epoch 13330 train loss 25470.4453125 val loss 47249.52734375\n",
      "Epoch 13340 train loss 25456.92578125 val loss 47312.06640625\n",
      "Epoch 13350 train loss 25444.564453125 val loss 47269.0625\n",
      "Epoch 13360 train loss 25433.373046875 val loss 47251.2578125\n",
      "Epoch 13370 train loss 25420.294921875 val loss 47171.4609375\n",
      "Epoch 13380 train loss 25408.015625 val loss 47229.296875\n",
      "Epoch 13390 train loss 25395.7734375 val loss 47148.6796875\n",
      "Epoch 13400 train loss 25384.099609375 val loss 47201.07421875\n",
      "Epoch 13410 train loss 25375.634765625 val loss 47105.8984375\n",
      "Epoch 13420 train loss 25358.0625 val loss 47054.16796875\n",
      "Epoch 13430 train loss 25346.326171875 val loss 47121.70703125\n",
      "Epoch 13440 train loss 25331.06640625 val loss 47060.7890625\n",
      "Epoch 13450 train loss 25318.30078125 val loss 47033.01171875\n",
      "Epoch 13460 train loss 25303.84375 val loss 47021.203125\n",
      "Epoch 13470 train loss 25291.263671875 val loss 46962.68359375\n",
      "Epoch 13480 train loss 25275.39453125 val loss 46976.46875\n",
      "Epoch 13490 train loss 25260.908203125 val loss 46966.07421875\n",
      "Epoch 13500 train loss 25245.939453125 val loss 46951.703125\n",
      "Epoch 13510 train loss 25232.1875 val loss 46905.60546875\n",
      "Epoch 13520 train loss 25220.330078125 val loss 46966.76171875\n",
      "Epoch 13530 train loss 25202.580078125 val loss 46833.3046875\n",
      "Epoch 13540 train loss 25189.265625 val loss 46893.54296875\n",
      "Epoch 13550 train loss 25173.392578125 val loss 46822.1171875\n",
      "Epoch 13560 train loss 25160.017578125 val loss 46835.0\n",
      "Epoch 13570 train loss 25143.935546875 val loss 46773.87109375\n",
      "Epoch 13580 train loss 25136.080078125 val loss 46735.57421875\n",
      "Epoch 13590 train loss 25123.71875 val loss 46828.0\n",
      "Epoch 13600 train loss 25100.474609375 val loss 46760.109375\n",
      "Epoch 13610 train loss 25086.150390625 val loss 46704.32421875\n",
      "Epoch 13620 train loss 25070.15625 val loss 46667.58203125\n",
      "Epoch 13630 train loss 25055.93359375 val loss 46642.40234375\n",
      "Epoch 13640 train loss 25044.353515625 val loss 46562.73046875\n",
      "Epoch 13650 train loss 25030.919921875 val loss 46571.53515625\n",
      "Epoch 13660 train loss 25017.076171875 val loss 46497.7890625\n",
      "Epoch 13670 train loss 25001.7109375 val loss 46476.3671875\n",
      "Epoch 13680 train loss 24990.82421875 val loss 46404.8828125\n",
      "Epoch 13690 train loss 24975.130859375 val loss 46380.828125\n",
      "Epoch 13700 train loss 24960.685546875 val loss 46359.90234375\n",
      "Epoch 13710 train loss 24952.255859375 val loss 46334.7109375\n",
      "Epoch 13720 train loss 24932.375 val loss 46279.82421875\n",
      "Epoch 13730 train loss 24921.330078125 val loss 46147.08984375\n",
      "Epoch 13740 train loss 24905.01953125 val loss 46164.7734375\n",
      "Epoch 13750 train loss 24890.9921875 val loss 46110.5703125\n",
      "Epoch 13760 train loss 24878.021484375 val loss 46133.6171875\n",
      "Epoch 13770 train loss 24863.26953125 val loss 45982.875\n",
      "Epoch 13780 train loss 24850.6484375 val loss 45898.4453125\n",
      "Epoch 13790 train loss 24836.75390625 val loss 45947.25\n",
      "Epoch 13800 train loss 24821.7578125 val loss 45850.2421875\n",
      "Epoch 13810 train loss 24808.033203125 val loss 45836.671875\n",
      "Epoch 13820 train loss 24796.90625 val loss 45827.0859375\n",
      "Epoch 13830 train loss 24781.095703125 val loss 45743.88671875\n",
      "Epoch 13840 train loss 24768.572265625 val loss 45700.9296875\n",
      "Epoch 13850 train loss 24753.865234375 val loss 45646.12109375\n",
      "Epoch 13860 train loss 24743.48046875 val loss 45573.4140625\n",
      "Epoch 13870 train loss 24728.376953125 val loss 45583.5390625\n",
      "Epoch 13880 train loss 24715.63671875 val loss 45512.87109375\n",
      "Epoch 13890 train loss 24701.8203125 val loss 45503.11328125\n",
      "Epoch 13900 train loss 24688.20703125 val loss 45439.3671875\n",
      "Epoch 13910 train loss 24673.14453125 val loss 45368.9375\n",
      "Epoch 13920 train loss 24660.287109375 val loss 45269.734375\n",
      "Epoch 13930 train loss 24660.083984375 val loss 45135.59375\n",
      "Epoch 13940 train loss 24640.71875 val loss 45239.15234375\n",
      "Epoch 13950 train loss 24621.26953125 val loss 45180.8828125\n",
      "Epoch 13960 train loss 24607.150390625 val loss 45093.04296875\n",
      "Epoch 13970 train loss 24595.0234375 val loss 45021.26953125\n",
      "Epoch 13980 train loss 24581.794921875 val loss 44960.73828125\n",
      "Epoch 13990 train loss 24569.037109375 val loss 44909.79296875\n",
      "Epoch 14000 train loss 24555.37890625 val loss 44897.8203125\n",
      "Epoch 14010 train loss 24542.166015625 val loss 44864.60546875\n",
      "Epoch 14020 train loss 24530.443359375 val loss 44782.12890625\n",
      "Epoch 14030 train loss 24516.728515625 val loss 44743.8671875\n",
      "Epoch 14040 train loss 24504.126953125 val loss 44741.7265625\n",
      "Epoch 14050 train loss 24490.8046875 val loss 44692.5703125\n",
      "Epoch 14060 train loss 24483.818359375 val loss 44713.9296875\n",
      "Epoch 14070 train loss 24466.33984375 val loss 44647.875\n",
      "Epoch 14080 train loss 24454.015625 val loss 44553.78125\n",
      "Epoch 14090 train loss 24440.64453125 val loss 44586.2109375\n",
      "Epoch 14100 train loss 24430.91796875 val loss 44557.22265625\n",
      "Epoch 14110 train loss 24419.48828125 val loss 44431.96875\n",
      "Epoch 14120 train loss 24403.162109375 val loss 44416.10546875\n",
      "Epoch 14130 train loss 24391.05078125 val loss 44422.11328125\n",
      "Epoch 14140 train loss 24379.435546875 val loss 44436.70703125\n",
      "Epoch 14150 train loss 24373.673828125 val loss 44408.359375\n",
      "Epoch 14160 train loss 24358.0625 val loss 44269.70703125\n",
      "Epoch 14170 train loss 24339.5859375 val loss 44289.85546875\n",
      "Epoch 14180 train loss 24326.001953125 val loss 44307.01953125\n",
      "Epoch 14190 train loss 24316.73828125 val loss 44193.17578125\n",
      "Epoch 14200 train loss 24300.224609375 val loss 44213.5546875\n",
      "Epoch 14210 train loss 24286.33984375 val loss 44197.86328125\n",
      "Epoch 14220 train loss 24274.404296875 val loss 44231.12890625\n",
      "Epoch 14230 train loss 24261.826171875 val loss 44179.73828125\n",
      "Epoch 14240 train loss 24247.986328125 val loss 44173.42578125\n",
      "Epoch 14250 train loss 24234.0546875 val loss 44104.88671875\n",
      "Epoch 14260 train loss 24220.251953125 val loss 44086.91015625\n",
      "Epoch 14270 train loss 24210.087890625 val loss 44106.9765625\n",
      "Epoch 14280 train loss 24199.486328125 val loss 43970.26171875\n",
      "Epoch 14290 train loss 24185.6640625 val loss 44057.05078125\n",
      "Epoch 14300 train loss 24169.982421875 val loss 43956.640625\n",
      "Epoch 14310 train loss 24156.595703125 val loss 43937.6796875\n",
      "Epoch 14320 train loss 24146.87109375 val loss 43918.80078125\n",
      "Epoch 14330 train loss 24132.78125 val loss 43931.64453125\n",
      "Epoch 14340 train loss 24121.908203125 val loss 43881.43359375\n",
      "Epoch 14350 train loss 24109.796875 val loss 43870.9609375\n",
      "Epoch 14360 train loss 24097.330078125 val loss 43839.4921875\n",
      "Epoch 14370 train loss 24089.97265625 val loss 43791.16796875\n",
      "Epoch 14380 train loss 24077.564453125 val loss 43854.41796875\n",
      "Epoch 14390 train loss 24063.4140625 val loss 43771.8671875\n",
      "Epoch 14400 train loss 24049.869140625 val loss 43789.92578125\n",
      "Epoch 14410 train loss 24039.48046875 val loss 43723.98046875\n",
      "Epoch 14420 train loss 24030.828125 val loss 43754.01171875\n",
      "Epoch 14430 train loss 24017.18359375 val loss 43689.69140625\n",
      "Epoch 14440 train loss 24007.423828125 val loss 43656.23828125\n",
      "Epoch 14450 train loss 23995.029296875 val loss 43656.109375\n",
      "Epoch 14460 train loss 23987.349609375 val loss 43651.89453125\n",
      "Epoch 14470 train loss 23973.845703125 val loss 43584.3046875\n",
      "Epoch 14480 train loss 23963.76953125 val loss 43552.12109375\n",
      "Epoch 14490 train loss 23961.38671875 val loss 43619.375\n",
      "Epoch 14500 train loss 23944.697265625 val loss 43462.578125\n",
      "Epoch 14510 train loss 23933.205078125 val loss 43510.0078125\n",
      "Epoch 14520 train loss 23922.296875 val loss 43399.63671875\n",
      "Epoch 14530 train loss 23910.541015625 val loss 43373.5\n",
      "Epoch 14540 train loss 23898.318359375 val loss 43393.9140625\n",
      "Epoch 14550 train loss 23887.654296875 val loss 43374.83203125\n",
      "Epoch 14560 train loss 23877.734375 val loss 43287.58984375\n",
      "Epoch 14570 train loss 23865.333984375 val loss 43280.14453125\n",
      "Epoch 14580 train loss 23853.80859375 val loss 43266.203125\n",
      "Epoch 14590 train loss 23842.123046875 val loss 43225.8828125\n",
      "Epoch 14600 train loss 23833.84375 val loss 43155.59375\n",
      "Epoch 14610 train loss 23821.265625 val loss 43213.51953125\n",
      "Epoch 14620 train loss 23808.873046875 val loss 43134.83203125\n",
      "Epoch 14630 train loss 23794.740234375 val loss 43147.24609375\n",
      "Epoch 14640 train loss 23783.279296875 val loss 43121.5390625\n",
      "Epoch 14650 train loss 23772.826171875 val loss 43143.40234375\n",
      "Epoch 14660 train loss 23761.9375 val loss 43103.0234375\n",
      "Epoch 14670 train loss 23751.814453125 val loss 43069.22265625\n",
      "Epoch 14680 train loss 23737.392578125 val loss 43039.0546875\n",
      "Epoch 14690 train loss 23725.5546875 val loss 43040.28515625\n",
      "Epoch 14700 train loss 23709.314453125 val loss 43034.546875\n",
      "Epoch 14710 train loss 23697.60546875 val loss 42998.36328125\n",
      "Epoch 14720 train loss 23682.974609375 val loss 42989.05859375\n",
      "Epoch 14730 train loss 23672.169921875 val loss 42842.82421875\n",
      "Epoch 14740 train loss 23651.8359375 val loss 42892.421875\n",
      "Epoch 14750 train loss 23636.681640625 val loss 42804.42578125\n",
      "Epoch 14760 train loss 23618.48046875 val loss 42795.07421875\n",
      "Epoch 14770 train loss 23600.951171875 val loss 42795.66015625\n",
      "Epoch 14780 train loss 23580.71875 val loss 42722.08984375\n",
      "Epoch 14790 train loss 23566.412109375 val loss 42804.32421875\n",
      "Epoch 14800 train loss 23546.490234375 val loss 42664.921875\n",
      "Epoch 14810 train loss 23530.05859375 val loss 42771.6171875\n",
      "Epoch 14820 train loss 23515.40234375 val loss 42709.65625\n",
      "Epoch 14830 train loss 23504.005859375 val loss 42725.765625\n",
      "Epoch 14840 train loss 23488.173828125 val loss 42728.484375\n",
      "Epoch 14850 train loss 23471.61328125 val loss 42647.9453125\n",
      "Epoch 14860 train loss 23455.892578125 val loss 42557.703125\n",
      "Epoch 14870 train loss 23439.8203125 val loss 42550.01171875\n",
      "Epoch 14880 train loss 23424.525390625 val loss 42537.53125\n",
      "Epoch 14890 train loss 23419.00390625 val loss 42552.01171875\n",
      "Epoch 14900 train loss 23399.6796875 val loss 42380.24609375\n",
      "Epoch 14910 train loss 23384.44140625 val loss 42471.17578125\n",
      "Epoch 14920 train loss 23371.177734375 val loss 42409.08984375\n",
      "Epoch 14930 train loss 23363.552734375 val loss 42406.75390625\n",
      "Epoch 14940 train loss 23346.0859375 val loss 42396.90234375\n",
      "Epoch 14950 train loss 23331.646484375 val loss 42374.1171875\n",
      "Epoch 14960 train loss 23317.591796875 val loss 42329.796875\n",
      "Epoch 14970 train loss 23304.392578125 val loss 42325.6015625\n",
      "Epoch 14980 train loss 23288.81640625 val loss 42434.3359375\n",
      "Epoch 14990 train loss 23274.39453125 val loss 42412.06640625\n",
      "Epoch 15000 train loss 23264.865234375 val loss 42489.09765625\n",
      "Epoch 15010 train loss 23246.576171875 val loss 42393.41015625\n",
      "Epoch 15020 train loss 23232.37109375 val loss 42414.73828125\n",
      "Epoch 15030 train loss 23220.62890625 val loss 42430.1171875\n",
      "Epoch 15040 train loss 23206.73046875 val loss 42420.3828125\n",
      "Epoch 15050 train loss 23198.16015625 val loss 42460.359375\n",
      "Epoch 15060 train loss 23179.54296875 val loss 42425.6328125\n",
      "Epoch 15070 train loss 23168.451171875 val loss 42406.4375\n",
      "Epoch 15080 train loss 23154.025390625 val loss 42410.64453125\n",
      "Epoch 15090 train loss 23145.181640625 val loss 42421.03515625\n",
      "Epoch 15100 train loss 23133.421875 val loss 42329.00390625\n",
      "Epoch 15110 train loss 23120.001953125 val loss 42388.90234375\n",
      "Epoch 15120 train loss 23110.2734375 val loss 42407.49609375\n",
      "Epoch 15130 train loss 23097.111328125 val loss 42366.95703125\n",
      "Epoch 15140 train loss 23094.2578125 val loss 42444.22265625\n",
      "Epoch 15150 train loss 23081.107421875 val loss 42328.7109375\n",
      "Epoch 15160 train loss 23071.7890625 val loss 42420.4375\n",
      "Epoch 15170 train loss 23053.263671875 val loss 42364.1328125\n",
      "Epoch 15180 train loss 23041.884765625 val loss 42430.85546875\n",
      "Epoch 15190 train loss 23031.853515625 val loss 42376.8359375\n",
      "Epoch 15200 train loss 23021.642578125 val loss 42402.41796875\n",
      "Epoch 15210 train loss 23010.99609375 val loss 42416.046875\n",
      "Epoch 15220 train loss 23001.41796875 val loss 42346.64453125\n",
      "Epoch 15230 train loss 22997.263671875 val loss 42348.97265625\n",
      "Epoch 15240 train loss 22986.453125 val loss 42442.8125\n",
      "Epoch 15250 train loss 22971.310546875 val loss 42341.8359375\n",
      "Epoch 15260 train loss 22961.4765625 val loss 42382.421875\n",
      "Epoch 15270 train loss 22954.935546875 val loss 42395.05078125\n",
      "Epoch 15280 train loss 22941.583984375 val loss 42360.81640625\n",
      "Epoch 15290 train loss 22933.599609375 val loss 42321.49609375\n",
      "Epoch 15300 train loss 22924.072265625 val loss 42407.33203125\n",
      "Epoch 15310 train loss 22912.306640625 val loss 42322.04296875\n",
      "Epoch 15320 train loss 22905.197265625 val loss 42318.54296875\n",
      "Epoch 15330 train loss 22894.654296875 val loss 42432.91015625\n",
      "Epoch 15340 train loss 22882.45703125 val loss 42360.0703125\n",
      "Epoch 15350 train loss 22872.75390625 val loss 42391.45703125\n",
      "Epoch 15360 train loss 22872.060546875 val loss 42434.2734375\n",
      "Epoch 15370 train loss 22857.994140625 val loss 42315.8125\n",
      "Epoch 15380 train loss 22844.12109375 val loss 42357.7265625\n",
      "Epoch 15390 train loss 22835.279296875 val loss 42272.94921875\n",
      "Epoch 15400 train loss 22834.06640625 val loss 42282.36328125\n",
      "Epoch 15410 train loss 22818.91015625 val loss 42389.50390625\n",
      "Epoch 15420 train loss 22807.287109375 val loss 42289.29296875\n",
      "Epoch 15430 train loss 22795.423828125 val loss 42329.68359375\n",
      "Epoch 15440 train loss 22786.771484375 val loss 42324.546875\n",
      "Epoch 15450 train loss 22782.8125 val loss 42222.63671875\n",
      "Epoch 15460 train loss 22766.462890625 val loss 42288.96875\n",
      "Epoch 15470 train loss 22761.53515625 val loss 42296.6328125\n",
      "Epoch 15480 train loss 22747.359375 val loss 42274.66015625\n",
      "Epoch 15490 train loss 22742.166015625 val loss 42158.1484375\n",
      "Epoch 15500 train loss 22729.845703125 val loss 42244.59375\n",
      "Epoch 15510 train loss 22721.357421875 val loss 42248.046875\n",
      "Epoch 15520 train loss 22722.609375 val loss 42288.625\n",
      "Epoch 15530 train loss 22706.556640625 val loss 42155.43359375\n",
      "Epoch 15540 train loss 22693.427734375 val loss 42256.22265625\n",
      "Epoch 15550 train loss 22680.638671875 val loss 42199.03125\n",
      "Epoch 15560 train loss 22671.060546875 val loss 42202.99609375\n",
      "Epoch 15570 train loss 22664.90625 val loss 42140.29296875\n",
      "Epoch 15580 train loss 22651.724609375 val loss 42207.90625\n",
      "Epoch 15590 train loss 22642.5859375 val loss 42183.77734375\n",
      "Epoch 15600 train loss 22634.76171875 val loss 42183.7265625\n",
      "Epoch 15610 train loss 22625.09765625 val loss 42180.4296875\n",
      "Epoch 15620 train loss 22617.443359375 val loss 42138.6484375\n",
      "Epoch 15630 train loss 22608.150390625 val loss 42147.37109375\n",
      "Epoch 15640 train loss 22603.716796875 val loss 42145.91796875\n",
      "Epoch 15650 train loss 22589.833984375 val loss 42197.67578125\n",
      "Epoch 15660 train loss 22587.685546875 val loss 42217.24609375\n",
      "Epoch 15670 train loss 22578.125 val loss 42137.83203125\n",
      "Epoch 15680 train loss 22563.58203125 val loss 42193.46875\n",
      "Epoch 15690 train loss 22554.96484375 val loss 42182.9375\n",
      "Epoch 15700 train loss 22547.08984375 val loss 42138.92578125\n",
      "Epoch 15710 train loss 22539.40234375 val loss 42164.6640625\n",
      "Epoch 15720 train loss 22534.7109375 val loss 42252.80859375\n",
      "Epoch 15730 train loss 22521.33984375 val loss 42204.671875\n",
      "Epoch 15740 train loss 22512.59375 val loss 42154.03125\n",
      "Epoch 15750 train loss 22504.50390625 val loss 42214.19921875\n",
      "Epoch 15760 train loss 22496.751953125 val loss 42184.77734375\n",
      "Epoch 15770 train loss 22486.6171875 val loss 42234.90625\n",
      "Epoch 15780 train loss 22483.97265625 val loss 42292.96875\n",
      "Epoch 15790 train loss 22471.08984375 val loss 42239.6484375\n",
      "Epoch 15800 train loss 22465.548828125 val loss 42207.7421875\n",
      "Epoch 15810 train loss 22452.51953125 val loss 42234.91015625\n",
      "Epoch 15820 train loss 22446.01171875 val loss 42216.953125\n",
      "Epoch 15830 train loss 22435.8984375 val loss 42276.91796875\n",
      "Epoch 15840 train loss 22413.125 val loss 42337.28515625\n",
      "Epoch 15850 train loss 22399.08984375 val loss 42451.7421875\n",
      "Epoch 15860 train loss 22371.94140625 val loss 42314.4921875\n",
      "Epoch 15870 train loss 22352.125 val loss 42482.9453125\n",
      "Epoch 15880 train loss 22330.962890625 val loss 42387.26953125\n",
      "Epoch 15890 train loss 22313.060546875 val loss 42473.0234375\n",
      "Epoch 15900 train loss 22298.451171875 val loss 42480.140625\n",
      "Epoch 15910 train loss 22283.744140625 val loss 42517.7421875\n",
      "Epoch 15920 train loss 22277.833984375 val loss 42449.09765625\n",
      "Epoch 15930 train loss 22257.666015625 val loss 42527.88671875\n",
      "Epoch 15940 train loss 22242.033203125 val loss 42582.5234375\n",
      "Epoch 15950 train loss 22216.5546875 val loss 42451.07421875\n",
      "Epoch 15960 train loss 22195.998046875 val loss 42362.91796875\n",
      "Epoch 15970 train loss 22180.44921875 val loss 42334.2734375\n",
      "Epoch 15980 train loss 22156.875 val loss 42413.45703125\n",
      "Epoch 15990 train loss 22138.18359375 val loss 42471.0703125\n",
      "Epoch 16000 train loss 22122.92578125 val loss 42408.6484375\n",
      "Epoch 16010 train loss 22092.400390625 val loss 42525.53515625\n",
      "Epoch 16020 train loss 22078.455078125 val loss 42566.859375\n",
      "Epoch 16030 train loss 22050.126953125 val loss 42474.6171875\n",
      "Epoch 16040 train loss 22037.25 val loss 42452.96484375\n",
      "Epoch 16050 train loss 22014.37890625 val loss 42562.796875\n",
      "Epoch 16060 train loss 21991.802734375 val loss 42560.64453125\n",
      "Epoch 16070 train loss 21969.498046875 val loss 42547.12109375\n",
      "Epoch 16080 train loss 21951.6328125 val loss 42438.078125\n",
      "Epoch 16090 train loss 21923.755859375 val loss 42592.01171875\n",
      "Epoch 16100 train loss 21901.8203125 val loss 42571.5546875\n",
      "Epoch 16110 train loss 21884.015625 val loss 42574.96875\n",
      "Epoch 16120 train loss 21855.517578125 val loss 42603.546875\n",
      "Epoch 16130 train loss 21832.95703125 val loss 42582.53125\n",
      "Epoch 16140 train loss 21813.029296875 val loss 42568.46484375\n",
      "Epoch 16150 train loss 21793.25 val loss 42717.3359375\n",
      "Epoch 16160 train loss 21776.86328125 val loss 42762.00390625\n",
      "Epoch 16170 train loss 21752.50390625 val loss 42773.95703125\n",
      "Epoch 16180 train loss 21736.01171875 val loss 42837.9296875\n",
      "Epoch 16190 train loss 21717.326171875 val loss 42815.33984375\n",
      "Epoch 16200 train loss 21700.6015625 val loss 42857.97265625\n",
      "Epoch 16210 train loss 21687.169921875 val loss 43007.1484375\n",
      "Epoch 16220 train loss 21667.99609375 val loss 42995.65625\n",
      "Epoch 16230 train loss 21645.66796875 val loss 42950.2890625\n",
      "Epoch 16240 train loss 21638.328125 val loss 42891.79296875\n",
      "Epoch 16250 train loss 21608.947265625 val loss 43059.859375\n",
      "Epoch 16260 train loss 21586.578125 val loss 42984.83203125\n",
      "Epoch 16270 train loss 21566.46484375 val loss 43055.6015625\n",
      "Epoch 16280 train loss 21552.921875 val loss 43142.203125\n",
      "Epoch 16290 train loss 21533.10546875 val loss 43137.921875\n",
      "Epoch 16300 train loss 21516.134765625 val loss 43121.58984375\n",
      "Epoch 16310 train loss 21505.6171875 val loss 43159.15234375\n",
      "Epoch 16320 train loss 21487.30859375 val loss 43309.97265625\n",
      "Epoch 16330 train loss 21466.087890625 val loss 43271.1796875\n",
      "Epoch 16340 train loss 21449.455078125 val loss 43400.72265625\n",
      "Epoch 16350 train loss 21432.177734375 val loss 43349.484375\n",
      "Epoch 16360 train loss 21411.46875 val loss 43441.66015625\n",
      "Epoch 16370 train loss 21399.427734375 val loss 43543.0390625\n",
      "Epoch 16380 train loss 21381.65625 val loss 43532.7265625\n",
      "Epoch 16390 train loss 21367.09765625 val loss 43558.28515625\n",
      "Epoch 16400 train loss 21353.5234375 val loss 43684.44921875\n",
      "Epoch 16410 train loss 21342.064453125 val loss 43712.62109375\n",
      "Epoch 16420 train loss 21321.822265625 val loss 43690.78125\n",
      "Epoch 16430 train loss 21315.66015625 val loss 43800.51953125\n",
      "Epoch 16440 train loss 21296.556640625 val loss 43730.3359375\n",
      "Epoch 16450 train loss 21278.029296875 val loss 43816.13671875\n",
      "Epoch 16460 train loss 21257.982421875 val loss 43788.15234375\n",
      "Epoch 16470 train loss 21249.52734375 val loss 43791.71484375\n",
      "Epoch 16480 train loss 21228.265625 val loss 43870.8828125\n",
      "Epoch 16490 train loss 21217.30078125 val loss 43979.80078125\n",
      "Epoch 16500 train loss 21201.1015625 val loss 44014.9765625\n",
      "Epoch 16510 train loss 21190.67578125 val loss 44126.5234375\n",
      "Epoch 16520 train loss 21167.53125 val loss 44049.43359375\n",
      "Epoch 16530 train loss 21154.697265625 val loss 44101.20703125\n",
      "Epoch 16540 train loss 21141.236328125 val loss 44182.16796875\n",
      "Epoch 16550 train loss 21125.833984375 val loss 44229.37890625\n",
      "Epoch 16560 train loss 21118.28515625 val loss 44207.015625\n",
      "Epoch 16570 train loss 21094.384765625 val loss 44257.94140625\n",
      "Epoch 16580 train loss 21076.998046875 val loss 44326.078125\n",
      "Epoch 16590 train loss 21056.423828125 val loss 44359.21875\n",
      "Epoch 16600 train loss 21044.49609375 val loss 44414.0\n",
      "Epoch 16610 train loss 21045.240234375 val loss 44388.296875\n",
      "Epoch 16620 train loss 21014.62109375 val loss 44601.51171875\n",
      "Epoch 16630 train loss 21002.30078125 val loss 44624.953125\n",
      "Epoch 16640 train loss 20991.349609375 val loss 44588.95703125\n",
      "Epoch 16650 train loss 20980.3046875 val loss 44650.31640625\n",
      "Epoch 16660 train loss 20959.072265625 val loss 44752.640625\n",
      "Epoch 16670 train loss 20932.931640625 val loss 44811.0078125\n",
      "Epoch 16680 train loss 20911.58984375 val loss 44940.99609375\n",
      "Epoch 16690 train loss 20880.58984375 val loss 44887.18359375\n",
      "Epoch 16700 train loss 20860.431640625 val loss 44991.21484375\n",
      "Epoch 16710 train loss 20851.048828125 val loss 45110.05859375\n",
      "Epoch 16720 train loss 20824.6171875 val loss 45086.36328125\n",
      "Epoch 16730 train loss 20810.87109375 val loss 45176.8828125\n",
      "Epoch 16740 train loss 20788.029296875 val loss 45312.0234375\n",
      "Epoch 16750 train loss 20773.791015625 val loss 45382.625\n",
      "Epoch 16760 train loss 20749.771484375 val loss 45365.7109375\n",
      "Epoch 16770 train loss 20746.658203125 val loss 45318.30078125\n",
      "Epoch 16780 train loss 20722.001953125 val loss 45457.734375\n",
      "Epoch 16790 train loss 20714.51171875 val loss 45522.86328125\n",
      "Epoch 16800 train loss 20695.271484375 val loss 45518.1953125\n",
      "Epoch 16810 train loss 20684.6484375 val loss 45583.65625\n",
      "Epoch 16820 train loss 20673.767578125 val loss 45653.3203125\n",
      "Epoch 16830 train loss 20662.9453125 val loss 45679.1171875\n",
      "Epoch 16840 train loss 20650.0078125 val loss 45695.21875\n",
      "Epoch 16850 train loss 20636.185546875 val loss 45607.22265625\n",
      "Epoch 16860 train loss 20611.130859375 val loss 45730.59375\n",
      "Epoch 16870 train loss 20601.46484375 val loss 45782.93359375\n",
      "Epoch 16880 train loss 20579.26171875 val loss 45711.68359375\n",
      "Epoch 16890 train loss 20556.259765625 val loss 45762.5234375\n",
      "Epoch 16900 train loss 20543.7265625 val loss 45813.56640625\n",
      "Epoch 16910 train loss 20526.869140625 val loss 45859.61328125\n",
      "Epoch 16920 train loss 20504.63671875 val loss 45889.1328125\n",
      "Epoch 16930 train loss 20490.3984375 val loss 45863.66796875\n",
      "Epoch 16940 train loss 20482.400390625 val loss 45927.83203125\n",
      "Epoch 16950 train loss 20462.732421875 val loss 45980.8515625\n",
      "Epoch 16960 train loss 20444.90234375 val loss 45978.58203125\n",
      "Epoch 16970 train loss 20449.046875 val loss 45930.7578125\n",
      "Epoch 16980 train loss 20423.72265625 val loss 46084.8984375\n",
      "Epoch 16990 train loss 20410.666015625 val loss 46116.1328125\n",
      "Epoch 17000 train loss 20395.833984375 val loss 46074.11328125\n",
      "Epoch 17010 train loss 20385.318359375 val loss 46168.1171875\n",
      "Epoch 17020 train loss 20385.447265625 val loss 46238.36328125\n",
      "Epoch 17030 train loss 20362.8671875 val loss 46211.8125\n",
      "Epoch 17040 train loss 20349.62890625 val loss 46249.46484375\n",
      "Epoch 17050 train loss 20339.8125 val loss 46271.0234375\n",
      "Epoch 17060 train loss 20327.171875 val loss 46307.01953125\n",
      "Epoch 17070 train loss 20376.3671875 val loss 46193.6796875\n",
      "Epoch 17080 train loss 20313.181640625 val loss 46343.6328125\n",
      "Epoch 17090 train loss 20303.01171875 val loss 46410.828125\n",
      "Epoch 17100 train loss 20288.58984375 val loss 46363.71875\n",
      "Epoch 17110 train loss 20275.857421875 val loss 46438.3984375\n",
      "Epoch 17120 train loss 20264.462890625 val loss 46421.30859375\n",
      "Epoch 17130 train loss 20255.892578125 val loss 46486.28515625\n",
      "Epoch 17140 train loss 20241.908203125 val loss 46445.39453125\n",
      "Epoch 17150 train loss 20231.36328125 val loss 46500.69140625\n",
      "Epoch 17160 train loss 20217.3125 val loss 46571.00390625\n",
      "Epoch 17170 train loss 20210.806640625 val loss 46664.8828125\n",
      "Epoch 17180 train loss 20192.2109375 val loss 46693.5859375\n",
      "Epoch 17190 train loss 20187.146484375 val loss 46612.0703125\n",
      "Epoch 17200 train loss 20168.05859375 val loss 46680.484375\n",
      "Epoch 17210 train loss 20159.48046875 val loss 46768.0546875\n",
      "Epoch 17220 train loss 20145.431640625 val loss 46683.55078125\n",
      "Epoch 17230 train loss 20122.814453125 val loss 46765.1328125\n",
      "Epoch 17240 train loss 20104.484375 val loss 46795.5703125\n",
      "Epoch 17250 train loss 20091.19921875 val loss 46847.2734375\n",
      "Epoch 17260 train loss 20090.625 val loss 46901.61328125\n",
      "Epoch 17270 train loss 20068.70703125 val loss 46842.12890625\n",
      "Epoch 17280 train loss 20068.048828125 val loss 46797.12109375\n",
      "Epoch 17290 train loss 20050.021484375 val loss 46929.71484375\n",
      "Epoch 17300 train loss 20041.7890625 val loss 46972.26171875\n",
      "Epoch 17310 train loss 20032.37109375 val loss 46990.765625\n",
      "Epoch 17320 train loss 20029.66015625 val loss 47015.8515625\n",
      "Epoch 17330 train loss 20016.052734375 val loss 47034.61328125\n",
      "Epoch 17340 train loss 20008.205078125 val loss 46972.94921875\n",
      "Epoch 17350 train loss 20005.611328125 val loss 47009.2734375\n",
      "Epoch 17360 train loss 19995.07421875 val loss 47119.89453125\n",
      "Epoch 17370 train loss 19976.65625 val loss 47091.14453125\n",
      "Epoch 17380 train loss 19969.111328125 val loss 47116.2890625\n",
      "Epoch 17390 train loss 19962.267578125 val loss 47154.43359375\n",
      "Epoch 17400 train loss 19956.61328125 val loss 47159.54296875\n",
      "Epoch 17410 train loss 19947.39453125 val loss 47313.609375\n",
      "Epoch 17420 train loss 19935.65625 val loss 47270.77734375\n",
      "Epoch 17430 train loss 19933.97265625 val loss 47272.83984375\n",
      "Epoch 17440 train loss 19921.69921875 val loss 47303.3125\n",
      "Epoch 17450 train loss 19912.68359375 val loss 47347.75390625\n",
      "Epoch 17460 train loss 19909.431640625 val loss 47346.13671875\n",
      "Epoch 17470 train loss 19900.720703125 val loss 47404.2421875\n",
      "Epoch 17480 train loss 19892.724609375 val loss 47479.16015625\n",
      "Epoch 17490 train loss 19895.75 val loss 47522.7734375\n",
      "Epoch 17500 train loss 19877.333984375 val loss 47512.921875\n",
      "Epoch 17510 train loss 19871.91796875 val loss 47521.75390625\n",
      "Epoch 17520 train loss 19844.5 val loss 47567.39453125\n",
      "Epoch 17530 train loss 19827.33203125 val loss 47532.44140625\n",
      "Epoch 17540 train loss 19828.935546875 val loss 47471.64453125\n",
      "Epoch 17550 train loss 19803.935546875 val loss 47606.0078125\n",
      "Epoch 17560 train loss 19800.78515625 val loss 47677.6484375\n",
      "Epoch 17570 train loss 19785.9296875 val loss 47642.99609375\n",
      "Epoch 17580 train loss 19776.17578125 val loss 47665.73828125\n",
      "Epoch 17590 train loss 19775.341796875 val loss 47693.31640625\n",
      "Epoch 17600 train loss 19756.44921875 val loss 47757.5\n",
      "Epoch 17610 train loss 19745.93359375 val loss 47780.6875\n",
      "Epoch 17620 train loss 19744.09375 val loss 47909.9453125\n",
      "Epoch 17630 train loss 19738.416015625 val loss 47913.515625\n",
      "Epoch 17640 train loss 19729.953125 val loss 47839.31640625\n",
      "Epoch 17650 train loss 19713.544921875 val loss 47968.921875\n",
      "Epoch 17660 train loss 19708.55859375 val loss 47986.6171875\n",
      "Epoch 17670 train loss 19699.458984375 val loss 48054.63671875\n",
      "Epoch 17680 train loss 19694.716796875 val loss 48049.55078125\n",
      "Epoch 17690 train loss 19698.63671875 val loss 48031.2578125\n",
      "Epoch 17700 train loss 19678.74609375 val loss 48122.41796875\n",
      "Epoch 17710 train loss 19680.7421875 val loss 48239.3828125\n",
      "Epoch 17720 train loss 19663.44921875 val loss 48185.65234375\n",
      "Epoch 17730 train loss 19659.521484375 val loss 48206.00390625\n",
      "Epoch 17740 train loss 19647.724609375 val loss 48234.5\n",
      "Epoch 17750 train loss 19640.767578125 val loss 48293.97265625\n",
      "Epoch 17760 train loss 19652.720703125 val loss 48396.26171875\n",
      "Epoch 17770 train loss 19629.306640625 val loss 48304.328125\n",
      "Epoch 17780 train loss 19619.70703125 val loss 48327.85546875\n",
      "Epoch 17790 train loss 19612.6640625 val loss 48424.8046875\n",
      "Epoch 17800 train loss 19605.12890625 val loss 48419.17578125\n",
      "Epoch 17810 train loss 19603.9609375 val loss 48412.6328125\n",
      "Epoch 17820 train loss 19592.5234375 val loss 48476.3359375\n",
      "Epoch 17830 train loss 19589.28515625 val loss 48554.375\n",
      "Epoch 17840 train loss 19583.7890625 val loss 48576.109375\n",
      "Epoch 17850 train loss 19571.611328125 val loss 48604.10546875\n",
      "Epoch 17860 train loss 19566.39453125 val loss 48589.8515625\n",
      "Epoch 17870 train loss 19576.625 val loss 48585.8515625\n",
      "Epoch 17880 train loss 19551.126953125 val loss 48685.21875\n",
      "Epoch 17890 train loss 19547.52734375 val loss 48749.73828125\n",
      "Epoch 17900 train loss 19540.876953125 val loss 48753.71875\n",
      "Epoch 17910 train loss 19533.763671875 val loss 48841.90625\n",
      "Epoch 17920 train loss 19526.21875 val loss 48774.8984375\n",
      "Epoch 17930 train loss 19524.107421875 val loss 48863.55859375\n",
      "Epoch 17940 train loss 19516.734375 val loss 48842.3671875\n",
      "Epoch 17950 train loss 19512.166015625 val loss 48882.19921875\n",
      "Epoch 17960 train loss 19500.9140625 val loss 48912.61328125\n",
      "Epoch 17970 train loss 19494.41796875 val loss 49016.6953125\n",
      "Epoch 17980 train loss 19493.671875 val loss 49074.9296875\n",
      "Epoch 17990 train loss 19490.478515625 val loss 49070.39453125\n",
      "Epoch 18000 train loss 19475.9609375 val loss 49099.9453125\n",
      "Epoch 18010 train loss 19476.791015625 val loss 49123.22265625\n",
      "Epoch 18020 train loss 19466.318359375 val loss 49139.3046875\n",
      "Epoch 18030 train loss 19466.03515625 val loss 49199.2578125\n",
      "Epoch 18040 train loss 19453.232421875 val loss 49210.90234375\n",
      "Epoch 18050 train loss 19446.990234375 val loss 49182.15625\n",
      "Epoch 18060 train loss 19449.705078125 val loss 49180.9765625\n",
      "Epoch 18070 train loss 19442.125 val loss 49264.3515625\n",
      "Epoch 18080 train loss 19435.69921875 val loss 49338.0078125\n",
      "Epoch 18090 train loss 19431.798828125 val loss 49392.8203125\n",
      "Epoch 18100 train loss 19420.392578125 val loss 49401.21875\n",
      "Epoch 18110 train loss 19422.927734375 val loss 49393.984375\n",
      "Epoch 18120 train loss 19405.87890625 val loss 49475.25390625\n",
      "Epoch 18130 train loss 19406.31640625 val loss 49553.28125\n",
      "Epoch 18140 train loss 19394.48828125 val loss 49575.34375\n",
      "Epoch 18150 train loss 19382.748046875 val loss 49590.59765625\n",
      "Epoch 18160 train loss 19375.6796875 val loss 49683.5390625\n",
      "Epoch 18170 train loss 19376.8359375 val loss 49707.37109375\n",
      "Epoch 18180 train loss 19366.861328125 val loss 49670.23828125\n",
      "Epoch 18190 train loss 19367.78515625 val loss 49678.3984375\n",
      "Epoch 18200 train loss 19356.767578125 val loss 49808.6796875\n",
      "Epoch 18210 train loss 19345.16796875 val loss 49703.0625\n",
      "Epoch 18220 train loss 19340.546875 val loss 49749.984375\n",
      "Epoch 18230 train loss 19330.94921875 val loss 49790.83203125\n",
      "Epoch 18240 train loss 19326.337890625 val loss 49830.578125\n",
      "Epoch 18250 train loss 19336.216796875 val loss 49906.57421875\n",
      "Epoch 18260 train loss 19314.603515625 val loss 49769.7578125\n",
      "Epoch 18270 train loss 19310.080078125 val loss 49848.87890625\n",
      "Epoch 18280 train loss 19311.04296875 val loss 49814.734375\n",
      "Epoch 18290 train loss 19298.37890625 val loss 49880.82421875\n",
      "Epoch 18300 train loss 19292.44921875 val loss 49910.14453125\n",
      "Epoch 18310 train loss 19290.001953125 val loss 49948.25390625\n",
      "Epoch 18320 train loss 19283.453125 val loss 49967.7265625\n",
      "Epoch 18330 train loss 19277.76171875 val loss 50038.83203125\n",
      "Epoch 18340 train loss 19284.4765625 val loss 50060.61328125\n",
      "Epoch 18350 train loss 19266.8828125 val loss 50033.63671875\n",
      "Epoch 18360 train loss 19260.759765625 val loss 50075.48828125\n",
      "Epoch 18370 train loss 19254.873046875 val loss 50052.26953125\n",
      "Epoch 18380 train loss 19252.89453125 val loss 50122.21484375\n",
      "Epoch 18390 train loss 19240.01171875 val loss 50021.31640625\n",
      "Epoch 18400 train loss 19245.205078125 val loss 50024.32421875\n",
      "Epoch 18410 train loss 19231.36328125 val loss 50043.9765625\n",
      "Epoch 18420 train loss 19231.421875 val loss 50102.6640625\n",
      "Epoch 18430 train loss 19220.416015625 val loss 50086.2421875\n",
      "Epoch 18440 train loss 19213.490234375 val loss 50100.6015625\n",
      "Epoch 18450 train loss 19210.3046875 val loss 50112.83203125\n",
      "Epoch 18460 train loss 19219.236328125 val loss 50171.26171875\n",
      "Epoch 18470 train loss 19203.849609375 val loss 50076.52734375\n",
      "Epoch 18480 train loss 19191.5390625 val loss 50104.953125\n",
      "Epoch 18490 train loss 19194.666015625 val loss 50193.82421875\n",
      "Epoch 18500 train loss 19186.69921875 val loss 50097.8359375\n",
      "Epoch 18510 train loss 19175.544921875 val loss 50172.8203125\n",
      "Epoch 18520 train loss 19178.615234375 val loss 50187.82421875\n",
      "Epoch 18530 train loss 19169.83203125 val loss 50124.44921875\n",
      "Epoch 18540 train loss 19163.263671875 val loss 50154.21484375\n",
      "Epoch 18550 train loss 19160.96484375 val loss 50183.296875\n",
      "Epoch 18560 train loss 19153.158203125 val loss 50149.5703125\n",
      "Epoch 18570 train loss 19160.326171875 val loss 50108.828125\n",
      "Epoch 18580 train loss 19146.92578125 val loss 50191.76171875\n",
      "Epoch 18590 train loss 19140.22265625 val loss 50194.734375\n",
      "Epoch 18600 train loss 19141.2109375 val loss 50233.2734375\n",
      "Epoch 18610 train loss 19132.623046875 val loss 50182.62109375\n",
      "Epoch 18620 train loss 19136.818359375 val loss 50141.421875\n",
      "Epoch 18630 train loss 19125.59375 val loss 50134.04296875\n",
      "Epoch 18640 train loss 19118.751953125 val loss 50218.59765625\n",
      "Epoch 18650 train loss 19120.064453125 val loss 50241.15234375\n",
      "Epoch 18660 train loss 19112.677734375 val loss 50181.578125\n",
      "Epoch 18670 train loss 19110.650390625 val loss 50179.22265625\n",
      "Epoch 18680 train loss 19102.33984375 val loss 50207.77734375\n",
      "Epoch 18690 train loss 19111.642578125 val loss 50156.16796875\n",
      "Epoch 18700 train loss 19093.615234375 val loss 50205.4375\n",
      "Epoch 18710 train loss 19094.611328125 val loss 50164.30859375\n",
      "Epoch 18720 train loss 19087.822265625 val loss 50283.6796875\n",
      "Epoch 18730 train loss 19081.1171875 val loss 50186.21484375\n",
      "Epoch 18740 train loss 19075.982421875 val loss 50237.6875\n",
      "Epoch 18750 train loss 19075.00390625 val loss 50198.38671875\n",
      "Epoch 18760 train loss 19066.01171875 val loss 50205.53515625\n",
      "Epoch 18770 train loss 19062.615234375 val loss 50232.98828125\n",
      "Epoch 18780 train loss 19062.365234375 val loss 50248.34375\n",
      "Epoch 18790 train loss 19054.123046875 val loss 50236.69921875\n",
      "Epoch 18800 train loss 19037.158203125 val loss 50196.41796875\n",
      "Epoch 18810 train loss 19005.556640625 val loss 50157.5390625\n",
      "Epoch 18820 train loss 18983.92578125 val loss 50168.1015625\n",
      "Epoch 18830 train loss 19012.765625 val loss 50132.41796875\n",
      "Epoch 18840 train loss 18977.5546875 val loss 50278.22265625\n",
      "Epoch 18850 train loss 18948.708984375 val loss 50222.87109375\n",
      "Epoch 18860 train loss 18921.923828125 val loss 50170.765625\n",
      "Epoch 18870 train loss 18892.91796875 val loss 50054.125\n",
      "Epoch 18880 train loss 18873.44921875 val loss 50167.9296875\n",
      "Epoch 18890 train loss 18854.74609375 val loss 50230.4453125\n",
      "Epoch 18900 train loss 18845.51171875 val loss 50320.578125\n",
      "Epoch 18910 train loss 18831.587890625 val loss 50365.67578125\n",
      "Epoch 18920 train loss 18817.626953125 val loss 50445.515625\n",
      "Epoch 18930 train loss 18809.80078125 val loss 50508.72265625\n",
      "Epoch 18940 train loss 18797.2734375 val loss 50503.0703125\n",
      "Epoch 18950 train loss 18783.896484375 val loss 50572.84765625\n",
      "Epoch 18960 train loss 18790.802734375 val loss 50583.04296875\n",
      "Epoch 18970 train loss 18766.234375 val loss 50543.328125\n",
      "Epoch 18980 train loss 18764.654296875 val loss 50531.52734375\n",
      "Epoch 18990 train loss 18756.306640625 val loss 50531.62890625\n",
      "Epoch 19000 train loss 18745.24609375 val loss 50562.03125\n",
      "Epoch 19010 train loss 18742.1015625 val loss 50600.859375\n",
      "Epoch 19020 train loss 18737.06640625 val loss 50586.04296875\n",
      "Epoch 19030 train loss 18732.533203125 val loss 50630.68359375\n",
      "Epoch 19040 train loss 18721.814453125 val loss 50521.9765625\n",
      "Epoch 19050 train loss 18713.259765625 val loss 50625.18359375\n",
      "Epoch 19060 train loss 18708.412109375 val loss 50599.99609375\n",
      "Epoch 19070 train loss 18699.4375 val loss 50611.796875\n",
      "Epoch 19080 train loss 18674.564453125 val loss 50605.3203125\n",
      "Epoch 19090 train loss 18682.34765625 val loss 50623.81640625\n",
      "Epoch 19100 train loss 18649.490234375 val loss 50635.91796875\n",
      "Epoch 19110 train loss 18640.20703125 val loss 50588.6015625\n",
      "Epoch 19120 train loss 18652.1640625 val loss 50565.25\n",
      "Epoch 19130 train loss 18622.625 val loss 50614.11328125\n",
      "Epoch 19140 train loss 18617.884765625 val loss 50617.63671875\n",
      "Epoch 19150 train loss 18608.748046875 val loss 50640.73828125\n",
      "Epoch 19160 train loss 18607.76171875 val loss 50604.93359375\n",
      "Epoch 19170 train loss 18592.716796875 val loss 50576.81640625\n",
      "Epoch 19180 train loss 18585.40234375 val loss 50574.1796875\n",
      "Epoch 19190 train loss 18588.35546875 val loss 50516.796875\n",
      "Epoch 19200 train loss 18572.822265625 val loss 50513.625\n",
      "Epoch 19210 train loss 18586.314453125 val loss 50523.22265625\n",
      "Epoch 19220 train loss 18573.76953125 val loss 50453.93359375\n",
      "Epoch 19230 train loss 18556.248046875 val loss 50501.640625\n",
      "Epoch 19240 train loss 18552.09375 val loss 50431.65625\n",
      "Epoch 19250 train loss 18549.583984375 val loss 50454.81640625\n",
      "Epoch 19260 train loss 18539.53125 val loss 50435.23046875\n",
      "Epoch 19270 train loss 18528.73046875 val loss 50464.921875\n",
      "Epoch 19280 train loss 18537.1171875 val loss 50477.1015625\n",
      "Epoch 19290 train loss 18522.900390625 val loss 50425.4375\n",
      "Epoch 19300 train loss 18512.58203125 val loss 50430.7421875\n",
      "Epoch 19310 train loss 18512.248046875 val loss 50433.34375\n",
      "Epoch 19320 train loss 18519.080078125 val loss 50443.375\n",
      "Epoch 19330 train loss 18500.623046875 val loss 50481.23828125\n",
      "Epoch 19340 train loss 18504.052734375 val loss 50492.54296875\n",
      "Epoch 19350 train loss 18491.31640625 val loss 50439.90625\n",
      "Epoch 19360 train loss 18487.2265625 val loss 50440.125\n",
      "Epoch 19370 train loss 18494.654296875 val loss 50508.4140625\n",
      "Epoch 19380 train loss 18479.205078125 val loss 50447.08203125\n",
      "Epoch 19390 train loss 18473.91796875 val loss 50499.87109375\n",
      "Epoch 19400 train loss 18469.6796875 val loss 50488.8125\n",
      "Epoch 19410 train loss 18463.765625 val loss 50455.734375\n",
      "Epoch 19420 train loss 18469.4296875 val loss 50474.98828125\n",
      "Epoch 19430 train loss 18464.51171875 val loss 50497.6484375\n",
      "Epoch 19440 train loss 18452.935546875 val loss 50492.640625\n",
      "Epoch 19450 train loss 18454.00390625 val loss 50489.640625\n",
      "Epoch 19460 train loss 18453.97265625 val loss 50446.671875\n",
      "Epoch 19470 train loss 18439.623046875 val loss 50466.94140625\n",
      "Epoch 19480 train loss 18435.6953125 val loss 50522.49609375\n",
      "Epoch 19490 train loss 18432.396484375 val loss 50471.6953125\n",
      "Epoch 19500 train loss 18431.5 val loss 50495.9609375\n",
      "Epoch 19510 train loss 18424.607421875 val loss 50469.15234375\n",
      "Epoch 19520 train loss 18419.611328125 val loss 50456.97265625\n",
      "Epoch 19530 train loss 18419.6796875 val loss 50459.3828125\n",
      "Epoch 19540 train loss 18406.2578125 val loss 50426.2109375\n",
      "Epoch 19550 train loss 18401.177734375 val loss 50415.79296875\n",
      "Epoch 19560 train loss 18426.357421875 val loss 50513.68359375\n",
      "Epoch 19570 train loss 18403.400390625 val loss 50373.46484375\n",
      "Epoch 19580 train loss 18392.87109375 val loss 50445.96484375\n",
      "Epoch 19590 train loss 18387.8671875 val loss 50377.44921875\n",
      "Epoch 19600 train loss 18382.45703125 val loss 50430.46484375\n",
      "Epoch 19610 train loss 18378.04296875 val loss 50390.4921875\n",
      "Epoch 19620 train loss 18374.416015625 val loss 50376.921875\n",
      "Epoch 19630 train loss 18377.720703125 val loss 50340.1953125\n",
      "Epoch 19640 train loss 18364.923828125 val loss 50365.57421875\n",
      "Epoch 19650 train loss 18366.6796875 val loss 50431.62109375\n",
      "Epoch 19660 train loss 18362.599609375 val loss 50388.27734375\n",
      "Epoch 19670 train loss 18361.255859375 val loss 50328.140625\n",
      "Epoch 19680 train loss 18357.70703125 val loss 50344.05859375\n",
      "Epoch 19690 train loss 18359.923828125 val loss 50412.26953125\n",
      "Epoch 19700 train loss 18345.150390625 val loss 50345.77734375\n",
      "Epoch 19710 train loss 18353.044921875 val loss 50315.375\n",
      "Epoch 19720 train loss 18340.04296875 val loss 50366.84765625\n",
      "Epoch 19730 train loss 18333.416015625 val loss 50363.1171875\n",
      "Epoch 19740 train loss 18330.337890625 val loss 50367.3671875\n",
      "Epoch 19750 train loss 18328.880859375 val loss 50384.39453125\n",
      "Epoch 19760 train loss 18339.36328125 val loss 50368.2421875\n",
      "Epoch 19770 train loss 18323.45703125 val loss 50289.15625\n",
      "Epoch 19780 train loss 18317.69140625 val loss 50346.80859375\n",
      "Epoch 19790 train loss 18318.09765625 val loss 50338.296875\n",
      "Epoch 19800 train loss 18314.04296875 val loss 50242.109375\n",
      "Epoch 19810 train loss 18310.1484375 val loss 50351.6953125\n",
      "Epoch 19820 train loss 18302.109375 val loss 50208.0390625\n",
      "Epoch 19830 train loss 18295.1015625 val loss 50241.51171875\n",
      "Epoch 19840 train loss 18293.712890625 val loss 50220.99609375\n",
      "Epoch 19850 train loss 18295.9921875 val loss 50188.125\n",
      "Epoch 19860 train loss 18285.3125 val loss 50244.70703125\n",
      "Epoch 19870 train loss 18299.80078125 val loss 50290.4296875\n",
      "Epoch 19880 train loss 18276.3671875 val loss 50160.50390625\n",
      "Epoch 19890 train loss 18271.970703125 val loss 50174.76171875\n",
      "Epoch 19900 train loss 18267.02734375 val loss 50175.99609375\n",
      "Epoch 19910 train loss 18266.814453125 val loss 50183.8984375\n",
      "Epoch 19920 train loss 18259.546875 val loss 50108.9921875\n",
      "Epoch 19930 train loss 18264.91796875 val loss 50102.01953125\n",
      "Epoch 19940 train loss 18250.76171875 val loss 50109.0\n",
      "Epoch 19950 train loss 18253.037109375 val loss 50059.578125\n",
      "Epoch 19960 train loss 18251.423828125 val loss 50088.3203125\n",
      "Epoch 19970 train loss 18239.443359375 val loss 50124.32421875\n",
      "Epoch 19980 train loss 18251.421875 val loss 50154.87890625\n",
      "Epoch 19990 train loss 18233.69921875 val loss 50133.7109375\n",
      "Epoch 20000 train loss 18237.453125 val loss 50125.62109375\n",
      "Epoch 20010 train loss 18227.404296875 val loss 50118.0078125\n",
      "Epoch 20020 train loss 18223.927734375 val loss 50131.36328125\n",
      "Epoch 20030 train loss 18219.9609375 val loss 50150.62109375\n",
      "Epoch 20040 train loss 18236.95703125 val loss 50220.6015625\n",
      "Epoch 20050 train loss 18211.8203125 val loss 50155.1484375\n",
      "Epoch 20060 train loss 18207.94921875 val loss 50168.73828125\n",
      "Epoch 20070 train loss 18211.857421875 val loss 50251.32421875\n",
      "Epoch 20080 train loss 18201.5234375 val loss 50232.5703125\n",
      "Epoch 20090 train loss 18195.74609375 val loss 50202.16015625\n",
      "Epoch 20100 train loss 18198.455078125 val loss 50184.6796875\n",
      "Epoch 20110 train loss 18191.703125 val loss 50249.6640625\n",
      "Epoch 20120 train loss 18191.90234375 val loss 50261.33984375\n",
      "Epoch 20130 train loss 18185.49609375 val loss 50232.66015625\n",
      "Epoch 20140 train loss 18179.40625 val loss 50277.4296875\n",
      "Epoch 20150 train loss 18177.98828125 val loss 50247.25390625\n",
      "Epoch 20160 train loss 18177.48828125 val loss 50251.45703125\n",
      "Epoch 20170 train loss 18174.705078125 val loss 50238.94140625\n",
      "Epoch 20180 train loss 18163.96484375 val loss 50272.0625\n",
      "Epoch 20190 train loss 18179.177734375 val loss 50246.984375\n",
      "Epoch 20200 train loss 18153.896484375 val loss 50269.63671875\n",
      "Epoch 20210 train loss 18153.990234375 val loss 50296.04296875\n",
      "Epoch 20220 train loss 18155.4375 val loss 50352.60546875\n",
      "Epoch 20230 train loss 18150.345703125 val loss 50290.85546875\n",
      "Epoch 20240 train loss 18140.9296875 val loss 50320.85546875\n",
      "Epoch 20250 train loss 18137.484375 val loss 50327.9453125\n",
      "Epoch 20260 train loss 18142.52734375 val loss 50291.23828125\n",
      "Epoch 20270 train loss 18131.876953125 val loss 50357.76953125\n",
      "Epoch 20280 train loss 18130.64453125 val loss 50396.7578125\n",
      "Epoch 20290 train loss 18131.0546875 val loss 50386.9375\n",
      "Epoch 20300 train loss 18120.658203125 val loss 50391.8046875\n",
      "Epoch 20310 train loss 18115.03515625 val loss 50387.90234375\n",
      "Epoch 20320 train loss 18117.267578125 val loss 50392.609375\n",
      "Epoch 20330 train loss 18110.638671875 val loss 50399.8984375\n",
      "Epoch 20340 train loss 18106.97265625 val loss 50373.24609375\n",
      "Epoch 20350 train loss 18108.283203125 val loss 50369.171875\n",
      "Epoch 20360 train loss 18095.4609375 val loss 50393.4921875\n",
      "Epoch 20370 train loss 18092.126953125 val loss 50358.24609375\n",
      "Epoch 20380 train loss 18086.1875 val loss 50364.546875\n",
      "Epoch 20390 train loss 18090.296875 val loss 50313.69140625\n",
      "Epoch 20400 train loss 18078.462890625 val loss 50385.63671875\n",
      "Epoch 20410 train loss 18074.1875 val loss 50360.2109375\n",
      "Epoch 20420 train loss 18081.25 val loss 50379.15234375\n",
      "Epoch 20430 train loss 18064.513671875 val loss 50295.23046875\n",
      "Epoch 20440 train loss 18069.681640625 val loss 50211.671875\n",
      "Epoch 20450 train loss 18049.287109375 val loss 50257.38671875\n",
      "Epoch 20460 train loss 18050.1171875 val loss 50218.51171875\n",
      "Epoch 20470 train loss 18037.40625 val loss 50205.046875\n",
      "Epoch 20480 train loss 18045.505859375 val loss 50221.11328125\n",
      "Epoch 20490 train loss 18031.42578125 val loss 50161.1796875\n",
      "Epoch 20500 train loss 18029.919921875 val loss 50134.16015625\n",
      "Epoch 20510 train loss 18028.005859375 val loss 50149.0\n",
      "Epoch 20520 train loss 18016.859375 val loss 50140.9296875\n",
      "Epoch 20530 train loss 18038.958984375 val loss 50098.73046875\n",
      "Epoch 20540 train loss 18018.2109375 val loss 50190.51953125\n",
      "Epoch 20550 train loss 18005.798828125 val loss 50121.6796875\n",
      "Epoch 20560 train loss 18004.251953125 val loss 50098.8359375\n",
      "Epoch 20570 train loss 18000.11328125 val loss 50123.2109375\n",
      "Epoch 20580 train loss 17995.744140625 val loss 50140.6640625\n",
      "Epoch 20590 train loss 18010.87890625 val loss 50162.23046875\n",
      "Epoch 20600 train loss 17995.482421875 val loss 50081.734375\n",
      "Epoch 20610 train loss 17988.44140625 val loss 50130.0078125\n",
      "Epoch 20620 train loss 17984.40625 val loss 50089.11328125\n",
      "Epoch 20630 train loss 17981.783203125 val loss 50073.9140625\n",
      "Epoch 20640 train loss 17977.734375 val loss 50100.93359375\n",
      "Epoch 20650 train loss 17976.875 val loss 50065.48828125\n",
      "Epoch 20660 train loss 17974.728515625 val loss 50050.6328125\n",
      "Epoch 20670 train loss 17968.544921875 val loss 50076.98828125\n",
      "Epoch 20680 train loss 17968.765625 val loss 50117.96484375\n",
      "Epoch 20690 train loss 17970.626953125 val loss 50086.921875\n",
      "Epoch 20700 train loss 17966.12109375 val loss 50062.625\n",
      "Epoch 20710 train loss 17960.306640625 val loss 50074.58203125\n",
      "Epoch 20720 train loss 17953.87890625 val loss 50037.62890625\n",
      "Epoch 20730 train loss 17951.431640625 val loss 50050.0234375\n",
      "Epoch 20740 train loss 17972.712890625 val loss 50036.60546875\n",
      "Epoch 20750 train loss 17949.083984375 val loss 50107.2265625\n",
      "Epoch 20760 train loss 17942.994140625 val loss 50059.359375\n",
      "Epoch 20770 train loss 17938.626953125 val loss 50073.59375\n",
      "Epoch 20780 train loss 17947.419921875 val loss 50128.75\n",
      "Epoch 20790 train loss 17935.4140625 val loss 50027.73828125\n",
      "Epoch 20800 train loss 17933.740234375 val loss 50055.56640625\n",
      "Epoch 20810 train loss 17932.490234375 val loss 50113.9609375\n",
      "Epoch 20820 train loss 17926.51171875 val loss 50100.54296875\n",
      "Epoch 20830 train loss 17920.759765625 val loss 50069.8359375\n",
      "Epoch 20840 train loss 17931.712890625 val loss 50103.6484375\n",
      "Epoch 20850 train loss 17916.70703125 val loss 50097.3125\n",
      "Epoch 20860 train loss 17921.64453125 val loss 50128.65234375\n",
      "Epoch 20870 train loss 17910.6171875 val loss 50099.62109375\n",
      "Epoch 20880 train loss 17919.203125 val loss 50125.265625\n",
      "Epoch 20890 train loss 17904.146484375 val loss 50095.28125\n",
      "Epoch 20900 train loss 17906.951171875 val loss 50071.2734375\n",
      "Epoch 20910 train loss 17898.44140625 val loss 50110.99609375\n",
      "Epoch 20920 train loss 17919.94921875 val loss 50152.5625\n",
      "Epoch 20930 train loss 17900.380859375 val loss 50070.46484375\n",
      "Epoch 20940 train loss 17890.7734375 val loss 50104.359375\n",
      "Epoch 20950 train loss 17894.361328125 val loss 50107.11328125\n",
      "Epoch 20960 train loss 17889.05859375 val loss 50117.1484375\n",
      "Epoch 20970 train loss 17890.0703125 val loss 50126.81640625\n",
      "Epoch 20980 train loss 17879.80859375 val loss 50087.4140625\n",
      "Epoch 20990 train loss 17887.04296875 val loss 50075.234375\n",
      "Epoch 21000 train loss 17874.96484375 val loss 50061.48046875\n",
      "Epoch 21010 train loss 17888.251953125 val loss 50110.93359375\n",
      "Epoch 21020 train loss 17869.88671875 val loss 50080.609375\n",
      "Epoch 21030 train loss 17874.15234375 val loss 50046.97265625\n",
      "Epoch 21040 train loss 17866.845703125 val loss 50099.4609375\n",
      "Epoch 21050 train loss 17862.83203125 val loss 50072.46875\n",
      "Epoch 21060 train loss 17861.228515625 val loss 50078.6015625\n",
      "Epoch 21070 train loss 17861.6640625 val loss 50068.64453125\n",
      "Epoch 21080 train loss 17857.326171875 val loss 50048.03515625\n",
      "Epoch 21090 train loss 17856.3359375 val loss 50032.5703125\n",
      "Epoch 21100 train loss 17846.32421875 val loss 50018.10546875\n",
      "Epoch 21110 train loss 17849.892578125 val loss 50036.2734375\n",
      "Epoch 21120 train loss 17845.349609375 val loss 49979.77734375\n",
      "Epoch 21130 train loss 17856.923828125 val loss 50091.3125\n",
      "Epoch 21140 train loss 17836.201171875 val loss 50027.54296875\n",
      "Epoch 21150 train loss 17844.177734375 val loss 50029.96875\n",
      "Epoch 21160 train loss 17832.296875 val loss 50046.3046875\n",
      "Epoch 21170 train loss 17841.4296875 val loss 50070.59375\n",
      "Epoch 21180 train loss 17830.029296875 val loss 50065.484375\n",
      "Epoch 21190 train loss 17831.66015625 val loss 50023.8046875\n",
      "Epoch 21200 train loss 17826.19921875 val loss 50047.375\n",
      "Epoch 21210 train loss 17829.48046875 val loss 50026.9609375\n",
      "Epoch 21220 train loss 17818.703125 val loss 50011.8203125\n",
      "Epoch 21230 train loss 17828.76953125 val loss 49978.875\n",
      "Epoch 21240 train loss 17815.744140625 val loss 49964.31640625\n",
      "Epoch 21250 train loss 17819.625 val loss 49962.30859375\n",
      "Epoch 21260 train loss 17816.1015625 val loss 49982.28125\n",
      "Epoch 21270 train loss 17809.984375 val loss 49964.7109375\n",
      "Epoch 21280 train loss 17803.841796875 val loss 49977.99609375\n",
      "Epoch 21290 train loss 17819.28125 val loss 50013.2265625\n",
      "Epoch 21300 train loss 17798.123046875 val loss 49998.15625\n",
      "Epoch 21310 train loss 17812.12890625 val loss 49919.2734375\n",
      "Epoch 21320 train loss 17798.57421875 val loss 50003.73046875\n",
      "Epoch 21330 train loss 17790.16796875 val loss 49981.8515625\n",
      "Epoch 21340 train loss 17794.41796875 val loss 49938.625\n",
      "Epoch 21350 train loss 17788.302734375 val loss 49966.4765625\n",
      "Epoch 21360 train loss 17784.1484375 val loss 49961.84375\n",
      "Epoch 21370 train loss 17787.013671875 val loss 49924.734375\n",
      "Epoch 21380 train loss 17777.89453125 val loss 49932.359375\n",
      "Epoch 21390 train loss 17782.412109375 val loss 49977.125\n",
      "Epoch 21400 train loss 17773.412109375 val loss 49939.74609375\n",
      "Epoch 21410 train loss 17772.3359375 val loss 49897.9296875\n",
      "Epoch 21420 train loss 17768.607421875 val loss 49898.54296875\n",
      "Epoch 21430 train loss 17775.330078125 val loss 49933.38671875\n",
      "Epoch 21440 train loss 17760.59375 val loss 49869.40625\n",
      "Epoch 21450 train loss 17767.34375 val loss 49859.8359375\n",
      "Epoch 21460 train loss 17761.568359375 val loss 49879.9140625\n",
      "Epoch 21470 train loss 17758.59375 val loss 49836.19921875\n",
      "Epoch 21480 train loss 17753.431640625 val loss 49859.84765625\n",
      "Epoch 21490 train loss 17760.0546875 val loss 49821.4765625\n",
      "Epoch 21500 train loss 17751.3515625 val loss 49818.890625\n",
      "Epoch 21510 train loss 17746.3515625 val loss 49849.5078125\n",
      "Epoch 21520 train loss 17755.21484375 val loss 49831.34765625\n",
      "Epoch 21530 train loss 17742.19921875 val loss 49837.98046875\n",
      "Epoch 21540 train loss 17744.728515625 val loss 49852.6640625\n",
      "Epoch 21550 train loss 17745.142578125 val loss 49874.0546875\n",
      "Epoch 21560 train loss 17733.900390625 val loss 49829.9375\n",
      "Epoch 21570 train loss 17741.25390625 val loss 49855.78515625\n",
      "Epoch 21580 train loss 17732.037109375 val loss 49832.87890625\n",
      "Epoch 21590 train loss 17735.310546875 val loss 49871.12109375\n",
      "Epoch 21600 train loss 17725.861328125 val loss 49765.546875\n",
      "Epoch 21610 train loss 17724.611328125 val loss 49770.23046875\n",
      "Epoch 21620 train loss 17721.1484375 val loss 49801.94140625\n",
      "Epoch 21630 train loss 17728.794921875 val loss 49766.77734375\n",
      "Epoch 21640 train loss 17716.48828125 val loss 49760.25390625\n",
      "Epoch 21650 train loss 17716.7421875 val loss 49785.0625\n",
      "Epoch 21660 train loss 17726.63671875 val loss 49791.02734375\n",
      "Epoch 21670 train loss 17712.443359375 val loss 49748.34765625\n",
      "Epoch 21680 train loss 17725.126953125 val loss 49729.21484375\n",
      "Epoch 21690 train loss 17706.79296875 val loss 49778.7265625\n",
      "Epoch 21700 train loss 17721.83203125 val loss 49809.1171875\n",
      "Epoch 21710 train loss 17703.548828125 val loss 49735.1015625\n",
      "Epoch 21720 train loss 17703.62109375 val loss 49779.4765625\n",
      "Epoch 21730 train loss 17706.52734375 val loss 49834.96875\n",
      "Epoch 21740 train loss 17695.98828125 val loss 49789.36328125\n",
      "Epoch 21750 train loss 17701.732421875 val loss 49777.19140625\n",
      "Epoch 21760 train loss 17699.701171875 val loss 49786.61328125\n",
      "Epoch 21770 train loss 17689.26953125 val loss 49772.4765625\n",
      "Epoch 21780 train loss 17699.7734375 val loss 49770.05859375\n",
      "Epoch 21790 train loss 17699.6796875 val loss 49838.28515625\n",
      "Epoch 21800 train loss 17685.39453125 val loss 49765.64453125\n",
      "Epoch 21810 train loss 17681.56640625 val loss 49816.4375\n",
      "Epoch 21820 train loss 17686.26953125 val loss 49848.63671875\n",
      "Epoch 21830 train loss 17679.083984375 val loss 49786.70703125\n",
      "Epoch 21840 train loss 17684.36328125 val loss 49789.62890625\n",
      "Epoch 21850 train loss 17680.634765625 val loss 49843.6796875\n",
      "Epoch 21860 train loss 17673.658203125 val loss 49827.18359375\n",
      "Epoch 21870 train loss 17679.236328125 val loss 49760.6015625\n",
      "Epoch 21880 train loss 17667.546875 val loss 49792.13671875\n",
      "Epoch 21890 train loss 17666.9765625 val loss 49757.40625\n",
      "Epoch 21900 train loss 17663.90234375 val loss 49787.69921875\n",
      "Epoch 21910 train loss 17661.544921875 val loss 49754.86328125\n",
      "Epoch 21920 train loss 17660.171875 val loss 49738.79296875\n",
      "Epoch 21930 train loss 17665.298828125 val loss 49757.37890625\n",
      "Epoch 21940 train loss 17659.58984375 val loss 49756.140625\n",
      "Epoch 21950 train loss 17657.306640625 val loss 49737.7890625\n",
      "Epoch 21960 train loss 17654.162109375 val loss 49730.015625\n",
      "Epoch 21970 train loss 17652.548828125 val loss 49700.2421875\n",
      "Epoch 21980 train loss 17655.650390625 val loss 49735.03515625\n",
      "Epoch 21990 train loss 17646.107421875 val loss 49719.8828125\n",
      "Epoch 22000 train loss 17651.9296875 val loss 49767.640625\n",
      "Epoch 22010 train loss 17644.205078125 val loss 49724.22265625\n",
      "Epoch 22020 train loss 17640.7890625 val loss 49681.2734375\n",
      "Epoch 22030 train loss 17655.447265625 val loss 49745.69921875\n",
      "Epoch 22040 train loss 17633.46484375 val loss 49686.5\n",
      "Epoch 22050 train loss 17654.02734375 val loss 49698.99609375\n",
      "Epoch 22060 train loss 17629.49609375 val loss 49763.4375\n",
      "Epoch 22070 train loss 17628.88671875 val loss 49700.6640625\n",
      "Epoch 22080 train loss 17623.603515625 val loss 49712.8671875\n",
      "Epoch 22090 train loss 17624.16796875 val loss 49724.953125\n",
      "Epoch 22100 train loss 17621.703125 val loss 49729.5625\n",
      "Epoch 22110 train loss 17622.677734375 val loss 49721.48828125\n",
      "Epoch 22120 train loss 17619.236328125 val loss 49670.47265625\n",
      "Epoch 22130 train loss 17610.8359375 val loss 49704.515625\n",
      "Epoch 22140 train loss 17620.447265625 val loss 49744.55859375\n",
      "Epoch 22150 train loss 17607.626953125 val loss 49699.1796875\n",
      "Epoch 22160 train loss 17608.28125 val loss 49653.4140625\n",
      "Epoch 22170 train loss 17606.794921875 val loss 49687.8125\n",
      "Epoch 22180 train loss 17600.90234375 val loss 49676.34765625\n",
      "Epoch 22190 train loss 17604.859375 val loss 49688.81640625\n",
      "Epoch 22200 train loss 17596.3828125 val loss 49689.79296875\n",
      "Epoch 22210 train loss 17608.994140625 val loss 49655.296875\n",
      "Epoch 22220 train loss 17607.2421875 val loss 49731.51953125\n",
      "Epoch 22230 train loss 17598.599609375 val loss 49703.66796875\n",
      "Epoch 22240 train loss 17587.08203125 val loss 49704.15234375\n",
      "Epoch 22250 train loss 17589.740234375 val loss 49705.421875\n",
      "Epoch 22260 train loss 17582.572265625 val loss 49679.8671875\n",
      "Epoch 22270 train loss 17586.572265625 val loss 49703.21484375\n",
      "Epoch 22280 train loss 17580.05078125 val loss 49728.87890625\n",
      "Epoch 22290 train loss 17584.97265625 val loss 49740.6171875\n",
      "Epoch 22300 train loss 17574.16015625 val loss 49708.11328125\n",
      "Epoch 22310 train loss 17587.72265625 val loss 49676.7109375\n",
      "Epoch 22320 train loss 17588.158203125 val loss 49732.1484375\n",
      "Epoch 22330 train loss 17568.095703125 val loss 49691.99609375\n",
      "Epoch 22340 train loss 17574.7421875 val loss 49696.01171875\n",
      "Epoch 22350 train loss 17565.45703125 val loss 49673.3671875\n",
      "Epoch 22360 train loss 17560.830078125 val loss 49664.3984375\n",
      "Epoch 22370 train loss 17565.3828125 val loss 49684.4765625\n",
      "Epoch 22380 train loss 17557.46875 val loss 49644.234375\n",
      "Epoch 22390 train loss 17563.45703125 val loss 49590.29296875\n",
      "Epoch 22400 train loss 17554.0234375 val loss 49639.46484375\n",
      "Epoch 22410 train loss 17558.810546875 val loss 49671.48046875\n",
      "Epoch 22420 train loss 17559.14453125 val loss 49582.7109375\n",
      "Epoch 22430 train loss 17546.06640625 val loss 49590.84765625\n",
      "Epoch 22440 train loss 17546.7890625 val loss 49602.17578125\n",
      "Epoch 22450 train loss 17561.56640625 val loss 49606.98046875\n",
      "Epoch 22460 train loss 17546.1640625 val loss 49568.01171875\n",
      "Epoch 22470 train loss 17539.330078125 val loss 49560.52734375\n",
      "Epoch 22480 train loss 17538.03125 val loss 49552.453125\n",
      "Epoch 22490 train loss 17546.66796875 val loss 49557.2109375\n",
      "Epoch 22500 train loss 17531.81640625 val loss 49512.19140625\n",
      "Epoch 22510 train loss 17541.2734375 val loss 49508.98046875\n",
      "Epoch 22520 train loss 17529.931640625 val loss 49493.953125\n",
      "Epoch 22530 train loss 17532.701171875 val loss 49493.109375\n",
      "Epoch 22540 train loss 17526.640625 val loss 49483.21875\n",
      "Epoch 22550 train loss 17525.25 val loss 49451.41796875\n",
      "Epoch 22560 train loss 17541.5546875 val loss 49492.05859375\n",
      "Epoch 22570 train loss 17518.314453125 val loss 49456.97265625\n",
      "Epoch 22580 train loss 17540.26953125 val loss 49364.80859375\n",
      "Epoch 22590 train loss 17526.396484375 val loss 49432.62890625\n",
      "Epoch 22600 train loss 17516.30078125 val loss 49415.97265625\n",
      "Epoch 22610 train loss 17510.869140625 val loss 49377.4375\n",
      "Epoch 22620 train loss 17519.53125 val loss 49386.3828125\n",
      "Epoch 22630 train loss 17508.3828125 val loss 49303.73828125\n",
      "Epoch 22640 train loss 17506.533203125 val loss 49264.71484375\n",
      "Epoch 22650 train loss 17502.1875 val loss 49232.30859375\n",
      "Epoch 22660 train loss 17515.208984375 val loss 49245.7265625\n",
      "Epoch 22670 train loss 17508.873046875 val loss 49166.9296875\n",
      "Epoch 22680 train loss 17495.5 val loss 49169.984375\n",
      "Epoch 22690 train loss 17493.03125 val loss 49154.33203125\n",
      "Epoch 22700 train loss 17494.626953125 val loss 49131.08984375\n",
      "Epoch 22710 train loss 17495.6875 val loss 49105.3203125\n",
      "Epoch 22720 train loss 17503.7421875 val loss 49072.625\n",
      "Epoch 22730 train loss 17488.29296875 val loss 49082.7421875\n",
      "Epoch 22740 train loss 17483.6171875 val loss 49063.81640625\n",
      "Epoch 22750 train loss 17482.9453125 val loss 49041.078125\n",
      "Epoch 22760 train loss 17486.94140625 val loss 48997.4140625\n",
      "Epoch 22770 train loss 17478.68359375 val loss 49004.72265625\n",
      "Epoch 22780 train loss 17479.0859375 val loss 48975.6171875\n",
      "Epoch 22790 train loss 17489.408203125 val loss 48952.3359375\n",
      "Epoch 22800 train loss 17472.595703125 val loss 48997.21875\n",
      "Epoch 22810 train loss 17480.111328125 val loss 49001.19140625\n",
      "Epoch 22820 train loss 17468.751953125 val loss 48939.5234375\n",
      "Epoch 22830 train loss 17478.421875 val loss 48902.16015625\n",
      "Epoch 22840 train loss 17462.462890625 val loss 48916.0859375\n",
      "Epoch 22850 train loss 17487.640625 val loss 48952.10546875\n",
      "Epoch 22860 train loss 17464.1328125 val loss 48858.26953125\n",
      "Epoch 22870 train loss 17456.96875 val loss 48851.359375\n",
      "Epoch 22880 train loss 17457.501953125 val loss 48853.37109375\n",
      "Epoch 22890 train loss 17458.265625 val loss 48801.6171875\n",
      "Epoch 22900 train loss 17456.0546875 val loss 48820.4453125\n",
      "Epoch 22910 train loss 17456.404296875 val loss 48797.43359375\n",
      "Epoch 22920 train loss 17455.216796875 val loss 48816.44140625\n",
      "Epoch 22930 train loss 17453.21484375 val loss 48770.6328125\n",
      "Epoch 22940 train loss 17445.87890625 val loss 48794.43359375\n",
      "Epoch 22950 train loss 17454.939453125 val loss 48814.73828125\n",
      "Epoch 22960 train loss 17439.76171875 val loss 48763.4765625\n",
      "Epoch 22970 train loss 17454.9921875 val loss 48732.78125\n",
      "Epoch 22980 train loss 17436.47265625 val loss 48733.9765625\n",
      "Epoch 22990 train loss 17442.361328125 val loss 48747.85546875\n",
      "Epoch 23000 train loss 17434.71875 val loss 48732.07421875\n",
      "Epoch 23010 train loss 17445.416015625 val loss 48725.265625\n",
      "Epoch 23020 train loss 17430.244140625 val loss 48703.10546875\n",
      "Epoch 23030 train loss 17440.74609375 val loss 48676.7265625\n",
      "Epoch 23040 train loss 17426.07421875 val loss 48689.66796875\n",
      "Epoch 23050 train loss 17423.646484375 val loss 48647.58203125\n",
      "Epoch 23060 train loss 17434.96484375 val loss 48691.93359375\n",
      "Epoch 23070 train loss 17425.578125 val loss 48672.94921875\n",
      "Epoch 23080 train loss 17425.19921875 val loss 48636.5\n",
      "Epoch 23090 train loss 17421.51171875 val loss 48648.4921875\n",
      "Epoch 23100 train loss 17423.47265625 val loss 48628.203125\n",
      "Epoch 23110 train loss 17420.232421875 val loss 48603.515625\n",
      "Epoch 23120 train loss 17423.494140625 val loss 48597.0703125\n",
      "Epoch 23130 train loss 17411.46875 val loss 48621.59765625\n",
      "Epoch 23140 train loss 17410.13671875 val loss 48588.81640625\n",
      "Epoch 23150 train loss 17415.6875 val loss 48624.8203125\n",
      "Epoch 23160 train loss 17412.755859375 val loss 48598.14453125\n",
      "Epoch 23170 train loss 17410.8828125 val loss 48586.35546875\n",
      "Epoch 23180 train loss 17415.8203125 val loss 48588.15625\n",
      "Epoch 23190 train loss 17402.2578125 val loss 48566.94921875\n",
      "Epoch 23200 train loss 17409.6796875 val loss 48552.5625\n",
      "Epoch 23210 train loss 17399.919921875 val loss 48520.66015625\n",
      "Epoch 23220 train loss 17397.375 val loss 48523.84375\n",
      "Epoch 23230 train loss 17431.958984375 val loss 48558.31640625\n",
      "Epoch 23240 train loss 17422.552734375 val loss 48473.33203125\n",
      "Epoch 23250 train loss 17404.751953125 val loss 48509.57421875\n",
      "Epoch 23260 train loss 17396.998046875 val loss 48460.34765625\n",
      "Epoch 23270 train loss 17389.0703125 val loss 48489.76171875\n",
      "Epoch 23280 train loss 17388.552734375 val loss 48448.3515625\n",
      "Epoch 23290 train loss 17394.236328125 val loss 48472.59765625\n",
      "Epoch 23300 train loss 17383.951171875 val loss 48485.90234375\n",
      "Epoch 23310 train loss 17387.82421875 val loss 48535.23828125\n",
      "Epoch 23320 train loss 17383.7578125 val loss 48492.46875\n",
      "Epoch 23330 train loss 17380.201171875 val loss 48457.6484375\n",
      "Epoch 23340 train loss 17383.0703125 val loss 48498.640625\n",
      "Epoch 23350 train loss 17391.380859375 val loss 48474.234375\n",
      "Epoch 23360 train loss 17377.66796875 val loss 48442.65625\n",
      "Epoch 23370 train loss 17372.037109375 val loss 48458.5\n",
      "Epoch 23380 train loss 17384.2421875 val loss 48492.31640625\n",
      "Epoch 23390 train loss 17370.083984375 val loss 48484.15234375\n",
      "Epoch 23400 train loss 17375.71484375 val loss 48403.73046875\n",
      "Epoch 23410 train loss 17366.201171875 val loss 48449.734375\n",
      "Epoch 23420 train loss 17380.45703125 val loss 48404.3203125\n",
      "Epoch 23430 train loss 17361.3671875 val loss 48430.8984375\n",
      "Epoch 23440 train loss 17372.74609375 val loss 48445.1796875\n",
      "Epoch 23450 train loss 17358.62109375 val loss 48427.89453125\n",
      "Epoch 23460 train loss 17372.69140625 val loss 48399.578125\n",
      "Epoch 23470 train loss 17358.29296875 val loss 48455.390625\n",
      "Epoch 23480 train loss 17353.80859375 val loss 48381.80859375\n",
      "Epoch 23490 train loss 17360.751953125 val loss 48404.90234375\n",
      "Epoch 23500 train loss 17351.015625 val loss 48424.97265625\n",
      "Epoch 23510 train loss 17351.943359375 val loss 48380.5234375\n",
      "Epoch 23520 train loss 17357.65625 val loss 48411.25\n",
      "Epoch 23530 train loss 17347.572265625 val loss 48414.86328125\n",
      "Epoch 23540 train loss 17345.7109375 val loss 48388.4140625\n",
      "Epoch 23550 train loss 17363.244140625 val loss 48439.05859375\n",
      "Epoch 23560 train loss 17343.91796875 val loss 48362.9765625\n",
      "Epoch 23570 train loss 17345.08203125 val loss 48373.6875\n",
      "Epoch 23580 train loss 17341.212890625 val loss 48351.06640625\n",
      "Epoch 23590 train loss 17355.595703125 val loss 48360.55078125\n",
      "Epoch 23600 train loss 17350.26953125 val loss 48370.32421875\n",
      "Epoch 23610 train loss 17346.9140625 val loss 48341.6171875\n",
      "Epoch 23620 train loss 17341.365234375 val loss 48352.296875\n",
      "Epoch 23630 train loss 17338.146484375 val loss 48334.4140625\n",
      "Epoch 23640 train loss 17329.412109375 val loss 48353.03515625\n",
      "Epoch 23650 train loss 17334.5390625 val loss 48341.2734375\n",
      "Epoch 23660 train loss 17329.220703125 val loss 48326.87890625\n",
      "Epoch 23670 train loss 17325.4375 val loss 48333.33984375\n",
      "Epoch 23680 train loss 17339.19140625 val loss 48343.60546875\n",
      "Epoch 23690 train loss 17321.96875 val loss 48316.5390625\n",
      "Epoch 23700 train loss 17330.638671875 val loss 48282.6328125\n",
      "Epoch 23710 train loss 17321.005859375 val loss 48268.21875\n",
      "Epoch 23720 train loss 17318.35546875 val loss 48288.85546875\n",
      "Epoch 23730 train loss 17339.091796875 val loss 48324.47265625\n",
      "Epoch 23740 train loss 17323.021484375 val loss 48268.7109375\n",
      "Epoch 23750 train loss 17315.08203125 val loss 48292.04296875\n",
      "Epoch 23760 train loss 17317.18359375 val loss 48312.5546875\n",
      "Epoch 23770 train loss 17315.5 val loss 48244.125\n",
      "Epoch 23780 train loss 17309.466796875 val loss 48257.03515625\n",
      "Epoch 23790 train loss 17334.1171875 val loss 48309.8046875\n",
      "Epoch 23800 train loss 17314.94921875 val loss 48176.85546875\n",
      "Epoch 23810 train loss 17307.056640625 val loss 48243.578125\n",
      "Epoch 23820 train loss 17308.826171875 val loss 48228.33203125\n",
      "Epoch 23830 train loss 17304.65625 val loss 48207.453125\n",
      "Epoch 23840 train loss 17317.30859375 val loss 48264.05078125\n",
      "Epoch 23850 train loss 17301.41796875 val loss 48221.93359375\n",
      "Epoch 23860 train loss 17311.095703125 val loss 48201.51171875\n",
      "Epoch 23870 train loss 17298.560546875 val loss 48210.98828125\n",
      "Epoch 23880 train loss 17306.279296875 val loss 48196.234375\n",
      "Epoch 23890 train loss 17296.359375 val loss 48185.421875\n",
      "Epoch 23900 train loss 17304.86328125 val loss 48182.625\n",
      "Epoch 23910 train loss 17293.25 val loss 48190.0703125\n",
      "Epoch 23920 train loss 17303.009765625 val loss 48198.609375\n",
      "Epoch 23930 train loss 17294.86328125 val loss 48180.2265625\n",
      "Epoch 23940 train loss 17289.1328125 val loss 48143.87890625\n",
      "Epoch 23950 train loss 17308.984375 val loss 48161.6484375\n",
      "Epoch 23960 train loss 17304.59765625 val loss 48191.87109375\n",
      "Epoch 23970 train loss 17293.498046875 val loss 48152.81640625\n",
      "Epoch 23980 train loss 17283.482421875 val loss 48154.08984375\n",
      "Epoch 23990 train loss 17282.353515625 val loss 48130.6796875\n",
      "Epoch 24000 train loss 17291.15625 val loss 48117.96484375\n",
      "Epoch 24010 train loss 17279.712890625 val loss 48148.62109375\n",
      "Epoch 24020 train loss 17281.453125 val loss 48128.65234375\n",
      "Epoch 24030 train loss 17285.861328125 val loss 48117.4296875\n",
      "Epoch 24040 train loss 17288.63671875 val loss 48163.67578125\n",
      "Epoch 24050 train loss 17276.359375 val loss 48111.71484375\n",
      "Epoch 24060 train loss 17279.4765625 val loss 48122.22265625\n",
      "Epoch 24070 train loss 17274.806640625 val loss 48095.78125\n",
      "Epoch 24080 train loss 17276.330078125 val loss 48093.26171875\n",
      "Epoch 24090 train loss 17276.044921875 val loss 48084.52734375\n",
      "Epoch 24100 train loss 17274.025390625 val loss 48103.453125\n",
      "Epoch 24110 train loss 17276.21484375 val loss 48084.82421875\n",
      "Epoch 24120 train loss 17284.5859375 val loss 48098.2578125\n",
      "Epoch 24130 train loss 17264.939453125 val loss 48079.51953125\n",
      "Epoch 24140 train loss 17265.326171875 val loss 48075.421875\n",
      "Epoch 24150 train loss 17284.31640625 val loss 48065.83203125\n",
      "Epoch 24160 train loss 17280.912109375 val loss 48121.24609375\n",
      "Epoch 24170 train loss 17271.44921875 val loss 48050.83984375\n",
      "Epoch 24180 train loss 17260.443359375 val loss 48050.7265625\n",
      "Epoch 24190 train loss 17261.974609375 val loss 48059.9765625\n",
      "Epoch 24200 train loss 17261.689453125 val loss 48046.09375\n",
      "Epoch 24210 train loss 17255.3203125 val loss 48037.15234375\n",
      "Epoch 24220 train loss 17261.072265625 val loss 48054.859375\n",
      "Epoch 24230 train loss 17254.2734375 val loss 48014.328125\n",
      "Epoch 24240 train loss 17251.16015625 val loss 48022.19140625\n",
      "Epoch 24250 train loss 17268.80078125 val loss 47999.0\n",
      "Epoch 24260 train loss 17250.15234375 val loss 48019.44921875\n",
      "Epoch 24270 train loss 17248.314453125 val loss 48005.09375\n",
      "Epoch 24280 train loss 17250.880859375 val loss 47992.24609375\n",
      "Epoch 24290 train loss 17250.32421875 val loss 48042.15625\n",
      "Epoch 24300 train loss 17248.515625 val loss 48010.68359375\n",
      "Epoch 24310 train loss 17244.541015625 val loss 47978.41796875\n",
      "Epoch 24320 train loss 17241.91796875 val loss 47983.5390625\n",
      "Epoch 24330 train loss 17264.302734375 val loss 47950.2734375\n",
      "Epoch 24340 train loss 17244.72265625 val loss 48017.03515625\n",
      "Epoch 24350 train loss 17242.224609375 val loss 47962.44140625\n",
      "Epoch 24360 train loss 17241.94921875 val loss 47951.734375\n",
      "Epoch 24370 train loss 17236.013671875 val loss 47985.1875\n",
      "Epoch 24380 train loss 17252.376953125 val loss 48018.60546875\n",
      "Epoch 24390 train loss 17234.107421875 val loss 47965.08203125\n",
      "Epoch 24400 train loss 17233.94921875 val loss 47975.609375\n",
      "Epoch 24410 train loss 17236.353515625 val loss 47939.04296875\n",
      "Epoch 24420 train loss 17237.17578125 val loss 47950.98046875\n",
      "Epoch 24430 train loss 17231.986328125 val loss 47972.48828125\n",
      "Epoch 24440 train loss 17234.955078125 val loss 47991.1015625\n",
      "Epoch 24450 train loss 17226.427734375 val loss 47915.40625\n",
      "Epoch 24460 train loss 17239.27734375 val loss 47914.32421875\n",
      "Epoch 24470 train loss 17226.337890625 val loss 47946.23046875\n",
      "Epoch 24480 train loss 17234.580078125 val loss 47917.64453125\n",
      "Epoch 24490 train loss 17225.087890625 val loss 47895.09375\n",
      "Epoch 24500 train loss 17222.29296875 val loss 47926.0\n",
      "Epoch 24510 train loss 17218.591796875 val loss 47890.02734375\n",
      "Epoch 24520 train loss 17216.7734375 val loss 47919.2109375\n",
      "Epoch 24530 train loss 17243.97265625 val loss 47875.4765625\n",
      "Epoch 24540 train loss 17217.328125 val loss 47940.21484375\n",
      "Epoch 24550 train loss 17214.08203125 val loss 47924.57421875\n",
      "Epoch 24560 train loss 17215.876953125 val loss 47887.11328125\n",
      "Epoch 24570 train loss 17218.19921875 val loss 47938.23046875\n",
      "Epoch 24580 train loss 17208.03515625 val loss 47912.44921875\n",
      "Epoch 24590 train loss 17209.953125 val loss 47904.56640625\n",
      "Epoch 24600 train loss 17205.234375 val loss 47918.2265625\n",
      "Epoch 24610 train loss 17219.099609375 val loss 47928.63671875\n",
      "Epoch 24620 train loss 17205.5859375 val loss 47897.90234375\n",
      "Epoch 24630 train loss 17221.27734375 val loss 47851.2265625\n",
      "Epoch 24640 train loss 17202.83984375 val loss 47916.03125\n",
      "Epoch 24650 train loss 17202.087890625 val loss 47909.6875\n",
      "Epoch 24660 train loss 17203.775390625 val loss 47873.00390625\n",
      "Epoch 24670 train loss 17199.490234375 val loss 47874.7734375\n",
      "Epoch 24680 train loss 17204.376953125 val loss 47917.265625\n",
      "Epoch 24690 train loss 17202.78515625 val loss 47873.90234375\n",
      "Epoch 24700 train loss 17190.4765625 val loss 47796.24609375\n",
      "Epoch 24710 train loss 17187.609375 val loss 47759.1328125\n",
      "Epoch 24720 train loss 17186.994140625 val loss 47707.7265625\n",
      "Epoch 24730 train loss 17205.5859375 val loss 47694.94140625\n",
      "Epoch 24740 train loss 17188.865234375 val loss 47607.84765625\n",
      "Epoch 24750 train loss 17175.515625 val loss 47602.48828125\n",
      "Epoch 24760 train loss 17172.185546875 val loss 47624.984375\n",
      "Epoch 24770 train loss 17171.80078125 val loss 47676.15234375\n",
      "Epoch 24780 train loss 17164.77734375 val loss 47658.5625\n",
      "Epoch 24790 train loss 17164.865234375 val loss 47646.07421875\n",
      "Epoch 24800 train loss 17170.861328125 val loss 47617.69140625\n",
      "Epoch 24810 train loss 17157.44921875 val loss 47630.359375\n",
      "Epoch 24820 train loss 17182.39453125 val loss 47660.35546875\n",
      "Epoch 24830 train loss 17169.0625 val loss 47563.5546875\n",
      "Epoch 24840 train loss 17157.83984375 val loss 47585.6875\n",
      "Epoch 24850 train loss 17151.611328125 val loss 47603.265625\n",
      "Epoch 24860 train loss 17149.244140625 val loss 47583.03125\n",
      "Epoch 24870 train loss 17144.068359375 val loss 47579.37109375\n",
      "Epoch 24880 train loss 17165.048828125 val loss 47539.36328125\n",
      "Epoch 24890 train loss 17142.591796875 val loss 47570.18359375\n",
      "Epoch 24900 train loss 17137.94921875 val loss 47554.03125\n",
      "Epoch 24910 train loss 17144.25 val loss 47535.44140625\n",
      "Epoch 24920 train loss 17133.58984375 val loss 47550.328125\n",
      "Epoch 24930 train loss 17144.681640625 val loss 47574.84375\n",
      "Epoch 24940 train loss 17131.328125 val loss 47525.6015625\n",
      "Epoch 24950 train loss 17142.591796875 val loss 47526.5703125\n",
      "Epoch 24960 train loss 17135.57421875 val loss 47532.5703125\n",
      "Epoch 24970 train loss 17126.62109375 val loss 47508.1484375\n",
      "Epoch 24980 train loss 17130.669921875 val loss 47509.21484375\n",
      "Epoch 24990 train loss 17122.65234375 val loss 47526.0078125\n",
      "Epoch 25000 train loss 17124.10546875 val loss 47530.42578125\n",
      "Epoch 25010 train loss 17134.669921875 val loss 47541.13671875\n",
      "Epoch 25020 train loss 17123.16796875 val loss 47497.87109375\n",
      "Epoch 25030 train loss 17118.0390625 val loss 47517.60546875\n",
      "Epoch 25040 train loss 17120.685546875 val loss 47505.01953125\n",
      "Epoch 25050 train loss 17113.115234375 val loss 47511.94140625\n",
      "Epoch 25060 train loss 17110.76171875 val loss 47477.19140625\n",
      "Epoch 25070 train loss 17139.873046875 val loss 47537.69140625\n",
      "Epoch 25080 train loss 17116.818359375 val loss 47488.703125\n",
      "Epoch 25090 train loss 17105.798828125 val loss 47539.328125\n",
      "Epoch 25100 train loss 17104.994140625 val loss 47540.234375\n",
      "Epoch 25110 train loss 17120.17578125 val loss 47469.28125\n",
      "Epoch 25120 train loss 17102.1640625 val loss 47518.8203125\n",
      "Epoch 25130 train loss 17103.8671875 val loss 47501.48828125\n",
      "Epoch 25140 train loss 17097.8828125 val loss 47528.546875\n",
      "Epoch 25150 train loss 17111.5 val loss 47550.08984375\n",
      "Epoch 25160 train loss 17093.908203125 val loss 47505.96484375\n",
      "Epoch 25170 train loss 17103.78515625 val loss 47490.359375\n",
      "Epoch 25180 train loss 17092.134765625 val loss 47521.34765625\n",
      "Epoch 25190 train loss 17098.46875 val loss 47513.88671875\n",
      "Epoch 25200 train loss 17088.916015625 val loss 47489.7265625\n",
      "Epoch 25210 train loss 17093.6484375 val loss 47467.4453125\n",
      "Epoch 25220 train loss 17100.32421875 val loss 47481.19921875\n",
      "Epoch 25230 train loss 17096.021484375 val loss 47495.9453125\n",
      "Epoch 25240 train loss 17083.2578125 val loss 47474.34765625\n",
      "Epoch 25250 train loss 17099.376953125 val loss 47457.71875\n",
      "Epoch 25260 train loss 17090.076171875 val loss 47480.6875\n",
      "Epoch 25270 train loss 17080.30078125 val loss 47445.8125\n",
      "Epoch 25280 train loss 17079.23046875 val loss 47394.2578125\n",
      "Epoch 25290 train loss 17090.935546875 val loss 47397.1171875\n",
      "Epoch 25300 train loss 17076.533203125 val loss 47426.0390625\n",
      "Epoch 25310 train loss 17095.5625 val loss 47422.19921875\n",
      "Epoch 25320 train loss 17087.697265625 val loss 47371.16796875\n",
      "Epoch 25330 train loss 17077.63671875 val loss 47374.1015625\n",
      "Epoch 25340 train loss 17070.17578125 val loss 47339.28125\n",
      "Epoch 25350 train loss 17079.1015625 val loss 47309.9453125\n",
      "Epoch 25360 train loss 17071.37109375 val loss 47344.83984375\n",
      "Epoch 25370 train loss 17062.201171875 val loss 47341.9921875\n",
      "Epoch 25380 train loss 17060.173828125 val loss 47370.81640625\n",
      "Epoch 25390 train loss 17072.66796875 val loss 47339.4296875\n",
      "Epoch 25400 train loss 17055.197265625 val loss 47325.296875\n",
      "Epoch 25410 train loss 17055.71484375 val loss 47320.6015625\n",
      "Epoch 25420 train loss 17071.091796875 val loss 47349.2109375\n",
      "Epoch 25430 train loss 17055.740234375 val loss 47310.0625\n",
      "Epoch 25440 train loss 17052.1796875 val loss 47322.140625\n",
      "Epoch 25450 train loss 17066.01171875 val loss 47330.0859375\n",
      "Epoch 25460 train loss 17045.802734375 val loss 47304.48046875\n",
      "Epoch 25470 train loss 17053.78515625 val loss 47273.109375\n",
      "Epoch 25480 train loss 17044.443359375 val loss 47268.98046875\n",
      "Epoch 25490 train loss 17038.4609375 val loss 47208.56640625\n",
      "Epoch 25500 train loss 17031.71875 val loss 47112.8515625\n",
      "Epoch 25510 train loss 17034.67578125 val loss 47071.02734375\n",
      "Epoch 25520 train loss 17017.78125 val loss 47092.7421875\n",
      "Epoch 25530 train loss 17020.1796875 val loss 47109.609375\n",
      "Epoch 25540 train loss 17021.796875 val loss 47080.28125\n",
      "Epoch 25550 train loss 17006.744140625 val loss 47076.74609375\n",
      "Epoch 25560 train loss 17017.076171875 val loss 47087.0390625\n",
      "Epoch 25570 train loss 17000.4453125 val loss 47100.9140625\n",
      "Epoch 25580 train loss 17018.705078125 val loss 47155.21875\n",
      "Epoch 25590 train loss 16996.318359375 val loss 47092.33984375\n",
      "Epoch 25600 train loss 17005.328125 val loss 47100.65625\n",
      "Epoch 25610 train loss 16995.53515625 val loss 47117.35546875\n",
      "Epoch 25620 train loss 16989.40625 val loss 47112.7734375\n",
      "Epoch 25630 train loss 16998.283203125 val loss 47108.859375\n",
      "Epoch 25640 train loss 16989.287109375 val loss 47112.78515625\n",
      "Epoch 25650 train loss 16982.052734375 val loss 47116.6953125\n",
      "Epoch 25660 train loss 16980.791015625 val loss 47123.17578125\n",
      "Epoch 25670 train loss 17019.8359375 val loss 47080.03125\n",
      "Epoch 25680 train loss 16990.306640625 val loss 47128.25\n",
      "Epoch 25690 train loss 16978.48046875 val loss 47109.6796875\n",
      "Epoch 25700 train loss 16973.072265625 val loss 47120.80859375\n",
      "Epoch 25710 train loss 16981.376953125 val loss 47144.03125\n",
      "Epoch 25720 train loss 16972.390625 val loss 47117.8984375\n",
      "Epoch 25730 train loss 16970.451171875 val loss 47106.07421875\n",
      "Epoch 25740 train loss 16987.869140625 val loss 47108.47265625\n",
      "Epoch 25750 train loss 16966.14453125 val loss 47115.4296875\n",
      "Epoch 25760 train loss 16974.869140625 val loss 47132.4140625\n",
      "Epoch 25770 train loss 16965.205078125 val loss 47124.70703125\n",
      "Epoch 25780 train loss 16982.775390625 val loss 47089.05859375\n",
      "Epoch 25790 train loss 16961.23828125 val loss 47130.7109375\n",
      "Epoch 25800 train loss 16959.884765625 val loss 47116.05859375\n",
      "Epoch 25810 train loss 16956.900390625 val loss 47115.60546875\n",
      "Epoch 25820 train loss 16968.787109375 val loss 47138.47265625\n",
      "Epoch 25830 train loss 16955.111328125 val loss 47134.00390625\n",
      "Epoch 25840 train loss 16958.712890625 val loss 47086.9296875\n",
      "Epoch 25850 train loss 16962.720703125 val loss 47106.93359375\n",
      "Epoch 25860 train loss 16955.990234375 val loss 47132.109375\n",
      "Epoch 25870 train loss 16953.611328125 val loss 47106.3515625\n",
      "Epoch 25880 train loss 16949.08984375 val loss 47112.21875\n",
      "Epoch 25890 train loss 16964.078125 val loss 47109.81640625\n",
      "Epoch 25900 train loss 16943.916015625 val loss 47117.578125\n",
      "Epoch 25910 train loss 16958.1796875 val loss 47102.9140625\n",
      "Epoch 25920 train loss 16939.947265625 val loss 47096.48828125\n",
      "Epoch 25930 train loss 16951.240234375 val loss 47095.453125\n",
      "Epoch 25940 train loss 16937.501953125 val loss 47093.58984375\n",
      "Epoch 25950 train loss 16937.955078125 val loss 47098.71875\n",
      "Epoch 25960 train loss 16960.3125 val loss 47114.65625\n",
      "Epoch 25970 train loss 16949.177734375 val loss 47102.30859375\n",
      "Epoch 25980 train loss 16931.9921875 val loss 47103.39453125\n",
      "Epoch 25990 train loss 16936.734375 val loss 47099.34375\n",
      "Epoch 26000 train loss 16946.998046875 val loss 47083.4765625\n",
      "Epoch 26010 train loss 16932.517578125 val loss 47126.4765625\n",
      "Epoch 26020 train loss 16927.369140625 val loss 47099.53125\n",
      "Epoch 26030 train loss 16925.322265625 val loss 47093.27734375\n",
      "Epoch 26040 train loss 16945.568359375 val loss 47123.87890625\n",
      "Epoch 26050 train loss 16923.30078125 val loss 47102.99609375\n",
      "Epoch 26060 train loss 16922.267578125 val loss 47079.1875\n",
      "Epoch 26070 train loss 16925.666015625 val loss 47093.625\n",
      "Epoch 26080 train loss 16919.505859375 val loss 47096.94921875\n",
      "Epoch 26090 train loss 16915.494140625 val loss 47088.6640625\n",
      "Epoch 26100 train loss 16931.248046875 val loss 47113.140625\n",
      "Epoch 26110 train loss 16915.0625 val loss 47078.65234375\n",
      "Epoch 26120 train loss 16919.673828125 val loss 47039.7890625\n",
      "Epoch 26130 train loss 16912.986328125 val loss 47092.4453125\n",
      "Epoch 26140 train loss 16913.9296875 val loss 47097.0625\n",
      "Epoch 26150 train loss 16916.54296875 val loss 47077.94921875\n",
      "Epoch 26160 train loss 16908.779296875 val loss 47077.11328125\n",
      "Epoch 26170 train loss 16917.02734375 val loss 47093.8671875\n",
      "Epoch 26180 train loss 16911.57421875 val loss 47055.16015625\n",
      "Epoch 26190 train loss 16915.427734375 val loss 47044.546875\n",
      "Epoch 26200 train loss 16900.85546875 val loss 47053.21484375\n",
      "Epoch 26210 train loss 16923.798828125 val loss 47083.265625\n",
      "Epoch 26220 train loss 16898.44140625 val loss 47062.28125\n",
      "Epoch 26230 train loss 16901.169921875 val loss 47027.875\n",
      "Epoch 26240 train loss 16905.189453125 val loss 47058.55859375\n",
      "Epoch 26250 train loss 16896.294921875 val loss 47016.0078125\n",
      "Epoch 26260 train loss 16905.755859375 val loss 47027.68359375\n",
      "Epoch 26270 train loss 16896.640625 val loss 47043.8515625\n",
      "Epoch 26280 train loss 16898.55078125 val loss 47063.65234375\n",
      "Epoch 26290 train loss 16894.54296875 val loss 47047.71484375\n",
      "Epoch 26300 train loss 16899.544921875 val loss 47001.3046875\n",
      "Epoch 26310 train loss 16888.10546875 val loss 46999.09765625\n",
      "Epoch 26320 train loss 16890.66796875 val loss 46989.83984375\n",
      "Epoch 26330 train loss 16891.3359375 val loss 47016.50390625\n",
      "Epoch 26340 train loss 16884.892578125 val loss 47014.984375\n",
      "Epoch 26350 train loss 16879.298828125 val loss 46990.96875\n",
      "Epoch 26360 train loss 16884.189453125 val loss 46962.35546875\n",
      "Epoch 26370 train loss 16887.36328125 val loss 46955.3671875\n",
      "Epoch 26380 train loss 16881.892578125 val loss 46980.48046875\n",
      "Epoch 26390 train loss 16872.439453125 val loss 46959.51953125\n",
      "Epoch 26400 train loss 16894.302734375 val loss 46916.7890625\n",
      "Epoch 26410 train loss 16869.787109375 val loss 46945.72265625\n",
      "Epoch 26420 train loss 16863.39453125 val loss 46947.94921875\n",
      "Epoch 26430 train loss 16864.86328125 val loss 46910.97265625\n",
      "Epoch 26440 train loss 16863.876953125 val loss 46902.12890625\n",
      "Epoch 26450 train loss 16859.5859375 val loss 46903.4609375\n",
      "Epoch 26460 train loss 16860.822265625 val loss 46934.01953125\n",
      "Epoch 26470 train loss 16870.45703125 val loss 46906.04296875\n",
      "Epoch 26480 train loss 16851.88671875 val loss 46897.92578125\n",
      "Epoch 26490 train loss 16869.1953125 val loss 46855.421875\n",
      "Epoch 26500 train loss 16848.740234375 val loss 46870.1796875\n",
      "Epoch 26510 train loss 16858.404296875 val loss 46888.58203125\n",
      "Epoch 26520 train loss 16847.271484375 val loss 46836.12109375\n",
      "Epoch 26530 train loss 16857.58984375 val loss 46836.98046875\n",
      "Epoch 26540 train loss 16839.890625 val loss 46843.68359375\n",
      "Epoch 26550 train loss 16841.47265625 val loss 46857.91796875\n",
      "Epoch 26560 train loss 16863.455078125 val loss 46855.49609375\n",
      "Epoch 26570 train loss 16836.0 val loss 46824.2109375\n",
      "Epoch 26580 train loss 16846.48828125 val loss 46801.96484375\n",
      "Epoch 26590 train loss 16835.220703125 val loss 46833.734375\n",
      "Epoch 26600 train loss 16835.546875 val loss 46812.2578125\n",
      "Epoch 26610 train loss 16831.60546875 val loss 46791.828125\n",
      "Epoch 26620 train loss 16828.7734375 val loss 46802.80078125\n",
      "Epoch 26630 train loss 16844.78125 val loss 46841.8046875\n",
      "Epoch 26640 train loss 16821.197265625 val loss 46816.58984375\n",
      "Epoch 26650 train loss 16828.166015625 val loss 46783.82421875\n",
      "Epoch 26660 train loss 16819.271484375 val loss 46790.32421875\n",
      "Epoch 26670 train loss 16832.546875 val loss 46852.90625\n",
      "Epoch 26680 train loss 16815.83203125 val loss 46823.8671875\n",
      "Epoch 26690 train loss 16831.078125 val loss 46799.5390625\n",
      "Epoch 26700 train loss 16811.390625 val loss 46822.58984375\n",
      "Epoch 26710 train loss 16823.85546875 val loss 46818.78125\n",
      "Epoch 26720 train loss 16807.431640625 val loss 46794.328125\n",
      "Epoch 26730 train loss 16825.73828125 val loss 46777.11328125\n",
      "Epoch 26740 train loss 16804.423828125 val loss 46802.90625\n",
      "Epoch 26750 train loss 16812.0546875 val loss 46797.55078125\n",
      "Epoch 26760 train loss 16807.259765625 val loss 46761.7421875\n",
      "Epoch 26770 train loss 16802.177734375 val loss 46786.1875\n",
      "Epoch 26780 train loss 16806.51171875 val loss 46783.6484375\n",
      "Epoch 26790 train loss 16799.501953125 val loss 46779.78515625\n",
      "Epoch 26800 train loss 16805.021484375 val loss 46812.94921875\n",
      "Epoch 26810 train loss 16797.4765625 val loss 46765.1640625\n",
      "Epoch 26820 train loss 16807.046875 val loss 46754.265625\n",
      "Epoch 26830 train loss 16789.5 val loss 46785.96484375\n",
      "Epoch 26840 train loss 16790.18359375 val loss 46761.6015625\n",
      "Epoch 26850 train loss 16792.9921875 val loss 46767.8046875\n",
      "Epoch 26860 train loss 16800.04296875 val loss 46759.3515625\n",
      "Epoch 26870 train loss 16785.8828125 val loss 46771.390625\n",
      "Epoch 26880 train loss 16796.07421875 val loss 46801.88671875\n",
      "Epoch 26890 train loss 16780.47265625 val loss 46772.58203125\n",
      "Epoch 26900 train loss 16797.71484375 val loss 46760.46875\n",
      "Epoch 26910 train loss 16777.837890625 val loss 46749.98828125\n",
      "Epoch 26920 train loss 16777.85546875 val loss 46780.046875\n",
      "Epoch 26930 train loss 16783.5078125 val loss 46781.88671875\n",
      "Epoch 26940 train loss 16781.15625 val loss 46788.03125\n",
      "Epoch 26950 train loss 16780.423828125 val loss 46754.7109375\n",
      "Epoch 26960 train loss 16773.970703125 val loss 46784.58984375\n",
      "Epoch 26970 train loss 16772.6015625 val loss 46783.62890625\n",
      "Epoch 26980 train loss 16769.66015625 val loss 46769.03125\n",
      "Epoch 26990 train loss 16789.375 val loss 46808.078125\n",
      "Epoch 27000 train loss 16764.521484375 val loss 46809.55078125\n",
      "Epoch 27010 train loss 16770.7578125 val loss 46778.42578125\n",
      "Epoch 27020 train loss 16761.689453125 val loss 46748.5078125\n",
      "Epoch 27030 train loss 16780.208984375 val loss 46780.921875\n",
      "Epoch 27040 train loss 16758.3359375 val loss 46760.33984375\n",
      "Epoch 27050 train loss 16766.453125 val loss 46786.34765625\n",
      "Epoch 27060 train loss 16757.390625 val loss 46796.3359375\n",
      "Epoch 27070 train loss 16757.88671875 val loss 46760.90625\n",
      "Epoch 27080 train loss 16781.359375 val loss 46800.76953125\n",
      "Epoch 27090 train loss 16753.60546875 val loss 46746.3359375\n",
      "Epoch 27100 train loss 16756.462890625 val loss 46765.5859375\n",
      "Epoch 27110 train loss 16753.5625 val loss 46782.48828125\n",
      "Epoch 27120 train loss 16759.638671875 val loss 46795.77734375\n",
      "Epoch 27130 train loss 16748.171875 val loss 46789.5546875\n",
      "Epoch 27140 train loss 16759.29296875 val loss 46813.17578125\n",
      "Epoch 27150 train loss 16741.53515625 val loss 46796.2265625\n",
      "Epoch 27160 train loss 16747.208984375 val loss 46763.06640625\n",
      "Epoch 27170 train loss 16741.94140625 val loss 46795.1796875\n",
      "Epoch 27180 train loss 16758.31640625 val loss 46806.8125\n",
      "Epoch 27190 train loss 16742.763671875 val loss 46774.2734375\n",
      "Epoch 27200 train loss 16734.9921875 val loss 46824.27734375\n",
      "Epoch 27210 train loss 16740.609375 val loss 46825.62109375\n",
      "Epoch 27220 train loss 16732.56640625 val loss 46828.28125\n",
      "Epoch 27230 train loss 16733.865234375 val loss 46821.55859375\n",
      "Epoch 27240 train loss 16768.353515625 val loss 46830.29296875\n",
      "Epoch 27250 train loss 16747.099609375 val loss 46779.87890625\n",
      "Epoch 27260 train loss 16732.96875 val loss 46836.1640625\n",
      "Epoch 27270 train loss 16726.53125 val loss 46806.7734375\n",
      "Epoch 27280 train loss 16721.91015625 val loss 46797.2578125\n",
      "Epoch 27290 train loss 16727.927734375 val loss 46820.08203125\n",
      "Epoch 27300 train loss 16731.521484375 val loss 46831.44140625\n",
      "Epoch 27310 train loss 16719.671875 val loss 46808.46875\n",
      "Epoch 27320 train loss 16738.8125 val loss 46824.62109375\n",
      "Epoch 27330 train loss 16718.794921875 val loss 46842.85546875\n",
      "Epoch 27340 train loss 16727.3671875 val loss 46860.734375\n",
      "Epoch 27350 train loss 16714.955078125 val loss 46845.33984375\n",
      "Epoch 27360 train loss 16711.455078125 val loss 46872.6484375\n",
      "Epoch 27370 train loss 16713.40625 val loss 46862.1171875\n",
      "Epoch 27380 train loss 16764.8125 val loss 46861.8359375\n",
      "Epoch 27390 train loss 16728.2265625 val loss 46905.23046875\n",
      "Epoch 27400 train loss 16707.97265625 val loss 46891.26171875\n",
      "Epoch 27410 train loss 16725.9375 val loss 46907.515625\n",
      "Epoch 27420 train loss 16706.126953125 val loss 46925.32421875\n",
      "Epoch 27430 train loss 16717.322265625 val loss 46928.33984375\n",
      "Epoch 27440 train loss 16703.013671875 val loss 46931.15625\n",
      "Epoch 27450 train loss 16704.82421875 val loss 46937.44921875\n",
      "Epoch 27460 train loss 16698.0703125 val loss 46922.3828125\n",
      "Epoch 27470 train loss 16730.23046875 val loss 46990.2421875\n",
      "Epoch 27480 train loss 16714.306640625 val loss 46926.09375\n",
      "Epoch 27490 train loss 16701.724609375 val loss 46983.43359375\n",
      "Epoch 27500 train loss 16695.28125 val loss 46945.33203125\n",
      "Epoch 27510 train loss 16700.001953125 val loss 46959.7421875\n",
      "Epoch 27520 train loss 16691.533203125 val loss 46961.90234375\n",
      "Epoch 27530 train loss 16687.7421875 val loss 46969.94921875\n",
      "Epoch 27540 train loss 16728.71484375 val loss 47005.58203125\n",
      "Epoch 27550 train loss 16704.9921875 val loss 46960.5625\n",
      "Epoch 27560 train loss 16711.70703125 val loss 47018.48828125\n",
      "Epoch 27570 train loss 16705.724609375 val loss 46958.6328125\n",
      "Epoch 27580 train loss 16699.748046875 val loss 46999.859375\n",
      "Epoch 27590 train loss 16685.025390625 val loss 46963.3515625\n",
      "Epoch 27600 train loss 16675.3828125 val loss 47008.12109375\n",
      "Epoch 27610 train loss 16671.958984375 val loss 47000.46875\n",
      "Epoch 27620 train loss 16669.9453125 val loss 47034.20703125\n",
      "Epoch 27630 train loss 16665.720703125 val loss 46998.9765625\n",
      "Epoch 27640 train loss 16661.17578125 val loss 46995.8046875\n",
      "Epoch 27650 train loss 16658.6953125 val loss 46999.9609375\n",
      "Epoch 27660 train loss 16658.484375 val loss 46971.92578125\n",
      "Epoch 27670 train loss 16653.94921875 val loss 46973.125\n",
      "Epoch 27680 train loss 16654.041015625 val loss 47009.3828125\n",
      "Epoch 27690 train loss 16744.458984375 val loss 46993.6796875\n",
      "Epoch 27700 train loss 16710.455078125 val loss 47054.5703125\n",
      "Epoch 27710 train loss 16677.96484375 val loss 47010.7734375\n",
      "Epoch 27720 train loss 16647.70703125 val loss 47005.72265625\n",
      "Epoch 27730 train loss 16644.705078125 val loss 47022.87109375\n",
      "Epoch 27740 train loss 16643.154296875 val loss 47033.33984375\n",
      "Epoch 27750 train loss 16642.0625 val loss 47024.9296875\n",
      "Epoch 27760 train loss 16648.36328125 val loss 47046.703125\n",
      "Epoch 27770 train loss 16640.5390625 val loss 47099.01953125\n",
      "Epoch 27780 train loss 16639.509765625 val loss 47058.54296875\n",
      "Epoch 27790 train loss 16639.17578125 val loss 47048.2109375\n",
      "Epoch 27800 train loss 16640.310546875 val loss 47055.36328125\n",
      "Epoch 27810 train loss 16635.015625 val loss 47077.6640625\n",
      "Epoch 27820 train loss 16634.482421875 val loss 47081.35546875\n",
      "Epoch 27830 train loss 16662.314453125 val loss 47120.2734375\n",
      "Epoch 27840 train loss 16641.876953125 val loss 47081.3125\n",
      "Epoch 27850 train loss 16626.88671875 val loss 47121.08984375\n",
      "Epoch 27860 train loss 16626.404296875 val loss 47125.375\n",
      "Epoch 27870 train loss 16626.322265625 val loss 47124.86328125\n",
      "Epoch 27880 train loss 16625.92578125 val loss 47106.4921875\n",
      "Epoch 27890 train loss 16640.306640625 val loss 47128.734375\n",
      "Epoch 27900 train loss 16617.974609375 val loss 47137.41796875\n",
      "Epoch 27910 train loss 16646.771484375 val loss 47148.16796875\n",
      "Epoch 27920 train loss 16613.9609375 val loss 47147.6015625\n",
      "Epoch 27930 train loss 16625.97265625 val loss 47185.61328125\n",
      "Epoch 27940 train loss 16610.28515625 val loss 47190.83203125\n",
      "Epoch 27950 train loss 16612.6640625 val loss 47179.1640625\n",
      "Epoch 27960 train loss 16608.955078125 val loss 47193.21484375\n",
      "Epoch 27970 train loss 16636.455078125 val loss 47235.2890625\n",
      "Epoch 27980 train loss 16625.462890625 val loss 47201.265625\n",
      "Epoch 27990 train loss 16603.927734375 val loss 47228.30859375\n",
      "Epoch 28000 train loss 16604.76171875 val loss 47196.1484375\n",
      "Epoch 28010 train loss 16608.78125 val loss 47246.9609375\n",
      "Epoch 28020 train loss 16608.39453125 val loss 47217.375\n",
      "Epoch 28030 train loss 16617.32421875 val loss 47255.453125\n",
      "Epoch 28040 train loss 16597.05078125 val loss 47240.66015625\n",
      "Epoch 28050 train loss 16599.28515625 val loss 47249.76171875\n",
      "Epoch 28060 train loss 16596.40625 val loss 47260.06640625\n",
      "Epoch 28070 train loss 16597.951171875 val loss 47301.25390625\n",
      "Epoch 28080 train loss 16592.4140625 val loss 47282.7734375\n",
      "Epoch 28090 train loss 16597.263671875 val loss 47302.9609375\n",
      "Epoch 28100 train loss 16601.34375 val loss 47280.37890625\n",
      "Epoch 28110 train loss 16595.244140625 val loss 47304.40625\n",
      "Epoch 28120 train loss 16587.35546875 val loss 47296.15234375\n",
      "Epoch 28130 train loss 16583.8125 val loss 47323.36328125\n",
      "Epoch 28140 train loss 16583.15234375 val loss 47317.07421875\n",
      "Epoch 28150 train loss 16594.1640625 val loss 47317.77734375\n",
      "Epoch 28160 train loss 16595.666015625 val loss 47348.63671875\n",
      "Epoch 28170 train loss 16593.578125 val loss 47370.5859375\n",
      "Epoch 28180 train loss 16579.115234375 val loss 47326.0078125\n",
      "Epoch 28190 train loss 16605.296875 val loss 47354.52734375\n",
      "Epoch 28200 train loss 16579.017578125 val loss 47403.67578125\n",
      "Epoch 28210 train loss 16574.076171875 val loss 47391.90625\n",
      "Epoch 28220 train loss 16578.578125 val loss 47419.9921875\n",
      "Epoch 28230 train loss 16573.896484375 val loss 47365.66015625\n",
      "Epoch 28240 train loss 16609.76171875 val loss 47427.2109375\n",
      "Epoch 28250 train loss 16592.142578125 val loss 47404.17578125\n",
      "Epoch 28260 train loss 16576.5234375 val loss 47438.13671875\n",
      "Epoch 28270 train loss 16568.072265625 val loss 47453.90234375\n",
      "Epoch 28280 train loss 16567.646484375 val loss 47400.3515625\n",
      "Epoch 28290 train loss 16579.80078125 val loss 47456.3984375\n",
      "Epoch 28300 train loss 16564.908203125 val loss 47444.33984375\n",
      "Epoch 28310 train loss 16569.94921875 val loss 47420.97265625\n",
      "Epoch 28320 train loss 16564.650390625 val loss 47455.7265625\n",
      "Epoch 28330 train loss 16565.005859375 val loss 47468.19140625\n",
      "Epoch 28340 train loss 16566.6328125 val loss 47483.4609375\n",
      "Epoch 28350 train loss 16575.150390625 val loss 47500.95703125\n",
      "Epoch 28360 train loss 16582.763671875 val loss 47498.578125\n",
      "Epoch 28370 train loss 16562.888671875 val loss 47488.5\n",
      "Epoch 28380 train loss 16563.716796875 val loss 47489.34765625\n",
      "Epoch 28390 train loss 16557.2265625 val loss 47503.99609375\n",
      "Epoch 28400 train loss 16558.638671875 val loss 47497.23828125\n",
      "Epoch 28410 train loss 16553.072265625 val loss 47506.5234375\n",
      "Epoch 28420 train loss 16569.46484375 val loss 47534.48046875\n",
      "Epoch 28430 train loss 16550.837890625 val loss 47532.3046875\n",
      "Epoch 28440 train loss 16570.751953125 val loss 47529.29296875\n",
      "Epoch 28450 train loss 16548.21484375 val loss 47516.04296875\n",
      "Epoch 28460 train loss 16552.0390625 val loss 47542.42578125\n",
      "Epoch 28470 train loss 16547.671875 val loss 47522.98046875\n",
      "Epoch 28480 train loss 16570.087890625 val loss 47561.5546875\n",
      "Epoch 28490 train loss 16549.0859375 val loss 47578.04296875\n",
      "Epoch 28500 train loss 16548.322265625 val loss 47560.54296875\n",
      "Epoch 28510 train loss 16551.677734375 val loss 47571.109375\n",
      "Epoch 28520 train loss 16539.8671875 val loss 47604.90234375\n",
      "Epoch 28530 train loss 16555.96875 val loss 47596.1015625\n",
      "Epoch 28540 train loss 16543.015625 val loss 47583.79296875\n",
      "Epoch 28550 train loss 16541.994140625 val loss 47622.5390625\n",
      "Epoch 28560 train loss 16556.19921875 val loss 47620.16796875\n",
      "Epoch 28570 train loss 16543.7109375 val loss 47632.78515625\n",
      "Epoch 28580 train loss 16539.66796875 val loss 47622.1171875\n",
      "Epoch 28590 train loss 16547.669921875 val loss 47642.58203125\n",
      "Epoch 28600 train loss 16538.7265625 val loss 47641.61328125\n",
      "Epoch 28610 train loss 16531.0390625 val loss 47646.05078125\n",
      "Epoch 28620 train loss 16554.130859375 val loss 47647.1484375\n",
      "Epoch 28630 train loss 16554.623046875 val loss 47675.3671875\n",
      "Epoch 28640 train loss 16544.400390625 val loss 47644.23046875\n",
      "Epoch 28650 train loss 16544.74609375 val loss 47689.43359375\n",
      "Epoch 28660 train loss 16534.107421875 val loss 47663.5\n",
      "Epoch 28670 train loss 16530.4375 val loss 47685.69140625\n",
      "Epoch 28680 train loss 16524.896484375 val loss 47671.63671875\n",
      "Epoch 28690 train loss 16526.615234375 val loss 47656.2890625\n",
      "Epoch 28700 train loss 16535.7109375 val loss 47704.4921875\n",
      "Epoch 28710 train loss 16521.158203125 val loss 47681.88671875\n",
      "Epoch 28720 train loss 16540.1796875 val loss 47701.2421875\n",
      "Epoch 28730 train loss 16522.208984375 val loss 47711.50390625\n",
      "Epoch 28740 train loss 16518.087890625 val loss 47739.5859375\n",
      "Epoch 28750 train loss 16556.7734375 val loss 47740.38671875\n",
      "Epoch 28760 train loss 16527.494140625 val loss 47731.99609375\n",
      "Epoch 28770 train loss 16519.169921875 val loss 47728.30859375\n",
      "Epoch 28780 train loss 16514.509765625 val loss 47698.73046875\n",
      "Epoch 28790 train loss 16513.0 val loss 47725.8671875\n",
      "Epoch 28800 train loss 16513.46484375 val loss 47733.44921875\n",
      "Epoch 28810 train loss 16512.505859375 val loss 47767.828125\n",
      "Epoch 28820 train loss 16513.2578125 val loss 47759.1875\n",
      "Epoch 28830 train loss 16530.734375 val loss 47745.2109375\n",
      "Epoch 28840 train loss 16512.177734375 val loss 47746.28125\n",
      "Epoch 28850 train loss 16512.34765625 val loss 47728.76953125\n",
      "Epoch 28860 train loss 16511.3828125 val loss 47775.13671875\n",
      "Epoch 28870 train loss 16520.392578125 val loss 47796.5703125\n",
      "Epoch 28880 train loss 16507.439453125 val loss 47765.83203125\n",
      "Epoch 28890 train loss 16507.611328125 val loss 47767.03515625\n",
      "Epoch 28900 train loss 16502.345703125 val loss 47810.99609375\n",
      "Epoch 28910 train loss 16521.091796875 val loss 47793.0859375\n",
      "Epoch 28920 train loss 16511.685546875 val loss 47815.62890625\n",
      "Epoch 28930 train loss 16495.66015625 val loss 47826.75390625\n",
      "Epoch 28940 train loss 16496.865234375 val loss 47836.49609375\n",
      "Epoch 28950 train loss 16497.943359375 val loss 47881.0859375\n",
      "Epoch 28960 train loss 16509.609375 val loss 47868.72265625\n",
      "Epoch 28970 train loss 16488.177734375 val loss 47879.74609375\n",
      "Epoch 28980 train loss 16495.970703125 val loss 47873.12890625\n",
      "Epoch 28990 train loss 16507.955078125 val loss 47879.0703125\n",
      "Epoch 29000 train loss 16486.783203125 val loss 47897.546875\n",
      "Epoch 29010 train loss 16493.49609375 val loss 47889.765625\n",
      "Epoch 29020 train loss 16482.39453125 val loss 47876.80078125\n",
      "Epoch 29030 train loss 16492.27734375 val loss 47884.734375\n",
      "Epoch 29040 train loss 16479.0703125 val loss 47901.8828125\n",
      "Epoch 29050 train loss 16480.033203125 val loss 47904.41796875\n",
      "Epoch 29060 train loss 16485.689453125 val loss 47906.4375\n",
      "Epoch 29070 train loss 16478.306640625 val loss 47907.80078125\n",
      "Epoch 29080 train loss 16490.568359375 val loss 47899.5546875\n",
      "Epoch 29090 train loss 16486.439453125 val loss 47920.9296875\n",
      "Epoch 29100 train loss 16471.736328125 val loss 47936.5\n",
      "Epoch 29110 train loss 16471.41796875 val loss 47936.6640625\n",
      "Epoch 29120 train loss 16491.33984375 val loss 47943.31640625\n",
      "Epoch 29130 train loss 16479.310546875 val loss 47943.078125\n",
      "Epoch 29140 train loss 16466.130859375 val loss 47919.47265625\n",
      "Epoch 29150 train loss 16467.814453125 val loss 47933.07421875\n",
      "Epoch 29160 train loss 16476.892578125 val loss 47964.9296875\n",
      "Epoch 29170 train loss 16462.478515625 val loss 47945.60546875\n",
      "Epoch 29180 train loss 16482.416015625 val loss 47958.9609375\n",
      "Epoch 29190 train loss 16465.802734375 val loss 47974.50390625\n",
      "Epoch 29200 train loss 16463.96875 val loss 47977.484375\n",
      "Epoch 29210 train loss 16463.3828125 val loss 47984.4765625\n",
      "Epoch 29220 train loss 16492.07421875 val loss 48032.6484375\n",
      "Epoch 29230 train loss 16463.794921875 val loss 47995.12890625\n",
      "Epoch 29240 train loss 16454.9453125 val loss 48027.0234375\n",
      "Epoch 29250 train loss 16453.90625 val loss 48011.83984375\n",
      "Epoch 29260 train loss 16452.361328125 val loss 48039.9609375\n",
      "Epoch 29270 train loss 16453.80078125 val loss 48052.18359375\n",
      "Epoch 29280 train loss 16450.544921875 val loss 48049.70703125\n",
      "Epoch 29290 train loss 16447.740234375 val loss 48060.8515625\n",
      "Epoch 29300 train loss 16471.298828125 val loss 48084.359375\n",
      "Epoch 29310 train loss 16450.091796875 val loss 48096.6015625\n",
      "Epoch 29320 train loss 16447.517578125 val loss 48066.1640625\n",
      "Epoch 29330 train loss 16453.873046875 val loss 48111.94140625\n",
      "Epoch 29340 train loss 16442.048828125 val loss 48099.94921875\n",
      "Epoch 29350 train loss 16449.013671875 val loss 48114.3828125\n",
      "Epoch 29360 train loss 16456.32421875 val loss 48129.1640625\n",
      "Epoch 29370 train loss 16447.75 val loss 48127.6171875\n",
      "Epoch 29380 train loss 16440.791015625 val loss 48118.50390625\n",
      "Epoch 29390 train loss 16446.134765625 val loss 48156.671875\n",
      "Epoch 29400 train loss 16437.044921875 val loss 48129.0390625\n",
      "Epoch 29410 train loss 16437.31640625 val loss 48176.61328125\n",
      "Epoch 29420 train loss 16447.564453125 val loss 48166.88671875\n",
      "Epoch 29430 train loss 16431.947265625 val loss 48169.828125\n",
      "Epoch 29440 train loss 16435.697265625 val loss 48195.296875\n",
      "Epoch 29450 train loss 16458.08984375 val loss 48186.21875\n",
      "Epoch 29460 train loss 16428.25390625 val loss 48176.80859375\n",
      "Epoch 29470 train loss 16444.42578125 val loss 48223.96875\n",
      "Epoch 29480 train loss 16427.8359375 val loss 48228.29296875\n",
      "Epoch 29490 train loss 16428.173828125 val loss 48222.296875\n",
      "Epoch 29500 train loss 16446.078125 val loss 48220.90234375\n",
      "Epoch 29510 train loss 16426.03515625 val loss 48230.51171875\n",
      "Epoch 29520 train loss 16432.1484375 val loss 48253.5625\n",
      "Epoch 29530 train loss 16441.73828125 val loss 48261.9375\n",
      "Epoch 29540 train loss 16422.326171875 val loss 48270.08984375\n",
      "Epoch 29550 train loss 16436.515625 val loss 48269.734375\n",
      "Epoch 29560 train loss 16418.404296875 val loss 48296.2734375\n",
      "Epoch 29570 train loss 16423.60546875 val loss 48305.62890625\n",
      "Epoch 29580 train loss 16420.046875 val loss 48282.1484375\n",
      "Epoch 29590 train loss 16416.044921875 val loss 48307.8046875\n",
      "Epoch 29600 train loss 16429.109375 val loss 48309.78125\n",
      "Epoch 29610 train loss 16413.19140625 val loss 48272.9609375\n",
      "Epoch 29620 train loss 16418.640625 val loss 48317.82421875\n",
      "Epoch 29630 train loss 16417.3515625 val loss 48347.10546875\n",
      "Epoch 29640 train loss 16408.697265625 val loss 48301.08984375\n",
      "Epoch 29650 train loss 16414.146484375 val loss 48320.234375\n",
      "Epoch 29660 train loss 16411.9765625 val loss 48347.765625\n",
      "Epoch 29670 train loss 16424.060546875 val loss 48347.8203125\n",
      "Epoch 29680 train loss 16404.728515625 val loss 48338.33984375\n",
      "Epoch 29690 train loss 16404.265625 val loss 48332.48828125\n",
      "Epoch 29700 train loss 16421.298828125 val loss 48362.98046875\n",
      "Epoch 29710 train loss 16411.8828125 val loss 48361.84765625\n",
      "Epoch 29720 train loss 16399.603515625 val loss 48343.8984375\n",
      "Epoch 29730 train loss 16405.078125 val loss 48353.93359375\n",
      "Epoch 29740 train loss 16399.11328125 val loss 48368.8671875\n",
      "Epoch 29750 train loss 16395.732421875 val loss 48357.0390625\n",
      "Epoch 29760 train loss 16411.763671875 val loss 48351.3046875\n",
      "Epoch 29770 train loss 16394.703125 val loss 48355.6015625\n",
      "Epoch 29780 train loss 16405.697265625 val loss 48396.76171875\n",
      "Epoch 29790 train loss 16395.76171875 val loss 48401.9609375\n",
      "Epoch 29800 train loss 16397.298828125 val loss 48363.39453125\n",
      "Epoch 29810 train loss 16390.078125 val loss 48418.75390625\n",
      "Epoch 29820 train loss 16400.23046875 val loss 48413.12109375\n",
      "Epoch 29830 train loss 16387.896484375 val loss 48412.87109375\n",
      "Epoch 29840 train loss 16424.142578125 val loss 48405.74609375\n",
      "Epoch 29850 train loss 16392.3671875 val loss 48442.39453125\n",
      "Epoch 29860 train loss 16388.392578125 val loss 48448.60546875\n",
      "Epoch 29870 train loss 16393.453125 val loss 48414.609375\n",
      "Epoch 29880 train loss 16382.400390625 val loss 48423.48046875\n",
      "Epoch 29890 train loss 16393.248046875 val loss 48440.8203125\n",
      "Epoch 29900 train loss 16385.640625 val loss 48459.8125\n",
      "Epoch 29910 train loss 16387.65625 val loss 48469.26171875\n",
      "Epoch 29920 train loss 16381.2861328125 val loss 48449.65234375\n",
      "Epoch 29930 train loss 16386.111328125 val loss 48479.734375\n",
      "Epoch 29940 train loss 16385.400390625 val loss 48497.05078125\n",
      "Epoch 29950 train loss 16375.7626953125 val loss 48458.0859375\n",
      "Epoch 29960 train loss 16401.3828125 val loss 48496.3203125\n",
      "Epoch 29970 train loss 16391.447265625 val loss 48490.87890625\n",
      "Epoch 29980 train loss 16375.203125 val loss 48487.21875\n",
      "Epoch 29990 train loss 16374.9013671875 val loss 48504.3671875\n",
      "Epoch 30000 train loss 16372.345703125 val loss 48495.46875\n",
      "Epoch 30010 train loss 16389.4375 val loss 48531.0546875\n",
      "Epoch 30020 train loss 16370.107421875 val loss 48523.69921875\n",
      "Epoch 30030 train loss 16371.5009765625 val loss 48530.15625\n",
      "Epoch 30040 train loss 16378.966796875 val loss 48545.359375\n",
      "Epoch 30050 train loss 16365.5322265625 val loss 48573.453125\n",
      "Epoch 30060 train loss 16381.109375 val loss 48578.78125\n",
      "Epoch 30070 train loss 16363.310546875 val loss 48552.40234375\n",
      "Epoch 30080 train loss 16368.4580078125 val loss 48573.4921875\n",
      "Epoch 30090 train loss 16367.701171875 val loss 48563.5625\n",
      "Epoch 30100 train loss 16368.5517578125 val loss 48578.40625\n",
      "Epoch 30110 train loss 16366.869140625 val loss 48607.82421875\n",
      "Epoch 30120 train loss 16360.4091796875 val loss 48598.75\n",
      "Epoch 30130 train loss 16361.5419921875 val loss 48613.515625\n",
      "Epoch 30140 train loss 16357.572265625 val loss 48629.52734375\n",
      "Epoch 30150 train loss 16358.251953125 val loss 48651.1171875\n",
      "Epoch 30160 train loss 16363.609375 val loss 48628.2734375\n",
      "Epoch 30170 train loss 16363.2626953125 val loss 48634.6328125\n",
      "Epoch 30180 train loss 16381.4345703125 val loss 48628.73046875\n",
      "Epoch 30190 train loss 16389.41796875 val loss 48685.4296875\n",
      "Epoch 30200 train loss 16377.111328125 val loss 48645.8984375\n",
      "Epoch 30210 train loss 16354.283203125 val loss 48667.70703125\n",
      "Epoch 30220 train loss 16349.0068359375 val loss 48688.28515625\n",
      "Epoch 30230 train loss 16361.3193359375 val loss 48682.8671875\n",
      "Epoch 30240 train loss 16347.9599609375 val loss 48710.65234375\n",
      "Epoch 30250 train loss 16348.71875 val loss 48728.37890625\n",
      "Epoch 30260 train loss 16352.8056640625 val loss 48704.7890625\n",
      "Epoch 30270 train loss 16351.79296875 val loss 48751.64453125\n",
      "Epoch 30280 train loss 16346.455078125 val loss 48738.03125\n",
      "Epoch 30290 train loss 16352.3134765625 val loss 48750.08984375\n",
      "Epoch 30300 train loss 16350.326171875 val loss 48759.56640625\n",
      "Epoch 30310 train loss 16350.783203125 val loss 48771.03515625\n",
      "Epoch 30320 train loss 16339.48828125 val loss 48746.5234375\n",
      "Epoch 30330 train loss 16369.93359375 val loss 48817.48828125\n",
      "Epoch 30340 train loss 16344.8056640625 val loss 48784.265625\n",
      "Epoch 30350 train loss 16338.640625 val loss 48792.2890625\n",
      "Epoch 30360 train loss 16337.0029296875 val loss 48809.75\n",
      "Epoch 30370 train loss 16335.1884765625 val loss 48820.07421875\n",
      "Epoch 30380 train loss 16346.404296875 val loss 48843.6875\n",
      "Epoch 30390 train loss 16337.09765625 val loss 48824.15234375\n",
      "Epoch 30400 train loss 16338.38671875 val loss 48829.55078125\n",
      "Epoch 30410 train loss 16331.51953125 val loss 48863.25\n",
      "Epoch 30420 train loss 16346.73046875 val loss 48876.8671875\n",
      "Epoch 30430 train loss 16351.5087890625 val loss 48891.1953125\n",
      "Epoch 30440 train loss 16329.4287109375 val loss 48869.5234375\n",
      "Epoch 30450 train loss 16327.0673828125 val loss 48876.01953125\n",
      "Epoch 30460 train loss 16345.431640625 val loss 48912.3359375\n",
      "Epoch 30470 train loss 16337.9033203125 val loss 48941.04296875\n",
      "Epoch 30480 train loss 16342.478515625 val loss 48930.515625\n",
      "Epoch 30490 train loss 16342.77734375 val loss 48930.31640625\n",
      "Epoch 30500 train loss 16334.4248046875 val loss 48942.578125\n",
      "Epoch 30510 train loss 16328.833984375 val loss 48925.19921875\n",
      "Epoch 30520 train loss 16322.5673828125 val loss 48944.10546875\n",
      "Epoch 30530 train loss 16321.5244140625 val loss 48947.8125\n",
      "Epoch 30540 train loss 16326.7734375 val loss 48942.41015625\n",
      "Epoch 30550 train loss 16318.9033203125 val loss 48980.45703125\n",
      "Epoch 30560 train loss 16331.5732421875 val loss 48991.83203125\n",
      "Epoch 30570 train loss 16319.0 val loss 48968.88671875\n",
      "Epoch 30580 train loss 16328.103515625 val loss 49008.65625\n",
      "Epoch 30590 train loss 16327.9326171875 val loss 49018.74609375\n",
      "Epoch 30600 train loss 16316.244140625 val loss 49027.765625\n",
      "Epoch 30610 train loss 16324.373046875 val loss 49034.8828125\n",
      "Epoch 30620 train loss 16312.0908203125 val loss 49012.44140625\n",
      "Epoch 30630 train loss 16310.7783203125 val loss 49003.234375\n",
      "Epoch 30640 train loss 16323.599609375 val loss 49005.8828125\n",
      "Epoch 30650 train loss 16315.4384765625 val loss 49028.60546875\n",
      "Epoch 30660 train loss 16335.826171875 val loss 49072.8984375\n",
      "Epoch 30670 train loss 16311.1396484375 val loss 49041.9296875\n",
      "Epoch 30680 train loss 16306.8447265625 val loss 49070.08203125\n",
      "Epoch 30690 train loss 16316.1982421875 val loss 49065.921875\n",
      "Epoch 30700 train loss 16306.3310546875 val loss 49058.2578125\n",
      "Epoch 30710 train loss 16302.5458984375 val loss 49073.70703125\n",
      "Epoch 30720 train loss 16301.0576171875 val loss 49067.17578125\n",
      "Epoch 30730 train loss 16311.666015625 val loss 49076.9375\n",
      "Epoch 30740 train loss 16322.12109375 val loss 49081.58984375\n",
      "Epoch 30750 train loss 16299.0712890625 val loss 49101.9921875\n",
      "Epoch 30760 train loss 16317.583984375 val loss 49098.1953125\n",
      "Epoch 30770 train loss 16296.748046875 val loss 49111.6796875\n",
      "Epoch 30780 train loss 16324.5537109375 val loss 49155.23046875\n",
      "Epoch 30790 train loss 16309.5224609375 val loss 49142.5078125\n",
      "Epoch 30800 train loss 16294.375 val loss 49150.41796875\n",
      "Epoch 30810 train loss 16299.380859375 val loss 49141.078125\n",
      "Epoch 30820 train loss 16305.716796875 val loss 49120.90234375\n",
      "Epoch 30830 train loss 16298.337890625 val loss 49159.19140625\n",
      "Epoch 30840 train loss 16298.169921875 val loss 49154.4296875\n",
      "Epoch 30850 train loss 16294.2294921875 val loss 49168.41796875\n",
      "Epoch 30860 train loss 16290.955078125 val loss 49170.7265625\n",
      "Epoch 30870 train loss 16290.7900390625 val loss 49160.875\n",
      "Epoch 30880 train loss 16288.29296875 val loss 49191.96484375\n",
      "Epoch 30890 train loss 16302.9443359375 val loss 49208.7109375\n",
      "Epoch 30900 train loss 16302.798828125 val loss 49206.6796875\n",
      "Epoch 30910 train loss 16297.2001953125 val loss 49191.2890625\n",
      "Epoch 30920 train loss 16298.5625 val loss 49208.25\n",
      "Epoch 30930 train loss 16282.287109375 val loss 49222.04296875\n",
      "Epoch 30940 train loss 16283.35546875 val loss 49223.72265625\n",
      "Epoch 30950 train loss 16301.2861328125 val loss 49220.3515625\n",
      "Epoch 30960 train loss 16285.1455078125 val loss 49235.19921875\n",
      "Epoch 30970 train loss 16278.501953125 val loss 49243.41796875\n",
      "Epoch 30980 train loss 16307.7646484375 val loss 49260.61328125\n",
      "Epoch 30990 train loss 16283.060546875 val loss 49246.078125\n",
      "Epoch 31000 train loss 16281.296875 val loss 49279.3671875\n",
      "Epoch 31010 train loss 16279.9404296875 val loss 49287.625\n",
      "Epoch 31020 train loss 16277.662109375 val loss 49278.390625\n",
      "Epoch 31030 train loss 16310.5048828125 val loss 49275.5\n",
      "Epoch 31040 train loss 16306.3134765625 val loss 49303.31640625\n",
      "Epoch 31050 train loss 16274.8740234375 val loss 49302.76953125\n",
      "Epoch 31060 train loss 16271.224609375 val loss 49287.01171875\n",
      "Epoch 31070 train loss 16273.603515625 val loss 49334.7578125\n",
      "Epoch 31080 train loss 16275.5556640625 val loss 49326.0859375\n",
      "Epoch 31090 train loss 16278.7177734375 val loss 49337.83203125\n",
      "Epoch 31100 train loss 16266.9140625 val loss 49354.09375\n",
      "Epoch 31110 train loss 16290.876953125 val loss 49344.08203125\n",
      "Epoch 31120 train loss 16264.017578125 val loss 49371.63671875\n",
      "Epoch 31130 train loss 16314.8056640625 val loss 49375.328125\n",
      "Epoch 31140 train loss 16294.4208984375 val loss 49387.80859375\n",
      "Epoch 31150 train loss 16287.1806640625 val loss 49378.73828125\n",
      "Epoch 31160 train loss 16279.0625 val loss 49382.65234375\n",
      "Epoch 31170 train loss 16267.6962890625 val loss 49400.3984375\n",
      "Epoch 31180 train loss 16259.177734375 val loss 49425.5703125\n",
      "Epoch 31190 train loss 16263.7685546875 val loss 49462.6875\n",
      "Epoch 31200 train loss 16257.763671875 val loss 49431.3046875\n",
      "Epoch 31210 train loss 16272.4677734375 val loss 49473.58984375\n",
      "Epoch 31220 train loss 16256.697265625 val loss 49440.9375\n",
      "Epoch 31230 train loss 16264.919921875 val loss 49466.37890625\n",
      "Epoch 31240 train loss 16269.078125 val loss 49507.58203125\n",
      "Epoch 31250 train loss 16252.556640625 val loss 49497.73046875\n",
      "Epoch 31260 train loss 16252.962890625 val loss 49502.4609375\n",
      "Epoch 31270 train loss 16247.376953125 val loss 49495.78125\n",
      "Epoch 31280 train loss 16263.76171875 val loss 49533.046875\n",
      "Epoch 31290 train loss 16245.400390625 val loss 49504.36328125\n",
      "Epoch 31300 train loss 16259.1806640625 val loss 49527.97265625\n",
      "Epoch 31310 train loss 16242.3505859375 val loss 49570.83984375\n",
      "Epoch 31320 train loss 16260.4658203125 val loss 49579.0078125\n",
      "Epoch 31330 train loss 16237.6083984375 val loss 49559.1875\n",
      "Epoch 31340 train loss 16257.0166015625 val loss 49556.39453125\n",
      "Epoch 31350 train loss 16241.376953125 val loss 49563.78515625\n",
      "Epoch 31360 train loss 16239.1181640625 val loss 49599.98828125\n",
      "Epoch 31370 train loss 16237.896484375 val loss 49615.953125\n",
      "Epoch 31380 train loss 16250.7578125 val loss 49619.51171875\n",
      "Epoch 31390 train loss 16240.3974609375 val loss 49642.734375\n",
      "Epoch 31400 train loss 16232.6025390625 val loss 49643.3125\n",
      "Epoch 31410 train loss 16240.5556640625 val loss 49643.09375\n",
      "Epoch 31420 train loss 16245.662109375 val loss 49637.7109375\n",
      "Epoch 31430 train loss 16228.3369140625 val loss 49639.40625\n",
      "Epoch 31440 train loss 16238.642578125 val loss 49668.42578125\n",
      "Epoch 31450 train loss 16227.1044921875 val loss 49676.7578125\n",
      "Epoch 31460 train loss 16227.8173828125 val loss 49671.98828125\n",
      "Epoch 31470 train loss 16227.0 val loss 49674.828125\n",
      "Epoch 31480 train loss 16230.4970703125 val loss 49715.16015625\n",
      "Epoch 31490 train loss 16223.4892578125 val loss 49705.52734375\n",
      "Epoch 31500 train loss 16240.5048828125 val loss 49732.91015625\n",
      "Epoch 31510 train loss 16222.8603515625 val loss 49722.58203125\n",
      "Epoch 31520 train loss 16230.2353515625 val loss 49720.85546875\n",
      "Epoch 31530 train loss 16243.767578125 val loss 49737.30078125\n",
      "Epoch 31540 train loss 16220.845703125 val loss 49740.05859375\n",
      "Epoch 31550 train loss 16223.4169921875 val loss 49763.59375\n",
      "Epoch 31560 train loss 16223.2646484375 val loss 49748.640625\n",
      "Epoch 31570 train loss 16217.6630859375 val loss 49768.9296875\n",
      "Epoch 31580 train loss 16220.1015625 val loss 49786.92578125\n",
      "Epoch 31590 train loss 16213.5546875 val loss 49796.37109375\n",
      "Epoch 31600 train loss 16215.759765625 val loss 49816.765625\n",
      "Epoch 31610 train loss 16236.3154296875 val loss 49835.40625\n",
      "Epoch 31620 train loss 16212.44921875 val loss 49833.32421875\n",
      "Epoch 31630 train loss 16217.9404296875 val loss 49856.1953125\n",
      "Epoch 31640 train loss 16205.587890625 val loss 49840.7109375\n",
      "Epoch 31650 train loss 16213.0947265625 val loss 49846.0078125\n",
      "Epoch 31660 train loss 16205.2314453125 val loss 49852.1640625\n",
      "Epoch 31670 train loss 16220.177734375 val loss 49853.62109375\n",
      "Epoch 31680 train loss 16198.1044921875 val loss 49856.55078125\n",
      "Epoch 31690 train loss 16230.30859375 val loss 49884.171875\n",
      "Epoch 31700 train loss 16225.9111328125 val loss 49896.046875\n",
      "Epoch 31710 train loss 16199.03515625 val loss 49880.3671875\n",
      "Epoch 31720 train loss 16193.2685546875 val loss 49886.61328125\n",
      "Epoch 31730 train loss 16197.2880859375 val loss 49922.1953125\n",
      "Epoch 31740 train loss 16196.47265625 val loss 49906.640625\n",
      "Epoch 31750 train loss 16201.623046875 val loss 49931.23828125\n",
      "Epoch 31760 train loss 16201.9375 val loss 49945.53515625\n",
      "Epoch 31770 train loss 16187.798828125 val loss 49955.109375\n",
      "Epoch 31780 train loss 16192.1171875 val loss 49958.83203125\n",
      "Epoch 31790 train loss 16186.2587890625 val loss 49972.078125\n",
      "Epoch 31800 train loss 16215.9775390625 val loss 49980.30078125\n",
      "Epoch 31810 train loss 16183.810546875 val loss 49987.38671875\n",
      "Epoch 31820 train loss 16187.9091796875 val loss 49980.21875\n",
      "Epoch 31830 train loss 16182.14453125 val loss 50006.03515625\n",
      "Epoch 31840 train loss 16211.0419921875 val loss 49990.70703125\n",
      "Epoch 31850 train loss 16181.4599609375 val loss 50004.8984375\n",
      "Epoch 31860 train loss 16180.421875 val loss 50021.30859375\n",
      "Epoch 31870 train loss 16184.154296875 val loss 50025.55078125\n",
      "Epoch 31880 train loss 16192.2041015625 val loss 49990.1171875\n",
      "Epoch 31890 train loss 16179.90625 val loss 50015.54296875\n",
      "Epoch 31900 train loss 16175.474609375 val loss 50077.546875\n",
      "Epoch 31910 train loss 16176.6474609375 val loss 50036.06640625\n",
      "Epoch 31920 train loss 16172.4990234375 val loss 50046.7734375\n",
      "Epoch 31930 train loss 16206.3759765625 val loss 50066.40234375\n",
      "Epoch 31940 train loss 16171.94921875 val loss 50047.03515625\n",
      "Epoch 31950 train loss 16181.8056640625 val loss 50056.44921875\n",
      "Epoch 31960 train loss 16187.41796875 val loss 50070.7890625\n",
      "Epoch 31970 train loss 16167.708984375 val loss 50083.90234375\n",
      "Epoch 31980 train loss 16183.1064453125 val loss 50090.42578125\n",
      "Epoch 31990 train loss 16170.11328125 val loss 50085.875\n",
      "Epoch 32000 train loss 16191.9091796875 val loss 50097.1484375\n",
      "Epoch 32010 train loss 16165.1796875 val loss 50086.3515625\n",
      "Epoch 32020 train loss 16162.6015625 val loss 50084.55859375\n",
      "Epoch 32030 train loss 16165.0234375 val loss 50118.16796875\n",
      "Epoch 32040 train loss 16212.9267578125 val loss 50127.5703125\n",
      "Epoch 32050 train loss 16187.638671875 val loss 50141.109375\n",
      "Epoch 32060 train loss 16163.8154296875 val loss 50141.89453125\n",
      "Epoch 32070 train loss 16159.3994140625 val loss 50121.5859375\n",
      "Epoch 32080 train loss 16159.4365234375 val loss 50140.75\n",
      "Epoch 32090 train loss 16166.7373046875 val loss 50133.48828125\n",
      "Epoch 32100 train loss 16160.6298828125 val loss 50143.796875\n",
      "Epoch 32110 train loss 16155.974609375 val loss 50139.203125\n",
      "Epoch 32120 train loss 16176.0107421875 val loss 50177.6953125\n",
      "Epoch 32130 train loss 16154.4130859375 val loss 50178.4609375\n",
      "Epoch 32140 train loss 16177.134765625 val loss 50211.78515625\n",
      "Epoch 32150 train loss 16167.6142578125 val loss 50190.62109375\n",
      "Epoch 32160 train loss 16150.82421875 val loss 50215.84375\n",
      "Epoch 32170 train loss 16186.7353515625 val loss 50207.98828125\n",
      "Epoch 32180 train loss 16164.6357421875 val loss 50210.0703125\n",
      "Epoch 32190 train loss 16149.2236328125 val loss 50240.01953125\n",
      "Epoch 32200 train loss 16159.96484375 val loss 50232.53515625\n",
      "Epoch 32210 train loss 16148.0283203125 val loss 50230.92578125\n",
      "Epoch 32220 train loss 16163.07421875 val loss 50228.11328125\n",
      "Epoch 32230 train loss 16153.0009765625 val loss 50237.53515625\n",
      "Epoch 32240 train loss 16144.84375 val loss 50237.18359375\n",
      "Epoch 32250 train loss 16143.4580078125 val loss 50248.58203125\n",
      "Epoch 32260 train loss 16160.953125 val loss 50272.36328125\n",
      "Epoch 32270 train loss 16142.87109375 val loss 50281.125\n",
      "Epoch 32280 train loss 16173.8056640625 val loss 50300.3984375\n",
      "Epoch 32290 train loss 16173.119140625 val loss 50300.1328125\n",
      "Epoch 32300 train loss 16169.123046875 val loss 50298.16015625\n",
      "Epoch 32310 train loss 16153.65625 val loss 50295.78515625\n",
      "Epoch 32320 train loss 16145.6787109375 val loss 50316.171875\n",
      "Epoch 32330 train loss 16137.783203125 val loss 50304.23828125\n",
      "Epoch 32340 train loss 16139.9521484375 val loss 50360.078125\n",
      "Epoch 32350 train loss 16139.626953125 val loss 50322.7109375\n",
      "Epoch 32360 train loss 16137.173828125 val loss 50351.28515625\n",
      "Epoch 32370 train loss 16143.427734375 val loss 50348.51953125\n",
      "Epoch 32380 train loss 16141.7841796875 val loss 50321.5625\n",
      "Epoch 32390 train loss 16149.41015625 val loss 50362.67578125\n",
      "Epoch 32400 train loss 16129.78515625 val loss 50379.2890625\n",
      "Epoch 32410 train loss 16154.7666015625 val loss 50387.2890625\n",
      "Epoch 32420 train loss 16125.3125 val loss 50368.29296875\n",
      "Epoch 32430 train loss 16148.169921875 val loss 50362.37109375\n",
      "Epoch 32440 train loss 16132.1220703125 val loss 50373.625\n",
      "Epoch 32450 train loss 16125.81640625 val loss 50383.40234375\n",
      "Epoch 32460 train loss 16124.7265625 val loss 50389.140625\n",
      "Epoch 32470 train loss 16133.82421875 val loss 50417.98828125\n",
      "Epoch 32480 train loss 16126.912109375 val loss 50415.21484375\n",
      "Epoch 32490 train loss 16120.4765625 val loss 50423.9140625\n",
      "Epoch 32500 train loss 16141.0029296875 val loss 50433.9453125\n",
      "Epoch 32510 train loss 16129.2119140625 val loss 50443.42578125\n",
      "Epoch 32520 train loss 16117.3125 val loss 50433.9453125\n",
      "Epoch 32530 train loss 16137.89453125 val loss 50445.5625\n",
      "Epoch 32540 train loss 16114.404296875 val loss 50463.046875\n",
      "Epoch 32550 train loss 16115.8837890625 val loss 50474.4453125\n",
      "Epoch 32560 train loss 16125.697265625 val loss 50488.48046875\n",
      "Epoch 32570 train loss 16128.8671875 val loss 50495.12109375\n",
      "Epoch 32580 train loss 16111.171875 val loss 50485.828125\n",
      "Epoch 32590 train loss 16111.6171875 val loss 50515.84765625\n",
      "Epoch 32600 train loss 16144.7490234375 val loss 50543.703125\n",
      "Epoch 32610 train loss 16142.8203125 val loss 50475.8046875\n",
      "Epoch 32620 train loss 16119.076171875 val loss 50496.86328125\n",
      "Epoch 32630 train loss 16106.43359375 val loss 50462.09765625\n",
      "Epoch 32640 train loss 16108.396484375 val loss 50495.1640625\n",
      "Epoch 32650 train loss 16101.3525390625 val loss 50470.10546875\n",
      "Epoch 32660 train loss 16103.912109375 val loss 50491.890625\n",
      "Epoch 32670 train loss 16101.744140625 val loss 50498.4140625\n",
      "Epoch 32680 train loss 16122.8330078125 val loss 50515.265625\n",
      "Epoch 32690 train loss 16116.6982421875 val loss 50489.52734375\n",
      "Epoch 32700 train loss 16094.7666015625 val loss 50494.55078125\n",
      "Epoch 32710 train loss 16122.345703125 val loss 50518.30078125\n",
      "Epoch 32720 train loss 16099.2998046875 val loss 50516.2890625\n",
      "Epoch 32730 train loss 16101.78125 val loss 50518.046875\n",
      "Epoch 32740 train loss 16092.4970703125 val loss 50487.0703125\n",
      "Epoch 32750 train loss 16094.0107421875 val loss 50493.28125\n",
      "Epoch 32760 train loss 16094.48046875 val loss 50508.41796875\n",
      "Epoch 32770 train loss 16121.2744140625 val loss 50547.87109375\n",
      "Epoch 32780 train loss 16091.4541015625 val loss 50493.921875\n",
      "Epoch 32790 train loss 16107.7607421875 val loss 50501.28515625\n",
      "Epoch 32800 train loss 16089.4765625 val loss 50522.21484375\n",
      "Epoch 32810 train loss 16090.0712890625 val loss 50533.7578125\n",
      "Epoch 32820 train loss 16089.0712890625 val loss 50553.0234375\n",
      "Epoch 32830 train loss 16114.9296875 val loss 50564.5625\n",
      "Epoch 32840 train loss 16086.759765625 val loss 50493.0546875\n",
      "Epoch 32850 train loss 16086.546875 val loss 50522.86328125\n",
      "Epoch 32860 train loss 16095.150390625 val loss 50515.68359375\n",
      "Epoch 32870 train loss 16100.2783203125 val loss 50576.87890625\n",
      "Epoch 32880 train loss 16080.216796875 val loss 50553.44140625\n",
      "Epoch 32890 train loss 16087.7705078125 val loss 50526.41796875\n",
      "Epoch 32900 train loss 16076.23046875 val loss 50543.5859375\n",
      "Epoch 32910 train loss 16107.5234375 val loss 50547.375\n",
      "Epoch 32920 train loss 16075.53515625 val loss 50566.62109375\n",
      "Epoch 32930 train loss 16080.6845703125 val loss 50528.85546875\n",
      "Epoch 32940 train loss 16100.1259765625 val loss 50554.2109375\n",
      "Epoch 32950 train loss 16074.4384765625 val loss 50575.18359375\n",
      "Epoch 32960 train loss 16077.791015625 val loss 50586.4609375\n",
      "Epoch 32970 train loss 16098.9580078125 val loss 50583.58984375\n",
      "Epoch 32980 train loss 16068.1865234375 val loss 50566.19140625\n",
      "Epoch 32990 train loss 16109.720703125 val loss 50578.36328125\n",
      "Epoch 33000 train loss 16067.3447265625 val loss 50576.7890625\n",
      "Epoch 33010 train loss 16068.05859375 val loss 50561.18359375\n",
      "Epoch 33020 train loss 16091.1611328125 val loss 50604.2890625\n",
      "Epoch 33030 train loss 16064.71875 val loss 50599.578125\n",
      "Epoch 33040 train loss 16084.669921875 val loss 50618.6640625\n",
      "Epoch 33050 train loss 16062.900390625 val loss 50601.02734375\n",
      "Epoch 33060 train loss 16076.2099609375 val loss 50615.484375\n",
      "Epoch 33070 train loss 16071.9892578125 val loss 50591.75\n",
      "Epoch 33080 train loss 16065.1728515625 val loss 50573.75\n",
      "Epoch 33090 train loss 16068.4599609375 val loss 50623.09375\n",
      "Epoch 33100 train loss 16067.869140625 val loss 50655.1796875\n",
      "Epoch 33110 train loss 16055.9072265625 val loss 50628.8828125\n",
      "Epoch 33120 train loss 16080.357421875 val loss 50603.7734375\n",
      "Epoch 33130 train loss 16054.3662109375 val loss 50624.90234375\n",
      "Epoch 33140 train loss 16071.734375 val loss 50641.95703125\n",
      "Epoch 33150 train loss 16054.4951171875 val loss 50612.5390625\n",
      "Epoch 33160 train loss 16061.869140625 val loss 50611.3515625\n",
      "Epoch 33170 train loss 16054.22265625 val loss 50619.60546875\n",
      "Epoch 33180 train loss 16053.23046875 val loss 50641.01953125\n",
      "Epoch 33190 train loss 16070.6220703125 val loss 50634.83203125\n",
      "Epoch 33200 train loss 16050.3359375 val loss 50649.234375\n",
      "Epoch 33210 train loss 16065.5419921875 val loss 50703.1015625\n",
      "Epoch 33220 train loss 16065.2607421875 val loss 50668.765625\n",
      "Epoch 33230 train loss 16060.3359375 val loss 50650.82421875\n",
      "Epoch 33240 train loss 16044.1826171875 val loss 50655.9609375\n",
      "Epoch 33250 train loss 16072.0 val loss 50650.4921875\n",
      "Epoch 33260 train loss 16041.6806640625 val loss 50654.70703125\n",
      "Epoch 33270 train loss 16060.1337890625 val loss 50680.171875\n",
      "Epoch 33280 train loss 16050.080078125 val loss 50647.0703125\n",
      "Epoch 33290 train loss 16041.9853515625 val loss 50669.48828125\n",
      "Epoch 33300 train loss 16045.50390625 val loss 50704.72265625\n",
      "Epoch 33310 train loss 16061.330078125 val loss 50690.4765625\n",
      "Epoch 33320 train loss 16035.5126953125 val loss 50683.86328125\n",
      "Epoch 33330 train loss 16060.63671875 val loss 50719.8515625\n",
      "Epoch 33340 train loss 16040.673828125 val loss 50716.328125\n",
      "Epoch 33350 train loss 16047.3232421875 val loss 50705.83203125\n",
      "Epoch 33360 train loss 16034.9150390625 val loss 50684.8515625\n",
      "Epoch 33370 train loss 16062.06640625 val loss 50713.70703125\n",
      "Epoch 33380 train loss 16032.7802734375 val loss 50735.765625\n",
      "Epoch 33390 train loss 16042.6533203125 val loss 50716.12890625\n",
      "Epoch 33400 train loss 16040.181640625 val loss 50724.48046875\n",
      "Epoch 33410 train loss 16032.6708984375 val loss 50703.9296875\n",
      "Epoch 33420 train loss 16038.4248046875 val loss 50735.66796875\n",
      "Epoch 33430 train loss 16039.2392578125 val loss 50718.546875\n",
      "Epoch 33440 train loss 16034.8515625 val loss 50721.421875\n",
      "Epoch 33450 train loss 16036.974609375 val loss 50767.53125\n",
      "Epoch 33460 train loss 16023.8740234375 val loss 50767.41796875\n",
      "Epoch 33470 train loss 16057.212890625 val loss 50737.10546875\n",
      "Epoch 33480 train loss 16022.6083984375 val loss 50737.12109375\n",
      "Epoch 33490 train loss 16031.4677734375 val loss 50755.25390625\n",
      "Epoch 33500 train loss 16034.4189453125 val loss 50801.75390625\n",
      "Epoch 33510 train loss 16029.755859375 val loss 50762.08984375\n",
      "Epoch 33520 train loss 16026.2705078125 val loss 50799.03515625\n",
      "Epoch 33530 train loss 16020.9755859375 val loss 50802.08203125\n",
      "Epoch 33540 train loss 16060.91796875 val loss 50807.1796875\n",
      "Epoch 33550 train loss 16027.7607421875 val loss 50795.5546875\n",
      "Epoch 33560 train loss 16015.9755859375 val loss 50786.05078125\n",
      "Epoch 33570 train loss 16036.2001953125 val loss 50789.00390625\n",
      "Epoch 33580 train loss 16013.5927734375 val loss 50770.984375\n",
      "Epoch 33590 train loss 16029.767578125 val loss 50767.1171875\n",
      "Epoch 33600 train loss 16014.8232421875 val loss 50804.56640625\n",
      "Epoch 33610 train loss 16014.5185546875 val loss 50831.6171875\n",
      "Epoch 33620 train loss 16024.5458984375 val loss 50861.40625\n",
      "Epoch 33630 train loss 16012.65234375 val loss 50826.2890625\n",
      "Epoch 33640 train loss 16031.345703125 val loss 50824.2890625\n",
      "Epoch 33650 train loss 16012.814453125 val loss 50846.95703125\n",
      "Epoch 33660 train loss 16025.12109375 val loss 50845.21484375\n",
      "Epoch 33670 train loss 16007.404296875 val loss 50849.12109375\n",
      "Epoch 33680 train loss 16033.966796875 val loss 50834.09375\n",
      "Epoch 33690 train loss 16003.509765625 val loss 50833.1953125\n",
      "Epoch 33700 train loss 16032.384765625 val loss 50870.92578125\n",
      "Epoch 33710 train loss 16005.814453125 val loss 50854.765625\n",
      "Epoch 33720 train loss 16007.50390625 val loss 50860.73828125\n",
      "Epoch 33730 train loss 16001.173828125 val loss 50894.61328125\n",
      "Epoch 33740 train loss 16010.494140625 val loss 50874.37890625\n",
      "Epoch 33750 train loss 16009.814453125 val loss 50862.84375\n",
      "Epoch 33760 train loss 16000.4921875 val loss 50920.47265625\n",
      "Epoch 33770 train loss 15993.759765625 val loss 50907.89453125\n",
      "Epoch 33780 train loss 15989.421875 val loss 50954.6328125\n",
      "Epoch 33790 train loss 16042.16796875 val loss 50971.03125\n",
      "Epoch 33800 train loss 15982.76953125 val loss 51015.8671875\n",
      "Epoch 33810 train loss 15985.3818359375 val loss 51038.69921875\n",
      "Epoch 33820 train loss 15990.8291015625 val loss 51060.80078125\n",
      "Epoch 33830 train loss 15975.6787109375 val loss 51052.14453125\n",
      "Epoch 33840 train loss 15995.5849609375 val loss 51107.6953125\n",
      "Epoch 33850 train loss 16001.291015625 val loss 51095.45703125\n",
      "Epoch 33860 train loss 15975.1845703125 val loss 51111.7578125\n",
      "Epoch 33870 train loss 15983.341796875 val loss 51114.15625\n",
      "Epoch 33880 train loss 15975.484375 val loss 51114.578125\n",
      "Epoch 33890 train loss 15971.4267578125 val loss 51116.5\n",
      "Epoch 33900 train loss 15964.373046875 val loss 51117.359375\n",
      "Epoch 33910 train loss 15969.828125 val loss 51131.140625\n",
      "Epoch 33920 train loss 15976.1748046875 val loss 51140.80078125\n",
      "Epoch 33930 train loss 15961.4345703125 val loss 51161.30859375\n",
      "Epoch 33940 train loss 15993.5146484375 val loss 51155.703125\n",
      "Epoch 33950 train loss 15958.2958984375 val loss 51184.0234375\n",
      "Epoch 33960 train loss 15966.177734375 val loss 51202.5859375\n",
      "Epoch 33970 train loss 15965.4765625 val loss 51211.69921875\n",
      "Epoch 33980 train loss 15968.9052734375 val loss 51235.04296875\n",
      "Epoch 33990 train loss 15964.5224609375 val loss 51203.3671875\n",
      "Epoch 34000 train loss 15974.00390625 val loss 51203.48828125\n",
      "Epoch 34010 train loss 15956.677734375 val loss 51245.37890625\n",
      "Epoch 34020 train loss 15956.369140625 val loss 51207.17578125\n",
      "Epoch 34030 train loss 15947.9892578125 val loss 51218.05859375\n",
      "Epoch 34040 train loss 15978.1826171875 val loss 51255.5\n",
      "Epoch 34050 train loss 15946.7822265625 val loss 51258.12890625\n",
      "Epoch 34060 train loss 15957.353515625 val loss 51231.43359375\n",
      "Epoch 34070 train loss 15957.6728515625 val loss 51269.296875\n",
      "Epoch 34080 train loss 15943.9765625 val loss 51291.22265625\n",
      "Epoch 34090 train loss 15945.0048828125 val loss 51295.7578125\n",
      "Epoch 34100 train loss 15956.052734375 val loss 51314.14453125\n",
      "Epoch 34110 train loss 15965.755859375 val loss 51244.953125\n",
      "Epoch 34120 train loss 15940.8056640625 val loss 51273.05859375\n",
      "Epoch 34130 train loss 15962.1787109375 val loss 51282.44921875\n",
      "Epoch 34140 train loss 15958.2724609375 val loss 51296.5546875\n",
      "Epoch 34150 train loss 15937.8623046875 val loss 51295.3984375\n",
      "Epoch 34160 train loss 15938.07421875 val loss 51331.55078125\n",
      "Epoch 34170 train loss 15959.921875 val loss 51322.33984375\n",
      "Epoch 34180 train loss 15937.990234375 val loss 51329.10546875\n",
      "Epoch 34190 train loss 15958.7626953125 val loss 51302.18359375\n",
      "Epoch 34200 train loss 15934.51953125 val loss 51345.01171875\n",
      "Epoch 34210 train loss 15939.5400390625 val loss 51372.37890625\n",
      "Epoch 34220 train loss 15933.8994140625 val loss 51371.6484375\n",
      "Epoch 34230 train loss 15963.99609375 val loss 51379.6875\n",
      "Epoch 34240 train loss 15932.2939453125 val loss 51406.23828125\n",
      "Epoch 34250 train loss 15945.0400390625 val loss 51382.73046875\n",
      "Epoch 34260 train loss 15928.3486328125 val loss 51393.8125\n",
      "Epoch 34270 train loss 15938.12109375 val loss 51390.79296875\n",
      "Epoch 34280 train loss 15924.6640625 val loss 51401.375\n",
      "Epoch 34290 train loss 15957.166015625 val loss 51401.6796875\n",
      "Epoch 34300 train loss 15923.7373046875 val loss 51404.7890625\n",
      "Epoch 34310 train loss 15926.4521484375 val loss 51395.7265625\n",
      "Epoch 34320 train loss 15936.05078125 val loss 51410.49609375\n",
      "Epoch 34330 train loss 15921.353515625 val loss 51387.9765625\n",
      "Epoch 34340 train loss 15915.9052734375 val loss 51393.8515625\n",
      "Epoch 34350 train loss 15947.54296875 val loss 51402.25390625\n",
      "Epoch 34360 train loss 15915.3271484375 val loss 51398.03125\n",
      "Epoch 34370 train loss 15937.4482421875 val loss 51395.34375\n",
      "Epoch 34380 train loss 15911.1953125 val loss 51422.140625\n",
      "Epoch 34390 train loss 15916.333984375 val loss 51397.234375\n",
      "Epoch 34400 train loss 15919.4921875 val loss 51390.671875\n",
      "Epoch 34410 train loss 15908.0615234375 val loss 51422.58203125\n",
      "Epoch 34420 train loss 15927.654296875 val loss 51412.2734375\n",
      "Epoch 34430 train loss 15928.8271484375 val loss 51405.421875\n",
      "Epoch 34440 train loss 15909.146484375 val loss 51472.7578125\n",
      "Epoch 34450 train loss 15906.208984375 val loss 51454.63671875\n",
      "Epoch 34460 train loss 15903.3076171875 val loss 51409.8203125\n",
      "Epoch 34470 train loss 15913.384765625 val loss 51478.00390625\n",
      "Epoch 34480 train loss 15905.697265625 val loss 51469.1796875\n",
      "Epoch 34490 train loss 15914.962890625 val loss 51434.48828125\n",
      "Epoch 34500 train loss 15912.3271484375 val loss 51452.4609375\n",
      "Epoch 34510 train loss 15904.634765625 val loss 51487.49609375\n",
      "Epoch 34520 train loss 15910.1337890625 val loss 51475.59375\n",
      "Epoch 34530 train loss 15897.2197265625 val loss 51517.33984375\n",
      "Epoch 34540 train loss 15889.0517578125 val loss 51518.296875\n",
      "Epoch 34550 train loss 15890.513671875 val loss 51501.1484375\n",
      "Epoch 34560 train loss 15927.9306640625 val loss 51476.62109375\n",
      "Epoch 34570 train loss 15891.4130859375 val loss 51481.9609375\n",
      "Epoch 34580 train loss 15899.61328125 val loss 51526.52734375\n",
      "Epoch 34590 train loss 15893.234375 val loss 51497.94921875\n",
      "Epoch 34600 train loss 15902.9111328125 val loss 51485.796875\n",
      "Epoch 34610 train loss 15878.0361328125 val loss 51487.90625\n",
      "Epoch 34620 train loss 15899.4931640625 val loss 51493.43359375\n",
      "Epoch 34630 train loss 15876.5048828125 val loss 51478.4296875\n",
      "Epoch 34640 train loss 15885.3759765625 val loss 51473.87109375\n",
      "Epoch 34650 train loss 15892.9287109375 val loss 51496.515625\n",
      "Epoch 34660 train loss 15873.9951171875 val loss 51508.07421875\n",
      "Epoch 34670 train loss 15868.9189453125 val loss 51483.17578125\n",
      "Epoch 34680 train loss 15909.078125 val loss 51515.3671875\n",
      "Epoch 34690 train loss 15893.5341796875 val loss 51532.703125\n",
      "Epoch 34700 train loss 15865.0732421875 val loss 51526.25390625\n",
      "Epoch 34710 train loss 15864.1220703125 val loss 51504.04296875\n",
      "Epoch 34720 train loss 15877.7041015625 val loss 51553.28125\n",
      "Epoch 34730 train loss 15862.61328125 val loss 51549.68359375\n",
      "Epoch 34740 train loss 15867.5986328125 val loss 51554.1171875\n",
      "Epoch 34750 train loss 15869.3828125 val loss 51581.8203125\n",
      "Epoch 34760 train loss 15882.33984375 val loss 51599.0234375\n",
      "Epoch 34770 train loss 15857.453125 val loss 51575.05859375\n",
      "Epoch 34780 train loss 15862.5908203125 val loss 51595.15625\n",
      "Epoch 34790 train loss 15857.9560546875 val loss 51631.34375\n",
      "Epoch 34800 train loss 15867.611328125 val loss 51649.4140625\n",
      "Epoch 34810 train loss 15858.462890625 val loss 51620.98046875\n",
      "Epoch 34820 train loss 15855.416015625 val loss 51615.3203125\n",
      "Epoch 34830 train loss 15865.142578125 val loss 51702.46484375\n",
      "Epoch 34840 train loss 15857.3642578125 val loss 51686.20703125\n",
      "Epoch 34850 train loss 15870.77734375 val loss 51710.39453125\n",
      "Epoch 34860 train loss 15853.673828125 val loss 51690.828125\n",
      "Epoch 34870 train loss 15871.1767578125 val loss 51712.015625\n",
      "Epoch 34880 train loss 15849.4296875 val loss 51733.44921875\n",
      "Epoch 34890 train loss 15852.736328125 val loss 51745.04296875\n",
      "Epoch 34900 train loss 15844.728515625 val loss 51726.875\n",
      "Epoch 34910 train loss 15844.970703125 val loss 51744.1484375\n",
      "Epoch 34920 train loss 15843.4189453125 val loss 51756.28125\n",
      "Epoch 34930 train loss 15889.080078125 val loss 51765.8125\n",
      "Epoch 34940 train loss 15845.65625 val loss 51731.484375\n",
      "Epoch 34950 train loss 15841.767578125 val loss 51764.078125\n",
      "Epoch 34960 train loss 15855.5439453125 val loss 51778.15625\n",
      "Epoch 34970 train loss 15839.6005859375 val loss 51807.5078125\n",
      "Epoch 34980 train loss 15857.0712890625 val loss 51806.10546875\n",
      "Epoch 34990 train loss 15834.8896484375 val loss 51829.3359375\n",
      "Epoch 35000 train loss 15848.4619140625 val loss 51828.98046875\n",
      "Epoch 35010 train loss 15834.900390625 val loss 51835.7265625\n",
      "Epoch 35020 train loss 15867.66796875 val loss 51861.9765625\n",
      "Epoch 35030 train loss 15838.8447265625 val loss 51846.453125\n",
      "Epoch 35040 train loss 15835.44140625 val loss 51898.03125\n",
      "Epoch 35050 train loss 15833.47265625 val loss 51884.35546875\n",
      "Epoch 35060 train loss 15864.9072265625 val loss 51880.375\n",
      "Epoch 35070 train loss 15847.640625 val loss 51916.21875\n",
      "Epoch 35080 train loss 15827.54296875 val loss 51922.3359375\n",
      "Epoch 35090 train loss 15850.7216796875 val loss 51918.6875\n",
      "Epoch 35100 train loss 15825.4970703125 val loss 51933.90625\n",
      "Epoch 35110 train loss 15849.19140625 val loss 51935.49609375\n",
      "Epoch 35120 train loss 15825.0546875 val loss 51949.15234375\n",
      "Epoch 35130 train loss 15830.2548828125 val loss 51968.6875\n",
      "Epoch 35140 train loss 15828.3310546875 val loss 51986.05859375\n",
      "Epoch 35150 train loss 15853.3056640625 val loss 51984.703125\n",
      "Epoch 35160 train loss 15822.041015625 val loss 51964.58203125\n",
      "Epoch 35170 train loss 15841.5595703125 val loss 51967.45703125\n",
      "Epoch 35180 train loss 15818.8798828125 val loss 51982.921875\n",
      "Epoch 35190 train loss 15839.376953125 val loss 52020.11328125\n",
      "Epoch 35200 train loss 15818.255859375 val loss 51994.9921875\n",
      "Epoch 35210 train loss 15832.3583984375 val loss 52012.0078125\n",
      "Epoch 35220 train loss 15817.7255859375 val loss 52039.9609375\n",
      "Epoch 35230 train loss 15822.7998046875 val loss 52045.86328125\n",
      "Epoch 35240 train loss 15830.2314453125 val loss 52077.72265625\n",
      "Epoch 35250 train loss 15814.060546875 val loss 52069.5078125\n",
      "Epoch 35260 train loss 15831.3603515625 val loss 52062.5625\n",
      "Epoch 35270 train loss 15831.90234375 val loss 52040.31640625\n",
      "Epoch 35280 train loss 15812.8271484375 val loss 52087.078125\n",
      "Epoch 35290 train loss 15831.9970703125 val loss 52125.1875\n",
      "Epoch 35300 train loss 15810.66796875 val loss 52132.609375\n",
      "Epoch 35310 train loss 15822.4296875 val loss 52151.14453125\n",
      "Epoch 35320 train loss 15808.8759765625 val loss 52172.7265625\n",
      "Epoch 35330 train loss 15819.4287109375 val loss 52186.203125\n",
      "Epoch 35340 train loss 15813.23828125 val loss 52165.45703125\n",
      "Epoch 35350 train loss 15823.34765625 val loss 52178.88671875\n",
      "Epoch 35360 train loss 15804.109375 val loss 52169.84765625\n",
      "Epoch 35370 train loss 15821.7392578125 val loss 52192.609375\n",
      "Epoch 35380 train loss 15800.2265625 val loss 52182.234375\n",
      "Epoch 35390 train loss 15813.8232421875 val loss 52207.3828125\n",
      "Epoch 35400 train loss 15804.7744140625 val loss 52191.5234375\n",
      "Epoch 35410 train loss 15803.6015625 val loss 52199.46484375\n",
      "Epoch 35420 train loss 15810.5810546875 val loss 52201.71484375\n",
      "Epoch 35430 train loss 15799.12890625 val loss 52224.04296875\n",
      "Epoch 35440 train loss 15807.234375 val loss 52231.171875\n",
      "Epoch 35450 train loss 15794.49609375 val loss 52227.96875\n",
      "Epoch 35460 train loss 15833.4580078125 val loss 52250.0234375\n",
      "Epoch 35470 train loss 15794.0224609375 val loss 52237.90234375\n",
      "Epoch 35480 train loss 15792.9111328125 val loss 52268.8125\n",
      "Epoch 35490 train loss 15800.3857421875 val loss 52261.9140625\n",
      "Epoch 35500 train loss 15791.6435546875 val loss 52272.4375\n",
      "Epoch 35510 train loss 15804.103515625 val loss 52275.421875\n",
      "Epoch 35520 train loss 15787.7568359375 val loss 52286.9140625\n",
      "Epoch 35530 train loss 15795.08984375 val loss 52296.46484375\n",
      "Epoch 35540 train loss 15785.697265625 val loss 52303.22265625\n",
      "Epoch 35550 train loss 15803.412109375 val loss 52340.421875\n",
      "Epoch 35560 train loss 15786.4267578125 val loss 52318.26171875\n",
      "Epoch 35570 train loss 15789.3955078125 val loss 52338.40234375\n",
      "Epoch 35580 train loss 15792.51171875 val loss 52354.52734375\n",
      "Epoch 35590 train loss 15782.771484375 val loss 52339.515625\n",
      "Epoch 35600 train loss 15809.8583984375 val loss 52368.3359375\n",
      "Epoch 35610 train loss 15780.8427734375 val loss 52361.37890625\n",
      "Epoch 35620 train loss 15791.875 val loss 52389.90234375\n",
      "Epoch 35630 train loss 15779.5302734375 val loss 52380.23046875\n",
      "Epoch 35640 train loss 15781.1953125 val loss 52380.62890625\n",
      "Epoch 35650 train loss 15791.4580078125 val loss 52423.41796875\n",
      "Epoch 35660 train loss 15791.8154296875 val loss 52407.0625\n",
      "Epoch 35670 train loss 15776.1787109375 val loss 52410.91015625\n",
      "Epoch 35680 train loss 15794.623046875 val loss 52428.2421875\n",
      "Epoch 35690 train loss 15773.126953125 val loss 52453.31640625\n",
      "Epoch 35700 train loss 15772.396484375 val loss 52423.19921875\n",
      "Epoch 35710 train loss 15822.0908203125 val loss 52459.16796875\n",
      "Epoch 35720 train loss 15779.7734375 val loss 52475.75\n",
      "Epoch 35730 train loss 15773.533203125 val loss 52472.65234375\n",
      "Epoch 35740 train loss 15786.4765625 val loss 52498.35546875\n",
      "Epoch 35750 train loss 15770.591796875 val loss 52477.1015625\n",
      "Epoch 35760 train loss 15776.9013671875 val loss 52515.5859375\n",
      "Epoch 35770 train loss 15767.9912109375 val loss 52508.98828125\n",
      "Epoch 35780 train loss 15779.4052734375 val loss 52528.70703125\n",
      "Epoch 35790 train loss 15763.0107421875 val loss 52524.97265625\n",
      "Epoch 35800 train loss 15799.9951171875 val loss 52564.35546875\n",
      "Epoch 35810 train loss 15766.640625 val loss 52545.5390625\n",
      "Epoch 35820 train loss 15770.3818359375 val loss 52568.3203125\n",
      "Epoch 35830 train loss 15761.166015625 val loss 52551.45703125\n",
      "Epoch 35840 train loss 15796.921875 val loss 52571.6953125\n",
      "Epoch 35850 train loss 15777.890625 val loss 52577.6640625\n",
      "Epoch 35860 train loss 15759.7109375 val loss 52601.0078125\n",
      "Epoch 35870 train loss 15780.5439453125 val loss 52590.76171875\n",
      "Epoch 35880 train loss 15764.18359375 val loss 52615.91796875\n",
      "Epoch 35890 train loss 15763.72265625 val loss 52633.56640625\n",
      "Epoch 35900 train loss 15771.47265625 val loss 52617.7421875\n",
      "Epoch 35910 train loss 15754.7197265625 val loss 52631.546875\n",
      "Epoch 35920 train loss 15773.20703125 val loss 52670.26171875\n",
      "Epoch 35930 train loss 15759.228515625 val loss 52665.40234375\n",
      "Epoch 35940 train loss 15762.1708984375 val loss 52664.1640625\n",
      "Epoch 35950 train loss 15752.0751953125 val loss 52656.50390625\n",
      "Epoch 35960 train loss 15786.251953125 val loss 52667.0390625\n",
      "Epoch 35970 train loss 15748.9462890625 val loss 52641.6328125\n",
      "Epoch 35980 train loss 15763.3349609375 val loss 52682.44921875\n",
      "Epoch 35990 train loss 15751.681640625 val loss 52687.84765625\n",
      "Epoch 36000 train loss 15764.3046875 val loss 52693.16796875\n",
      "Epoch 36010 train loss 15761.6240234375 val loss 52696.953125\n",
      "Epoch 36020 train loss 15751.1572265625 val loss 52714.625\n",
      "Epoch 36030 train loss 15761.0859375 val loss 52749.9921875\n",
      "Epoch 36040 train loss 15745.353515625 val loss 52743.83203125\n",
      "Epoch 36050 train loss 15760.7138671875 val loss 52742.9375\n",
      "Epoch 36060 train loss 15743.5146484375 val loss 52746.4140625\n",
      "Epoch 36070 train loss 15761.8173828125 val loss 52784.9375\n",
      "Epoch 36080 train loss 15764.142578125 val loss 52769.55859375\n",
      "Epoch 36090 train loss 15740.935546875 val loss 52793.68359375\n",
      "Epoch 36100 train loss 15758.0185546875 val loss 52813.24609375\n",
      "Epoch 36110 train loss 15739.7646484375 val loss 52803.00390625\n",
      "Epoch 36120 train loss 15760.9462890625 val loss 52758.36328125\n",
      "Epoch 36130 train loss 15750.025390625 val loss 52808.84375\n",
      "Epoch 36140 train loss 15743.373046875 val loss 52819.7109375\n",
      "Epoch 36150 train loss 15743.404296875 val loss 52810.515625\n",
      "Epoch 36160 train loss 15748.8173828125 val loss 52810.22265625\n",
      "Epoch 36170 train loss 15734.2578125 val loss 52819.95703125\n",
      "Epoch 36180 train loss 15734.306640625 val loss 52833.03125\n",
      "Epoch 36190 train loss 15781.576171875 val loss 52850.03125\n",
      "Epoch 36200 train loss 15747.537109375 val loss 52859.90625\n",
      "Epoch 36210 train loss 15733.494140625 val loss 52864.71875\n",
      "Epoch 36220 train loss 15732.1435546875 val loss 52882.5546875\n",
      "Epoch 36230 train loss 15763.63671875 val loss 52905.3203125\n",
      "Epoch 36240 train loss 15744.9775390625 val loss 52923.5078125\n",
      "Epoch 36250 train loss 15727.9208984375 val loss 52890.41796875\n",
      "Epoch 36260 train loss 15737.095703125 val loss 52922.93359375\n",
      "Epoch 36270 train loss 15739.6630859375 val loss 52915.359375\n",
      "Epoch 36280 train loss 15731.9697265625 val loss 52930.31640625\n",
      "Epoch 36290 train loss 15737.9013671875 val loss 52936.671875\n",
      "Epoch 36300 train loss 15726.240234375 val loss 52963.9765625\n",
      "Epoch 36310 train loss 15748.8076171875 val loss 52965.16796875\n",
      "Epoch 36320 train loss 15726.3369140625 val loss 52979.80078125\n",
      "Epoch 36330 train loss 15722.8173828125 val loss 52996.9140625\n",
      "Epoch 36340 train loss 15767.4697265625 val loss 53032.67578125\n",
      "Epoch 36350 train loss 15751.3779296875 val loss 53001.03125\n",
      "Epoch 36360 train loss 15730.5517578125 val loss 53021.9140625\n",
      "Epoch 36370 train loss 15726.2861328125 val loss 53073.484375\n",
      "Epoch 36380 train loss 15724.40625 val loss 53040.68359375\n",
      "Epoch 36390 train loss 15726.375 val loss 53029.64453125\n",
      "Epoch 36400 train loss 15716.9638671875 val loss 53082.59375\n",
      "Epoch 36410 train loss 15743.810546875 val loss 53077.34375\n",
      "Epoch 36420 train loss 15716.9599609375 val loss 53103.3671875\n",
      "Epoch 36430 train loss 15748.52734375 val loss 53117.09765625\n",
      "Epoch 36440 train loss 15722.498046875 val loss 53106.17578125\n",
      "Epoch 36450 train loss 15717.7763671875 val loss 53118.70703125\n",
      "Epoch 36460 train loss 15718.8701171875 val loss 53123.015625\n",
      "Epoch 36470 train loss 15720.990234375 val loss 53130.96484375\n",
      "Epoch 36480 train loss 15734.8369140625 val loss 53136.86328125\n",
      "Epoch 36490 train loss 15711.8740234375 val loss 53168.44140625\n",
      "Epoch 36500 train loss 15729.9287109375 val loss 53172.5546875\n",
      "Epoch 36510 train loss 15719.21484375 val loss 53146.23828125\n",
      "Epoch 36520 train loss 15708.9404296875 val loss 53188.66015625\n",
      "Epoch 36530 train loss 15714.7294921875 val loss 53187.828125\n",
      "Epoch 36540 train loss 15735.623046875 val loss 53256.75\n",
      "Epoch 36550 train loss 15712.3701171875 val loss 53201.875\n",
      "Epoch 36560 train loss 15727.4931640625 val loss 53244.80859375\n",
      "Epoch 36570 train loss 15708.140625 val loss 53261.3671875\n",
      "Epoch 36580 train loss 15728.9990234375 val loss 53252.83203125\n",
      "Epoch 36590 train loss 15727.4755859375 val loss 53253.7578125\n",
      "Epoch 36600 train loss 15704.42578125 val loss 53259.23046875\n",
      "Epoch 36610 train loss 15719.1318359375 val loss 53290.609375\n",
      "Epoch 36620 train loss 15706.2880859375 val loss 53286.33984375\n",
      "Epoch 36630 train loss 15703.814453125 val loss 53295.4296875\n",
      "Epoch 36640 train loss 15705.986328125 val loss 53285.08203125\n",
      "Epoch 36650 train loss 15714.2236328125 val loss 53314.09765625\n",
      "Epoch 36660 train loss 15699.07421875 val loss 53320.30859375\n",
      "Epoch 36670 train loss 15711.8037109375 val loss 53353.46875\n",
      "Epoch 36680 train loss 15711.2353515625 val loss 53369.48828125\n",
      "Epoch 36690 train loss 15709.2490234375 val loss 53352.21484375\n",
      "Epoch 36700 train loss 15701.8271484375 val loss 53379.26953125\n",
      "Epoch 36710 train loss 15712.5634765625 val loss 53396.53515625\n",
      "Epoch 36720 train loss 15696.662109375 val loss 53391.9609375\n",
      "Epoch 36730 train loss 15695.5322265625 val loss 53394.00390625\n",
      "Epoch 36740 train loss 15723.064453125 val loss 53396.18359375\n",
      "Epoch 36750 train loss 15694.5654296875 val loss 53412.87109375\n",
      "Epoch 36760 train loss 15713.40234375 val loss 53399.35546875\n",
      "Epoch 36770 train loss 15692.8525390625 val loss 53401.67578125\n",
      "Epoch 36780 train loss 15709.4658203125 val loss 53468.52734375\n",
      "Epoch 36790 train loss 15689.73046875 val loss 53444.1015625\n",
      "Epoch 36800 train loss 15704.072265625 val loss 53463.5859375\n",
      "Epoch 36810 train loss 15708.671875 val loss 53478.421875\n",
      "Epoch 36820 train loss 15688.3095703125 val loss 53497.40234375\n",
      "Epoch 36830 train loss 15697.8779296875 val loss 53510.17578125\n",
      "Epoch 36840 train loss 15694.2041015625 val loss 53532.859375\n",
      "Epoch 36850 train loss 15692.267578125 val loss 53472.99609375\n",
      "Epoch 36860 train loss 15693.2939453125 val loss 53503.109375\n",
      "Epoch 36870 train loss 15706.2568359375 val loss 53505.53125\n",
      "Epoch 36880 train loss 15686.4267578125 val loss 53510.4140625\n",
      "Epoch 36890 train loss 15710.337890625 val loss 53532.83984375\n",
      "Epoch 36900 train loss 15682.619140625 val loss 53545.95703125\n",
      "Epoch 36910 train loss 15706.919921875 val loss 53572.5859375\n",
      "Epoch 36920 train loss 15681.6533203125 val loss 53563.19140625\n",
      "Epoch 36930 train loss 15696.490234375 val loss 53565.2265625\n",
      "Epoch 36940 train loss 15681.1044921875 val loss 53617.77734375\n",
      "Epoch 36950 train loss 15694.828125 val loss 53655.75\n",
      "Epoch 36960 train loss 15686.9443359375 val loss 53625.22265625\n",
      "Epoch 36970 train loss 15690.7587890625 val loss 53645.48828125\n",
      "Epoch 36980 train loss 15684.1005859375 val loss 53635.12109375\n",
      "Epoch 36990 train loss 15708.7080078125 val loss 53661.234375\n",
      "Epoch 37000 train loss 15678.9580078125 val loss 53651.328125\n",
      "Epoch 37010 train loss 15696.462890625 val loss 53656.05078125\n",
      "Epoch 37020 train loss 15699.6474609375 val loss 53717.671875\n",
      "Epoch 37030 train loss 15680.8857421875 val loss 53657.9609375\n",
      "Epoch 37040 train loss 15673.9755859375 val loss 53700.38671875\n",
      "Epoch 37050 train loss 15673.888671875 val loss 53713.28515625\n",
      "Epoch 37060 train loss 15680.9951171875 val loss 53717.35546875\n",
      "Epoch 37070 train loss 15675.572265625 val loss 53726.45703125\n",
      "Epoch 37080 train loss 15673.193359375 val loss 53749.3984375\n",
      "Epoch 37090 train loss 15702.109375 val loss 53752.21484375\n",
      "Epoch 37100 train loss 15673.5380859375 val loss 53759.796875\n",
      "Epoch 37110 train loss 15681.6396484375 val loss 53754.9921875\n",
      "Epoch 37120 train loss 15667.3251953125 val loss 53763.53125\n",
      "Epoch 37130 train loss 15668.0576171875 val loss 53767.76953125\n",
      "Epoch 37140 train loss 15713.4716796875 val loss 53767.25\n",
      "Epoch 37150 train loss 15670.658203125 val loss 53794.23828125\n",
      "Epoch 37160 train loss 15675.6083984375 val loss 53812.28125\n",
      "Epoch 37170 train loss 15669.236328125 val loss 53826.265625\n",
      "Epoch 37180 train loss 15690.59765625 val loss 53873.8203125\n",
      "Epoch 37190 train loss 15664.1416015625 val loss 53830.12890625\n",
      "Epoch 37200 train loss 15680.2509765625 val loss 53838.2421875\n",
      "Epoch 37210 train loss 15667.6376953125 val loss 53865.97265625\n",
      "Epoch 37220 train loss 15661.5419921875 val loss 53876.86328125\n",
      "Epoch 37230 train loss 15666.3818359375 val loss 53870.796875\n",
      "Epoch 37240 train loss 15686.98046875 val loss 53887.91015625\n",
      "Epoch 37250 train loss 15675.45703125 val loss 53924.8671875\n",
      "Epoch 37260 train loss 15662.93359375 val loss 53927.25\n",
      "Epoch 37270 train loss 15679.65234375 val loss 53935.9375\n",
      "Epoch 37280 train loss 15659.3212890625 val loss 53924.00390625\n",
      "Epoch 37290 train loss 15666.1787109375 val loss 53932.6328125\n",
      "Epoch 37300 train loss 15657.1748046875 val loss 53931.859375\n",
      "Epoch 37310 train loss 15677.0439453125 val loss 53940.421875\n",
      "Epoch 37320 train loss 15661.2861328125 val loss 53941.19140625\n",
      "Epoch 37330 train loss 15668.44921875 val loss 53969.921875\n",
      "Epoch 37340 train loss 15655.1171875 val loss 53972.5859375\n",
      "Epoch 37350 train loss 15664.498046875 val loss 53978.32421875\n",
      "Epoch 37360 train loss 15659.87109375 val loss 53995.2265625\n",
      "Epoch 37370 train loss 15660.28125 val loss 54041.48828125\n",
      "Epoch 37380 train loss 15670.677734375 val loss 54030.06640625\n",
      "Epoch 37390 train loss 15658.205078125 val loss 54018.79296875\n",
      "Epoch 37400 train loss 15657.884765625 val loss 54026.9609375\n",
      "Epoch 37410 train loss 15651.6259765625 val loss 54042.359375\n",
      "Epoch 37420 train loss 15685.0078125 val loss 54077.15625\n",
      "Epoch 37430 train loss 15649.052734375 val loss 54044.02734375\n",
      "Epoch 37440 train loss 15663.8828125 val loss 54049.125\n",
      "Epoch 37450 train loss 15660.515625 val loss 54075.19140625\n",
      "Epoch 37460 train loss 15651.05078125 val loss 54116.53515625\n",
      "Epoch 37470 train loss 15660.337890625 val loss 54105.91015625\n",
      "Epoch 37480 train loss 15645.3759765625 val loss 54121.07421875\n",
      "Epoch 37490 train loss 15671.4326171875 val loss 54151.7265625\n",
      "Epoch 37500 train loss 15652.3798828125 val loss 54118.34375\n",
      "Epoch 37510 train loss 15656.4814453125 val loss 54120.33203125\n",
      "Epoch 37520 train loss 15662.6064453125 val loss 54147.38671875\n",
      "Epoch 37530 train loss 15641.6142578125 val loss 54163.88671875\n",
      "Epoch 37540 train loss 15665.919921875 val loss 54151.54296875\n",
      "Epoch 37550 train loss 15640.123046875 val loss 54155.14453125\n",
      "Epoch 37560 train loss 15651.0341796875 val loss 54210.67578125\n",
      "Epoch 37570 train loss 15644.408203125 val loss 54202.51171875\n",
      "Epoch 37580 train loss 15656.51171875 val loss 54203.48828125\n",
      "Epoch 37590 train loss 15652.0625 val loss 54225.3828125\n",
      "Epoch 37600 train loss 15644.1826171875 val loss 54224.80859375\n",
      "Epoch 37610 train loss 15658.6005859375 val loss 54233.16796875\n",
      "Epoch 37620 train loss 15638.419921875 val loss 54247.1171875\n",
      "Epoch 37630 train loss 15664.6982421875 val loss 54298.3984375\n",
      "Epoch 37640 train loss 15637.4755859375 val loss 54285.29296875\n",
      "Epoch 37650 train loss 15651.083984375 val loss 54292.85546875\n",
      "Epoch 37660 train loss 15633.5556640625 val loss 54305.9609375\n",
      "Epoch 37670 train loss 15664.7685546875 val loss 54309.9765625\n",
      "Epoch 37680 train loss 15635.150390625 val loss 54295.6875\n",
      "Epoch 37690 train loss 15646.5546875 val loss 54310.27734375\n",
      "Epoch 37700 train loss 15632.3310546875 val loss 54321.54296875\n",
      "Epoch 37710 train loss 15635.4951171875 val loss 54327.0625\n",
      "Epoch 37720 train loss 15653.41015625 val loss 54364.33203125\n",
      "Epoch 37730 train loss 15664.791015625 val loss 54383.3359375\n",
      "Epoch 37740 train loss 15630.470703125 val loss 54369.65234375\n",
      "Epoch 37750 train loss 15641.001953125 val loss 54372.3984375\n",
      "Epoch 37760 train loss 15634.6796875 val loss 54393.546875\n",
      "Epoch 37770 train loss 15631.59375 val loss 54425.0078125\n",
      "Epoch 37780 train loss 15664.208984375 val loss 54457.62890625\n",
      "Epoch 37790 train loss 15633.748046875 val loss 54435.5546875\n",
      "Epoch 37800 train loss 15632.6181640625 val loss 54435.5234375\n",
      "Epoch 37810 train loss 15636.8046875 val loss 54461.7265625\n",
      "Epoch 37820 train loss 15635.4609375 val loss 54486.4453125\n",
      "Epoch 37830 train loss 15634.5224609375 val loss 54474.66796875\n",
      "Epoch 37840 train loss 15623.1728515625 val loss 54459.64453125\n",
      "Epoch 37850 train loss 15629.548828125 val loss 54468.26953125\n",
      "Epoch 37860 train loss 15640.6083984375 val loss 54523.75390625\n",
      "Epoch 37870 train loss 15632.78515625 val loss 54486.5859375\n",
      "Epoch 37880 train loss 15622.2060546875 val loss 54512.71484375\n",
      "Epoch 37890 train loss 15620.17578125 val loss 54529.10546875\n",
      "Epoch 37900 train loss 15673.7216796875 val loss 54513.57421875\n",
      "Epoch 37910 train loss 15627.7041015625 val loss 54547.09765625\n",
      "Epoch 37920 train loss 15620.8447265625 val loss 54565.78515625\n",
      "Epoch 37930 train loss 15633.685546875 val loss 54562.8203125\n",
      "Epoch 37940 train loss 15622.189453125 val loss 54548.37109375\n",
      "Epoch 37950 train loss 15626.6708984375 val loss 54606.91796875\n",
      "Epoch 37960 train loss 15619.0498046875 val loss 54596.23828125\n",
      "Epoch 37970 train loss 15633.6796875 val loss 54597.453125\n",
      "Epoch 37980 train loss 15627.1044921875 val loss 54627.86328125\n",
      "Epoch 37990 train loss 15614.9052734375 val loss 54622.78125\n",
      "Epoch 38000 train loss 15613.3603515625 val loss 54633.1640625\n",
      "Epoch 38010 train loss 15616.052734375 val loss 54652.90234375\n",
      "Epoch 38020 train loss 15644.953125 val loss 54641.4296875\n",
      "Epoch 38030 train loss 15623.99609375 val loss 54630.4921875\n",
      "Epoch 38040 train loss 15614.1005859375 val loss 54667.75390625\n",
      "Epoch 38050 train loss 15643.294921875 val loss 54699.1640625\n",
      "Epoch 38060 train loss 15610.3115234375 val loss 54717.51171875\n",
      "Epoch 38070 train loss 15634.4306640625 val loss 54735.94140625\n",
      "Epoch 38080 train loss 15609.1630859375 val loss 54710.734375\n",
      "Epoch 38090 train loss 15632.365234375 val loss 54695.5859375\n",
      "Epoch 38100 train loss 15610.66796875 val loss 54714.5\n",
      "Epoch 38110 train loss 15636.4521484375 val loss 54755.25390625\n",
      "Epoch 38120 train loss 15606.5400390625 val loss 54740.32421875\n",
      "Epoch 38130 train loss 15621.220703125 val loss 54738.59375\n",
      "Epoch 38140 train loss 15616.744140625 val loss 54790.6953125\n",
      "Epoch 38150 train loss 15617.4541015625 val loss 54796.84765625\n",
      "Epoch 38160 train loss 15606.05859375 val loss 54792.45703125\n",
      "Epoch 38170 train loss 15606.2119140625 val loss 54829.94140625\n",
      "Epoch 38180 train loss 15638.18359375 val loss 54798.9765625\n",
      "Epoch 38190 train loss 15622.69921875 val loss 54817.0\n",
      "Epoch 38200 train loss 15601.970703125 val loss 54826.34765625\n",
      "Epoch 38210 train loss 15608.0927734375 val loss 54839.58203125\n",
      "Epoch 38220 train loss 15622.8134765625 val loss 54882.328125\n",
      "Epoch 38230 train loss 15599.4658203125 val loss 54852.55078125\n",
      "Epoch 38240 train loss 15623.8427734375 val loss 54891.08984375\n",
      "Epoch 38250 train loss 15598.9248046875 val loss 54903.97265625\n",
      "Epoch 38260 train loss 15609.4462890625 val loss 54912.74609375\n",
      "Epoch 38270 train loss 15601.9697265625 val loss 54906.59375\n",
      "Epoch 38280 train loss 15617.4462890625 val loss 54920.0078125\n",
      "Epoch 38290 train loss 15623.033203125 val loss 54904.22265625\n",
      "Epoch 38300 train loss 15597.3896484375 val loss 54884.96484375\n",
      "Epoch 38310 train loss 15609.892578125 val loss 54900.4375\n",
      "Epoch 38320 train loss 15594.615234375 val loss 54940.34765625\n",
      "Epoch 38330 train loss 15629.3779296875 val loss 54995.56640625\n",
      "Epoch 38340 train loss 15594.3291015625 val loss 54963.6328125\n",
      "Epoch 38350 train loss 15601.7705078125 val loss 54948.6640625\n",
      "Epoch 38360 train loss 15590.677734375 val loss 54989.6796875\n",
      "Epoch 38370 train loss 15646.1640625 val loss 55012.65625\n",
      "Epoch 38380 train loss 15606.142578125 val loss 55010.01171875\n",
      "Epoch 38390 train loss 15588.7353515625 val loss 55024.9453125\n",
      "Epoch 38400 train loss 15610.630859375 val loss 55046.97265625\n",
      "Epoch 38410 train loss 15588.83203125 val loss 55053.31640625\n",
      "Epoch 38420 train loss 15621.6279296875 val loss 55056.375\n",
      "Epoch 38430 train loss 15592.11328125 val loss 55069.62109375\n",
      "Epoch 38440 train loss 15590.705078125 val loss 55091.828125\n",
      "Epoch 38450 train loss 15608.056640625 val loss 55113.61328125\n",
      "Epoch 38460 train loss 15585.7412109375 val loss 55106.328125\n",
      "Epoch 38470 train loss 15614.0185546875 val loss 55108.578125\n",
      "Epoch 38480 train loss 15584.9677734375 val loss 55122.96875\n",
      "Epoch 38490 train loss 15598.5439453125 val loss 55140.078125\n",
      "Epoch 38500 train loss 15584.451171875 val loss 55159.55078125\n",
      "Epoch 38510 train loss 15597.923828125 val loss 55168.25390625\n",
      "Epoch 38520 train loss 15592.9833984375 val loss 55186.42578125\n",
      "Epoch 38530 train loss 15581.6689453125 val loss 55207.23828125\n",
      "Epoch 38540 train loss 15593.755859375 val loss 55195.55859375\n",
      "Epoch 38550 train loss 15598.2587890625 val loss 55201.67578125\n",
      "Epoch 38560 train loss 15616.2109375 val loss 55188.4453125\n",
      "Epoch 38570 train loss 15585.6142578125 val loss 55212.48828125\n",
      "Epoch 38580 train loss 15583.4912109375 val loss 55248.37109375\n",
      "Epoch 38590 train loss 15581.7978515625 val loss 55215.27734375\n",
      "Epoch 38600 train loss 15584.8662109375 val loss 55233.20703125\n",
      "Epoch 38610 train loss 15580.572265625 val loss 55253.66015625\n",
      "Epoch 38620 train loss 15610.9970703125 val loss 55315.53125\n",
      "Epoch 38630 train loss 15575.9931640625 val loss 55263.0703125\n",
      "Epoch 38640 train loss 15591.599609375 val loss 55284.49609375\n",
      "Epoch 38650 train loss 15591.703125 val loss 55346.83203125\n",
      "Epoch 38660 train loss 15574.470703125 val loss 55315.10546875\n",
      "Epoch 38670 train loss 15587.943359375 val loss 55298.55078125\n",
      "Epoch 38680 train loss 15573.353515625 val loss 55305.26953125\n",
      "Epoch 38690 train loss 15590.00390625 val loss 55360.03125\n",
      "Epoch 38700 train loss 15570.736328125 val loss 55324.5625\n",
      "Epoch 38710 train loss 15572.26953125 val loss 55350.05078125\n",
      "Epoch 38720 train loss 15584.8330078125 val loss 55381.71484375\n",
      "Epoch 38730 train loss 15572.1279296875 val loss 55381.26953125\n",
      "Epoch 38740 train loss 15588.6806640625 val loss 55388.8125\n",
      "Epoch 38750 train loss 15571.3798828125 val loss 55385.84375\n",
      "Epoch 38760 train loss 15584.5185546875 val loss 55401.48828125\n",
      "Epoch 38770 train loss 15571.576171875 val loss 55368.84375\n",
      "Epoch 38780 train loss 15571.046875 val loss 55416.48046875\n",
      "Epoch 38790 train loss 15587.8564453125 val loss 55440.85546875\n",
      "Epoch 38800 train loss 15564.8759765625 val loss 55418.67578125\n",
      "Epoch 38810 train loss 15565.55859375 val loss 55417.8203125\n",
      "Epoch 38820 train loss 15563.232421875 val loss 55458.74609375\n",
      "Epoch 38830 train loss 15565.96484375 val loss 55433.99609375\n",
      "Epoch 38840 train loss 15567.375 val loss 55449.30078125\n",
      "Epoch 38850 train loss 15558.10546875 val loss 55476.53125\n",
      "Epoch 38860 train loss 15593.619140625 val loss 55517.16015625\n",
      "Epoch 38870 train loss 15578.2275390625 val loss 55453.88671875\n",
      "Epoch 38880 train loss 15555.1689453125 val loss 55473.00390625\n",
      "Epoch 38890 train loss 15591.4052734375 val loss 55485.99609375\n",
      "Epoch 38900 train loss 15577.400390625 val loss 55474.00390625\n",
      "Epoch 38910 train loss 15556.447265625 val loss 55481.203125\n",
      "Epoch 38920 train loss 15564.9443359375 val loss 55524.875\n",
      "Epoch 38930 train loss 15561.9619140625 val loss 55488.0859375\n",
      "Epoch 38940 train loss 15557.1455078125 val loss 55498.1484375\n",
      "Epoch 38950 train loss 15555.9375 val loss 55539.83203125\n",
      "Epoch 38960 train loss 15571.1845703125 val loss 55542.88671875\n",
      "Epoch 38970 train loss 15552.6357421875 val loss 55571.01953125\n",
      "Epoch 38980 train loss 15580.826171875 val loss 55552.828125\n",
      "Epoch 38990 train loss 15552.4697265625 val loss 55542.8828125\n",
      "Epoch 39000 train loss 15570.8564453125 val loss 55571.07421875\n",
      "Epoch 39010 train loss 15546.74609375 val loss 55531.09375\n",
      "Epoch 39020 train loss 15554.7275390625 val loss 55536.3046875\n",
      "Epoch 39030 train loss 15564.5263671875 val loss 55540.99609375\n",
      "Epoch 39040 train loss 15552.302734375 val loss 55582.10546875\n",
      "Epoch 39050 train loss 15556.16015625 val loss 55605.50390625\n",
      "Epoch 39060 train loss 15541.8828125 val loss 55576.5078125\n",
      "Epoch 39070 train loss 15584.8623046875 val loss 55644.94921875\n",
      "Epoch 39080 train loss 15545.4853515625 val loss 55580.03515625\n",
      "Epoch 39090 train loss 15543.1630859375 val loss 55610.93359375\n",
      "Epoch 39100 train loss 15551.92578125 val loss 55619.4609375\n",
      "Epoch 39110 train loss 15540.4306640625 val loss 55621.2578125\n",
      "Epoch 39120 train loss 15554.9951171875 val loss 55617.2578125\n",
      "Epoch 39130 train loss 15541.685546875 val loss 55616.875\n",
      "Epoch 39140 train loss 15554.01953125 val loss 55604.23046875\n",
      "Epoch 39150 train loss 15539.087890625 val loss 55646.05078125\n",
      "Epoch 39160 train loss 15535.705078125 val loss 55624.1796875\n",
      "Epoch 39170 train loss 15542.2900390625 val loss 55646.37109375\n",
      "Epoch 39180 train loss 15568.1875 val loss 55655.19921875\n",
      "Epoch 39190 train loss 15542.4423828125 val loss 55687.09765625\n",
      "Epoch 39200 train loss 15557.6787109375 val loss 55686.9453125\n",
      "Epoch 39210 train loss 15534.841796875 val loss 55648.69140625\n",
      "Epoch 39220 train loss 15556.3876953125 val loss 55677.09375\n",
      "Epoch 39230 train loss 15549.5087890625 val loss 55705.06640625\n",
      "Epoch 39240 train loss 15528.943359375 val loss 55663.9375\n",
      "Epoch 39250 train loss 15543.2490234375 val loss 55700.46875\n",
      "Epoch 39260 train loss 15527.376953125 val loss 55700.19921875\n",
      "Epoch 39270 train loss 15548.6044921875 val loss 55738.14453125\n",
      "Epoch 39280 train loss 15525.1357421875 val loss 55728.2578125\n",
      "Epoch 39290 train loss 15527.4013671875 val loss 55703.0\n",
      "Epoch 39300 train loss 15537.7001953125 val loss 55717.140625\n",
      "Epoch 39310 train loss 15529.8388671875 val loss 55697.85546875\n",
      "Epoch 39320 train loss 15521.4599609375 val loss 55731.68359375\n",
      "Epoch 39330 train loss 15531.375 val loss 55750.109375\n",
      "Epoch 39340 train loss 15543.373046875 val loss 55750.99609375\n",
      "Epoch 39350 train loss 15543.353515625 val loss 55754.37890625\n",
      "Epoch 39360 train loss 15533.330078125 val loss 55790.74609375\n",
      "Epoch 39370 train loss 15520.595703125 val loss 55775.40234375\n",
      "Epoch 39380 train loss 15526.412109375 val loss 55768.14453125\n",
      "Epoch 39390 train loss 15532.2666015625 val loss 55784.97265625\n",
      "Epoch 39400 train loss 15518.38671875 val loss 55764.46484375\n",
      "Epoch 39410 train loss 15520.0068359375 val loss 55760.20703125\n",
      "Epoch 39420 train loss 15529.9169921875 val loss 55765.92578125\n",
      "Epoch 39430 train loss 15514.8056640625 val loss 55736.97265625\n",
      "Epoch 39440 train loss 15513.046875 val loss 55792.99609375\n",
      "Epoch 39450 train loss 15521.263671875 val loss 55793.96484375\n",
      "Epoch 39460 train loss 15530.9423828125 val loss 55758.51171875\n",
      "Epoch 39470 train loss 15535.068359375 val loss 55800.36328125\n",
      "Epoch 39480 train loss 15507.7158203125 val loss 55749.4921875\n",
      "Epoch 39490 train loss 15532.2373046875 val loss 55752.35546875\n",
      "Epoch 39500 train loss 15506.341796875 val loss 55762.5625\n",
      "Epoch 39510 train loss 15514.5087890625 val loss 55773.92578125\n",
      "Epoch 39520 train loss 15528.9599609375 val loss 55768.20703125\n",
      "Epoch 39530 train loss 15504.3037109375 val loss 55760.96484375\n",
      "Epoch 39540 train loss 15516.98828125 val loss 55765.78125\n",
      "Epoch 39550 train loss 15512.390625 val loss 55801.52734375\n",
      "Epoch 39560 train loss 15506.8056640625 val loss 55757.796875\n",
      "Epoch 39570 train loss 15530.892578125 val loss 55767.71875\n",
      "Epoch 39580 train loss 15498.8955078125 val loss 55796.35546875\n",
      "Epoch 39590 train loss 15519.8408203125 val loss 55804.4921875\n",
      "Epoch 39600 train loss 15496.5263671875 val loss 55791.140625\n",
      "Epoch 39610 train loss 15524.2314453125 val loss 55780.65234375\n",
      "Epoch 39620 train loss 15496.634765625 val loss 55810.95703125\n",
      "Epoch 39630 train loss 15505.04296875 val loss 55801.65625\n",
      "Epoch 39640 train loss 15494.115234375 val loss 55788.7578125\n",
      "Epoch 39650 train loss 15512.751953125 val loss 55785.45703125\n",
      "Epoch 39660 train loss 15494.62890625 val loss 55797.2734375\n",
      "Epoch 39670 train loss 15501.0859375 val loss 55804.359375\n",
      "Epoch 39680 train loss 15507.9521484375 val loss 55835.02734375\n",
      "Epoch 39690 train loss 15491.6826171875 val loss 55806.98046875\n",
      "Epoch 39700 train loss 15495.5986328125 val loss 55855.93359375\n",
      "Epoch 39710 train loss 15493.9892578125 val loss 55850.71484375\n",
      "Epoch 39720 train loss 15492.40234375 val loss 55841.390625\n",
      "Epoch 39730 train loss 15504.3896484375 val loss 55846.2265625\n",
      "Epoch 39740 train loss 15495.9833984375 val loss 55839.02734375\n",
      "Epoch 39750 train loss 15509.822265625 val loss 55845.6640625\n",
      "Epoch 39760 train loss 15485.78125 val loss 55857.76171875\n",
      "Epoch 39770 train loss 15497.7880859375 val loss 55894.4453125\n",
      "Epoch 39780 train loss 15483.8642578125 val loss 55867.44140625\n",
      "Epoch 39790 train loss 15500.044921875 val loss 55875.06640625\n",
      "Epoch 39800 train loss 15490.3974609375 val loss 55890.53515625\n",
      "Epoch 39810 train loss 15479.421875 val loss 55900.046875\n",
      "Epoch 39820 train loss 15502.0830078125 val loss 55929.5390625\n",
      "Epoch 39830 train loss 15483.8046875 val loss 55893.2734375\n",
      "Epoch 39840 train loss 15494.9111328125 val loss 55882.421875\n",
      "Epoch 39850 train loss 15495.2685546875 val loss 55949.98828125\n",
      "Epoch 39860 train loss 15480.685546875 val loss 55914.24609375\n",
      "Epoch 39870 train loss 15488.0009765625 val loss 55925.0625\n",
      "Epoch 39880 train loss 15486.734375 val loss 55971.4375\n",
      "Epoch 39890 train loss 15480.08203125 val loss 55961.91015625\n",
      "Epoch 39900 train loss 15479.748046875 val loss 55955.05078125\n",
      "Epoch 39910 train loss 15479.333984375 val loss 55961.1875\n",
      "Epoch 39920 train loss 15508.1904296875 val loss 55953.3671875\n",
      "Epoch 39930 train loss 15483.6962890625 val loss 55999.4296875\n",
      "Epoch 39940 train loss 15478.0791015625 val loss 55996.24609375\n",
      "Epoch 39950 train loss 15474.5693359375 val loss 55987.671875\n",
      "Epoch 39960 train loss 15485.8193359375 val loss 55976.48046875\n",
      "Epoch 39970 train loss 15467.4345703125 val loss 55998.18359375\n",
      "Epoch 39980 train loss 15481.14453125 val loss 56026.203125\n",
      "Epoch 39990 train loss 15485.1806640625 val loss 56012.53515625\n",
      "Epoch 40000 train loss 15482.361328125 val loss 56000.29296875\n",
      "Epoch 40010 train loss 15491.0615234375 val loss 56031.60546875\n",
      "Epoch 40020 train loss 15465.47265625 val loss 56029.17578125\n",
      "Epoch 40030 train loss 15470.34375 val loss 56036.96484375\n",
      "Epoch 40040 train loss 15469.8115234375 val loss 56089.9375\n",
      "Epoch 40050 train loss 15478.5341796875 val loss 56076.94921875\n",
      "Epoch 40060 train loss 15480.5234375 val loss 56061.76171875\n",
      "Epoch 40070 train loss 15460.658203125 val loss 56089.91796875\n",
      "Epoch 40080 train loss 15463.1005859375 val loss 56112.6953125\n",
      "Epoch 40090 train loss 15475.185546875 val loss 56110.37890625\n",
      "Epoch 40100 train loss 15474.20703125 val loss 56099.8828125\n",
      "Epoch 40110 train loss 15483.3876953125 val loss 56079.9375\n",
      "Epoch 40120 train loss 15458.8291015625 val loss 56087.2734375\n",
      "Epoch 40130 train loss 15460.9208984375 val loss 56138.08203125\n",
      "Epoch 40140 train loss 15490.8603515625 val loss 56161.7265625\n",
      "Epoch 40150 train loss 15473.5595703125 val loss 56136.91015625\n",
      "Epoch 40160 train loss 15455.3544921875 val loss 56158.4609375\n",
      "Epoch 40170 train loss 15454.0263671875 val loss 56161.328125\n",
      "Epoch 40180 train loss 15467.4990234375 val loss 56196.734375\n",
      "Epoch 40190 train loss 15467.4638671875 val loss 56183.31640625\n",
      "Epoch 40200 train loss 15473.1337890625 val loss 56162.6328125\n",
      "Epoch 40210 train loss 15450.326171875 val loss 56200.4453125\n",
      "Epoch 40220 train loss 15495.517578125 val loss 56241.8359375\n",
      "Epoch 40230 train loss 15488.111328125 val loss 56206.6875\n",
      "Epoch 40240 train loss 15461.08203125 val loss 56249.29296875\n",
      "Epoch 40250 train loss 15448.8544921875 val loss 56236.03125\n",
      "Epoch 40260 train loss 15450.4560546875 val loss 56244.6171875\n",
      "Epoch 40270 train loss 15466.9384765625 val loss 56287.83203125\n",
      "Epoch 40280 train loss 15446.3486328125 val loss 56258.82421875\n",
      "Epoch 40290 train loss 15458.708984375 val loss 56292.75390625\n",
      "Epoch 40300 train loss 15459.865234375 val loss 56283.43359375\n",
      "Epoch 40310 train loss 15473.076171875 val loss 56319.24609375\n",
      "Epoch 40320 train loss 15442.416015625 val loss 56291.9921875\n",
      "Epoch 40330 train loss 15467.2138671875 val loss 56312.5234375\n",
      "Epoch 40340 train loss 15446.951171875 val loss 56335.4375\n",
      "Epoch 40350 train loss 15448.4873046875 val loss 56348.26953125\n",
      "Epoch 40360 train loss 15440.5546875 val loss 56366.328125\n",
      "Epoch 40370 train loss 15471.326171875 val loss 56348.35546875\n",
      "Epoch 40380 train loss 15439.896484375 val loss 56380.7734375\n",
      "Epoch 40390 train loss 15450.9345703125 val loss 56361.5390625\n",
      "Epoch 40400 train loss 15447.1953125 val loss 56380.921875\n",
      "Epoch 40410 train loss 15437.771484375 val loss 56381.84375\n",
      "Epoch 40420 train loss 15447.4912109375 val loss 56428.84765625\n",
      "Epoch 40430 train loss 15433.94140625 val loss 56408.2109375\n",
      "Epoch 40440 train loss 15456.69921875 val loss 56420.3125\n",
      "Epoch 40450 train loss 15431.9443359375 val loss 56454.31640625\n",
      "Epoch 40460 train loss 15468.8330078125 val loss 56482.91796875\n",
      "Epoch 40470 train loss 15435.05859375 val loss 56448.7109375\n",
      "Epoch 40480 train loss 15443.890625 val loss 56455.98828125\n",
      "Epoch 40490 train loss 15429.2158203125 val loss 56458.2421875\n",
      "Epoch 40500 train loss 15479.9326171875 val loss 56537.81640625\n",
      "Epoch 40510 train loss 15467.0595703125 val loss 56492.0625\n",
      "Epoch 40520 train loss 15439.19921875 val loss 56528.10546875\n",
      "Epoch 40530 train loss 15429.6396484375 val loss 56533.0234375\n",
      "Epoch 40540 train loss 15434.208984375 val loss 56559.34375\n",
      "Epoch 40550 train loss 15440.9755859375 val loss 56573.2265625\n",
      "Epoch 40560 train loss 15424.8935546875 val loss 56576.75\n",
      "Epoch 40570 train loss 15455.4541015625 val loss 56556.76953125\n",
      "Epoch 40580 train loss 15433.142578125 val loss 56570.64453125\n",
      "Epoch 40590 train loss 15428.3916015625 val loss 56568.98828125\n",
      "Epoch 40600 train loss 15445.2724609375 val loss 56584.18359375\n",
      "Epoch 40610 train loss 15433.8798828125 val loss 56615.30078125\n",
      "Epoch 40620 train loss 15423.01953125 val loss 56625.26953125\n",
      "Epoch 40630 train loss 15447.08984375 val loss 56642.19921875\n",
      "Epoch 40640 train loss 15425.7978515625 val loss 56678.37109375\n",
      "Epoch 40650 train loss 15418.1923828125 val loss 56659.2578125\n",
      "Epoch 40660 train loss 15442.1826171875 val loss 56655.99609375\n",
      "Epoch 40670 train loss 15417.07421875 val loss 56672.390625\n",
      "Epoch 40680 train loss 15438.9873046875 val loss 56708.37890625\n",
      "Epoch 40690 train loss 15416.44921875 val loss 56705.41015625\n",
      "Epoch 40700 train loss 15424.8232421875 val loss 56721.8125\n",
      "Epoch 40710 train loss 15421.7314453125 val loss 56732.04296875\n",
      "Epoch 40720 train loss 15433.240234375 val loss 56766.51953125\n",
      "Epoch 40730 train loss 15415.7333984375 val loss 56736.19921875\n",
      "Epoch 40740 train loss 15425.29296875 val loss 56760.84765625\n",
      "Epoch 40750 train loss 15414.9580078125 val loss 56750.0234375\n",
      "Epoch 40760 train loss 15446.130859375 val loss 56734.56640625\n",
      "Epoch 40770 train loss 15439.8876953125 val loss 56813.69921875\n",
      "Epoch 40780 train loss 15415.701171875 val loss 56806.5625\n",
      "Epoch 40790 train loss 15413.7646484375 val loss 56793.92578125\n",
      "Epoch 40800 train loss 15408.4599609375 val loss 56834.546875\n",
      "Epoch 40810 train loss 15408.173828125 val loss 56820.83203125\n",
      "Epoch 40820 train loss 15474.6025390625 val loss 56803.8359375\n",
      "Epoch 40830 train loss 15433.025390625 val loss 56847.73046875\n",
      "Epoch 40840 train loss 15409.6982421875 val loss 56847.76171875\n",
      "Epoch 40850 train loss 15417.1953125 val loss 56860.4140625\n",
      "Epoch 40860 train loss 15413.3876953125 val loss 56856.15625\n",
      "Epoch 40870 train loss 15420.1279296875 val loss 56869.23828125\n",
      "Epoch 40880 train loss 15427.2236328125 val loss 56910.22265625\n",
      "Epoch 40890 train loss 15403.1806640625 val loss 56897.87890625\n",
      "Epoch 40900 train loss 15427.23828125 val loss 56908.93359375\n",
      "Epoch 40910 train loss 15408.0517578125 val loss 56926.81640625\n",
      "Epoch 40920 train loss 15404.2734375 val loss 56922.8359375\n",
      "Epoch 40930 train loss 15425.779296875 val loss 56989.71484375\n",
      "Epoch 40940 train loss 15399.708984375 val loss 56962.01171875\n",
      "Epoch 40950 train loss 15412.3125 val loss 56963.9453125\n",
      "Epoch 40960 train loss 15408.2822265625 val loss 57006.203125\n",
      "Epoch 40970 train loss 15402.5419921875 val loss 57006.05859375\n",
      "Epoch 40980 train loss 15431.703125 val loss 56977.32421875\n",
      "Epoch 40990 train loss 15403.8896484375 val loss 57017.08203125\n",
      "Epoch 41000 train loss 15397.982421875 val loss 57008.7734375\n",
      "Epoch 41010 train loss 15425.287109375 val loss 57021.328125\n",
      "Epoch 41020 train loss 15395.4375 val loss 57048.92578125\n",
      "Epoch 41030 train loss 15397.8720703125 val loss 57038.1328125\n",
      "Epoch 41040 train loss 15393.1787109375 val loss 57082.86328125\n",
      "Epoch 41050 train loss 15439.25390625 val loss 57054.8125\n",
      "Epoch 41060 train loss 15391.9931640625 val loss 57066.96875\n",
      "Epoch 41070 train loss 15427.31640625 val loss 57114.4453125\n",
      "Epoch 41080 train loss 15394.552734375 val loss 57086.6328125\n",
      "Epoch 41090 train loss 15409.7822265625 val loss 57075.16796875\n",
      "Epoch 41100 train loss 15391.369140625 val loss 57108.9375\n",
      "Epoch 41110 train loss 15420.7021484375 val loss 57147.05078125\n",
      "Epoch 41120 train loss 15395.6015625 val loss 57112.375\n",
      "Epoch 41130 train loss 15401.208984375 val loss 57127.16796875\n",
      "Epoch 41140 train loss 15388.94140625 val loss 57171.8125\n",
      "Epoch 41150 train loss 15425.3408203125 val loss 57167.73828125\n",
      "Epoch 41160 train loss 15397.4755859375 val loss 57105.62890625\n",
      "Epoch 41170 train loss 15390.65234375 val loss 57102.4609375\n",
      "Epoch 41180 train loss 15386.4326171875 val loss 57149.75\n",
      "Epoch 41190 train loss 15408.537109375 val loss 57179.296875\n",
      "Epoch 41200 train loss 15408.98046875 val loss 57146.44921875\n",
      "Epoch 41210 train loss 15384.04296875 val loss 57192.6640625\n",
      "Epoch 41220 train loss 15383.4921875 val loss 57219.390625\n",
      "Epoch 41230 train loss 15419.8896484375 val loss 57207.81640625\n",
      "Epoch 41240 train loss 15411.1201171875 val loss 57247.98046875\n",
      "Epoch 41250 train loss 15382.02734375 val loss 57252.53515625\n",
      "Epoch 41260 train loss 15385.7470703125 val loss 57243.453125\n",
      "Epoch 41270 train loss 15400.9599609375 val loss 57262.21875\n",
      "Epoch 41280 train loss 15379.865234375 val loss 57292.07421875\n",
      "Epoch 41290 train loss 15423.0361328125 val loss 57281.5078125\n",
      "Epoch 41300 train loss 15383.205078125 val loss 57286.92578125\n",
      "Epoch 41310 train loss 15395.546875 val loss 57298.0625\n",
      "Epoch 41320 train loss 15387.1533203125 val loss 57309.34765625\n",
      "Epoch 41330 train loss 15383.384765625 val loss 57303.0390625\n",
      "Epoch 41340 train loss 15377.2333984375 val loss 57334.5859375\n",
      "Epoch 41350 train loss 15418.6943359375 val loss 57354.91796875\n",
      "Epoch 41360 train loss 15389.76953125 val loss 57314.94921875\n",
      "Epoch 41370 train loss 15376.138671875 val loss 57348.8125\n",
      "Epoch 41380 train loss 15407.162109375 val loss 57421.83203125\n",
      "Epoch 41390 train loss 15374.34375 val loss 57380.296875\n",
      "Epoch 41400 train loss 15394.001953125 val loss 57361.65234375\n",
      "Epoch 41410 train loss 15372.0732421875 val loss 57379.5\n",
      "Epoch 41420 train loss 15408.458984375 val loss 57443.1171875\n",
      "Epoch 41430 train loss 15371.9921875 val loss 57397.2265625\n",
      "Epoch 41440 train loss 15398.544921875 val loss 57376.12109375\n",
      "Epoch 41450 train loss 15370.3828125 val loss 57432.04296875\n",
      "Epoch 41460 train loss 15402.763671875 val loss 57470.8203125\n",
      "Epoch 41470 train loss 15369.990234375 val loss 57441.98046875\n",
      "Epoch 41480 train loss 15387.10546875 val loss 57434.08203125\n",
      "Epoch 41490 train loss 15375.203125 val loss 57445.86328125\n",
      "Epoch 41500 train loss 15388.111328125 val loss 57472.1796875\n",
      "Epoch 41510 train loss 15370.2265625 val loss 57476.76953125\n",
      "Epoch 41520 train loss 15373.7734375 val loss 57494.2109375\n",
      "Epoch 41530 train loss 15384.9716796875 val loss 57466.6796875\n",
      "Epoch 41540 train loss 15367.484375 val loss 57522.89453125\n",
      "Epoch 41550 train loss 15415.4755859375 val loss 57541.01171875\n",
      "Epoch 41560 train loss 15375.765625 val loss 57506.1015625\n",
      "Epoch 41570 train loss 15369.9833984375 val loss 57512.51953125\n",
      "Epoch 41580 train loss 15391.93359375 val loss 57583.9921875\n",
      "Epoch 41590 train loss 15363.2265625 val loss 57539.49609375\n",
      "Epoch 41600 train loss 15378.8779296875 val loss 57563.2578125\n",
      "Epoch 41610 train loss 15388.697265625 val loss 57594.23046875\n",
      "Epoch 41620 train loss 15364.02734375 val loss 57579.19921875\n",
      "Epoch 41630 train loss 15367.365234375 val loss 57594.5625\n",
      "Epoch 41640 train loss 15375.203125 val loss 57615.82421875\n",
      "Epoch 41650 train loss 15362.9892578125 val loss 57604.5234375\n",
      "Epoch 41660 train loss 15388.5654296875 val loss 57627.3203125\n",
      "Epoch 41670 train loss 15363.1474609375 val loss 57604.796875\n",
      "Epoch 41680 train loss 15373.41015625 val loss 57617.4375\n",
      "Epoch 41690 train loss 15360.623046875 val loss 57650.86328125\n",
      "Epoch 41700 train loss 15358.2119140625 val loss 57654.8515625\n",
      "Epoch 41710 train loss 15394.2490234375 val loss 57658.39453125\n",
      "Epoch 41720 train loss 15360.4365234375 val loss 57700.90625\n",
      "Epoch 41730 train loss 15355.794921875 val loss 57693.5546875\n",
      "Epoch 41740 train loss 15366.9189453125 val loss 57673.64453125\n",
      "Epoch 41750 train loss 15363.5 val loss 57704.96484375\n",
      "Epoch 41760 train loss 15368.9892578125 val loss 57723.02734375\n",
      "Epoch 41770 train loss 15354.232421875 val loss 57701.65234375\n",
      "Epoch 41780 train loss 15365.64453125 val loss 57707.36328125\n",
      "Epoch 41790 train loss 15359.091796875 val loss 57717.51171875\n",
      "Epoch 41800 train loss 15354.060546875 val loss 57726.6953125\n",
      "Epoch 41810 train loss 15385.0205078125 val loss 57726.87890625\n",
      "Epoch 41820 train loss 15354.34375 val loss 57760.0703125\n",
      "Epoch 41830 train loss 15369.2099609375 val loss 57820.34375\n",
      "Epoch 41840 train loss 15350.208984375 val loss 57779.07421875\n",
      "Epoch 41850 train loss 15364.099609375 val loss 57794.96484375\n",
      "Epoch 41860 train loss 15376.2958984375 val loss 57806.78515625\n",
      "Epoch 41870 train loss 15351.181640625 val loss 57821.2578125\n",
      "Epoch 41880 train loss 15368.1708984375 val loss 57842.05078125\n",
      "Epoch 41890 train loss 15352.771484375 val loss 57853.9609375\n",
      "Epoch 41900 train loss 15363.91796875 val loss 57876.97265625\n",
      "Epoch 41910 train loss 15348.380859375 val loss 57832.671875\n",
      "Epoch 41920 train loss 15357.3359375 val loss 57836.5703125\n",
      "Epoch 41930 train loss 15364.7451171875 val loss 57890.8515625\n",
      "Epoch 41940 train loss 15344.7763671875 val loss 57872.6484375\n",
      "Epoch 41950 train loss 15367.0693359375 val loss 57880.96875\n",
      "Epoch 41960 train loss 15347.009765625 val loss 57882.95703125\n",
      "Epoch 41970 train loss 15365.646484375 val loss 57894.7578125\n",
      "Epoch 41980 train loss 15347.7333984375 val loss 57950.87109375\n",
      "Epoch 41990 train loss 15347.7060546875 val loss 57903.66796875\n",
      "Epoch 42000 train loss 15374.509765625 val loss 57893.5078125\n",
      "Epoch 42010 train loss 15351.9072265625 val loss 57960.3046875\n",
      "Epoch 42020 train loss 15361.353515625 val loss 57970.48046875\n",
      "Epoch 42030 train loss 15341.4443359375 val loss 57950.78125\n",
      "Epoch 42040 train loss 15378.9306640625 val loss 57974.609375\n",
      "Epoch 42050 train loss 15343.8720703125 val loss 57999.2734375\n",
      "Epoch 42060 train loss 15358.2294921875 val loss 58045.68359375\n",
      "Epoch 42070 train loss 15380.23046875 val loss 57952.11328125\n",
      "Epoch 42080 train loss 15358.1474609375 val loss 58042.1328125\n",
      "Epoch 42090 train loss 15337.0205078125 val loss 58014.72265625\n",
      "Epoch 42100 train loss 15344.322265625 val loss 58026.734375\n",
      "Epoch 42110 train loss 15352.626953125 val loss 58064.84765625\n",
      "Epoch 42120 train loss 15344.197265625 val loss 58051.88671875\n",
      "Epoch 42130 train loss 15344.0703125 val loss 58067.09375\n",
      "Epoch 42140 train loss 15336.5185546875 val loss 58071.015625\n",
      "Epoch 42150 train loss 15358.47265625 val loss 58087.98046875\n",
      "Epoch 42160 train loss 15334.4345703125 val loss 58070.453125\n",
      "Epoch 42170 train loss 15343.7626953125 val loss 58090.44140625\n",
      "Epoch 42180 train loss 15346.2509765625 val loss 58075.10546875\n",
      "Epoch 42190 train loss 15337.779296875 val loss 58095.2890625\n",
      "Epoch 42200 train loss 15356.9375 val loss 58110.25\n",
      "Epoch 42210 train loss 15334.4931640625 val loss 58107.8984375\n",
      "Epoch 42220 train loss 15329.962890625 val loss 58124.13671875\n",
      "Epoch 42230 train loss 15379.8681640625 val loss 58172.58203125\n",
      "Epoch 42240 train loss 15364.6396484375 val loss 58128.640625\n",
      "Epoch 42250 train loss 15331.169921875 val loss 58166.1953125\n",
      "Epoch 42260 train loss 15349.0107421875 val loss 58173.1328125\n",
      "Epoch 42270 train loss 15352.630859375 val loss 58151.9609375\n",
      "Epoch 42280 train loss 15329.3427734375 val loss 58179.93359375\n",
      "Epoch 42290 train loss 15337.0927734375 val loss 58215.67578125\n",
      "Epoch 42300 train loss 15330.3173828125 val loss 58178.4140625\n",
      "Epoch 42310 train loss 15333.2119140625 val loss 58173.47265625\n",
      "Epoch 42320 train loss 15333.2373046875 val loss 58198.34765625\n",
      "Epoch 42330 train loss 15361.2158203125 val loss 58234.17578125\n",
      "Epoch 42340 train loss 15324.513671875 val loss 58180.0078125\n",
      "Epoch 42350 train loss 15352.6826171875 val loss 58189.7265625\n",
      "Epoch 42360 train loss 15324.962890625 val loss 58227.58984375\n",
      "Epoch 42370 train loss 15345.1416015625 val loss 58249.46875\n",
      "Epoch 42380 train loss 15322.39453125 val loss 58257.2734375\n",
      "Epoch 42390 train loss 15355.83203125 val loss 58221.38671875\n",
      "Epoch 42400 train loss 15320.88671875 val loss 58223.66015625\n",
      "Epoch 42410 train loss 15347.62109375 val loss 58261.65625\n",
      "Epoch 42420 train loss 15320.763671875 val loss 58240.90234375\n",
      "Epoch 42430 train loss 15337.2607421875 val loss 58239.76953125\n",
      "Epoch 42440 train loss 15321.275390625 val loss 58276.30859375\n",
      "Epoch 42450 train loss 15334.5634765625 val loss 58315.54296875\n",
      "Epoch 42460 train loss 15324.5869140625 val loss 58306.0625\n",
      "Epoch 42470 train loss 15335.9443359375 val loss 58275.73046875\n",
      "Epoch 42480 train loss 15318.3818359375 val loss 58279.296875\n",
      "Epoch 42490 train loss 15329.421875 val loss 58321.9921875\n",
      "Epoch 42500 train loss 15331.078125 val loss 58331.93359375\n",
      "Epoch 42510 train loss 15316.1474609375 val loss 58326.4453125\n",
      "Epoch 42520 train loss 15373.271484375 val loss 58370.5859375\n",
      "Epoch 42530 train loss 15322.2783203125 val loss 58339.3515625\n",
      "Epoch 42540 train loss 15328.3212890625 val loss 58345.671875\n",
      "Epoch 42550 train loss 15356.2919921875 val loss 58390.328125\n",
      "Epoch 42560 train loss 15321.1728515625 val loss 58341.71484375\n",
      "Epoch 42570 train loss 15314.7490234375 val loss 58329.9609375\n",
      "Epoch 42580 train loss 15334.70703125 val loss 58372.8359375\n",
      "Epoch 42590 train loss 15316.6826171875 val loss 58349.640625\n",
      "Epoch 42600 train loss 15323.8056640625 val loss 58372.93359375\n",
      "Epoch 42610 train loss 15326.4873046875 val loss 58388.984375\n",
      "Epoch 42620 train loss 15311.2177734375 val loss 58409.640625\n",
      "Epoch 42630 train loss 15325.109375 val loss 58391.9921875\n",
      "Epoch 42640 train loss 15309.7431640625 val loss 58426.11328125\n",
      "Epoch 42650 train loss 15330.8271484375 val loss 58408.78125\n",
      "Epoch 42660 train loss 15308.9755859375 val loss 58396.29296875\n",
      "Epoch 42670 train loss 15306.7041015625 val loss 58423.2109375\n",
      "Epoch 42680 train loss 15341.03125 val loss 58446.83203125\n",
      "Epoch 42690 train loss 15307.109375 val loss 58418.7109375\n",
      "Epoch 42700 train loss 15308.3759765625 val loss 58456.71875\n",
      "Epoch 42710 train loss 15314.505859375 val loss 58489.03125\n",
      "Epoch 42720 train loss 15313.5654296875 val loss 58468.08984375\n",
      "Epoch 42730 train loss 15316.455078125 val loss 58463.9296875\n",
      "Epoch 42740 train loss 15310.2998046875 val loss 58481.5078125\n",
      "Epoch 42750 train loss 15321.9189453125 val loss 58510.9765625\n",
      "Epoch 42760 train loss 15307.5732421875 val loss 58495.921875\n",
      "Epoch 42770 train loss 15326.7412109375 val loss 58465.8515625\n",
      "Epoch 42780 train loss 15301.6123046875 val loss 58506.56640625\n",
      "Epoch 42790 train loss 15336.0986328125 val loss 58543.2578125\n",
      "Epoch 42800 train loss 15305.634765625 val loss 58501.37890625\n",
      "Epoch 42810 train loss 15304.3173828125 val loss 58514.55859375\n",
      "Epoch 42820 train loss 15310.794921875 val loss 58541.546875\n",
      "Epoch 42830 train loss 15304.73828125 val loss 58554.5859375\n",
      "Epoch 42840 train loss 15316.4052734375 val loss 58549.43359375\n",
      "Epoch 42850 train loss 15302.263671875 val loss 58575.9609375\n",
      "Epoch 42860 train loss 15296.845703125 val loss 58586.76171875\n",
      "Epoch 42870 train loss 15345.0517578125 val loss 58545.25390625\n",
      "Epoch 42880 train loss 15322.94921875 val loss 58622.58984375\n",
      "Epoch 42890 train loss 15296.349609375 val loss 58598.83203125\n",
      "Epoch 42900 train loss 15313.0771484375 val loss 58569.6015625\n",
      "Epoch 42910 train loss 15296.4677734375 val loss 58601.15234375\n",
      "Epoch 42920 train loss 15321.5751953125 val loss 58591.78515625\n",
      "Epoch 42930 train loss 15292.7119140625 val loss 58607.13671875\n",
      "Epoch 42940 train loss 15335.373046875 val loss 58613.05078125\n",
      "Epoch 42950 train loss 15299.701171875 val loss 58615.76953125\n",
      "Epoch 42960 train loss 15304.791015625 val loss 58620.171875\n",
      "Epoch 42970 train loss 15292.3291015625 val loss 58645.23046875\n",
      "Epoch 42980 train loss 15338.056640625 val loss 58634.0625\n",
      "Epoch 42990 train loss 15299.1083984375 val loss 58675.26171875\n",
      "Epoch 43000 train loss 15299.6953125 val loss 58697.71484375\n",
      "Epoch 43010 train loss 15290.341796875 val loss 58700.91015625\n",
      "Epoch 43020 train loss 15322.5283203125 val loss 58687.296875\n",
      "Epoch 43030 train loss 15325.1572265625 val loss 58729.421875\n",
      "Epoch 43040 train loss 15290.76171875 val loss 58710.30078125\n",
      "Epoch 43050 train loss 15295.7705078125 val loss 58724.13671875\n",
      "Epoch 43060 train loss 15288.654296875 val loss 58710.83203125\n",
      "Epoch 43070 train loss 15313.00390625 val loss 58708.65625\n",
      "Epoch 43080 train loss 15288.6513671875 val loss 58742.75\n",
      "Epoch 43090 train loss 15306.0869140625 val loss 58761.26953125\n",
      "Epoch 43100 train loss 15301.4501953125 val loss 58762.07421875\n",
      "Epoch 43110 train loss 15283.484375 val loss 58751.15625\n",
      "Epoch 43120 train loss 15292.201171875 val loss 58789.08984375\n",
      "Epoch 43130 train loss 15301.8544921875 val loss 58812.4921875\n",
      "Epoch 43140 train loss 15281.458984375 val loss 58812.30078125\n",
      "Epoch 43150 train loss 15309.818359375 val loss 58764.4140625\n",
      "Epoch 43160 train loss 15281.5986328125 val loss 58790.59765625\n",
      "Epoch 43170 train loss 15301.19921875 val loss 58811.65234375\n",
      "Epoch 43180 train loss 15283.080078125 val loss 58830.09765625\n",
      "Epoch 43190 train loss 15297.158203125 val loss 58836.81640625\n",
      "Epoch 43200 train loss 15286.06640625 val loss 58838.19140625\n",
      "Epoch 43210 train loss 15277.888671875 val loss 58854.49609375\n",
      "Epoch 43220 train loss 15294.876953125 val loss 58866.890625\n",
      "Epoch 43230 train loss 15300.419921875 val loss 58871.86328125\n",
      "Epoch 43240 train loss 15321.2119140625 val loss 58843.859375\n",
      "Epoch 43250 train loss 15283.3271484375 val loss 58887.65234375\n",
      "Epoch 43260 train loss 15278.4755859375 val loss 58875.1953125\n",
      "Epoch 43270 train loss 15282.5390625 val loss 58882.68359375\n",
      "Epoch 43280 train loss 15295.57421875 val loss 58874.58203125\n",
      "Epoch 43290 train loss 15287.2841796875 val loss 58939.59765625\n",
      "Epoch 43300 train loss 15273.615234375 val loss 58935.1796875\n",
      "Epoch 43310 train loss 15307.716796875 val loss 58923.796875\n",
      "Epoch 43320 train loss 15272.0224609375 val loss 58942.11328125\n",
      "Epoch 43330 train loss 15285.966796875 val loss 58972.13671875\n",
      "Epoch 43340 train loss 15283.2314453125 val loss 58980.42578125\n",
      "Epoch 43350 train loss 15270.3935546875 val loss 58981.83984375\n",
      "Epoch 43360 train loss 15299.484375 val loss 59003.203125\n",
      "Epoch 43370 train loss 15268.6005859375 val loss 58983.78515625\n",
      "Epoch 43380 train loss 15286.337890625 val loss 58978.5078125\n",
      "Epoch 43390 train loss 15297.1240234375 val loss 59041.01171875\n",
      "Epoch 43400 train loss 15266.9013671875 val loss 58988.640625\n",
      "Epoch 43410 train loss 15289.287109375 val loss 58974.22265625\n",
      "Epoch 43420 train loss 15276.263671875 val loss 59043.20703125\n",
      "Epoch 43430 train loss 15280.779296875 val loss 59058.31640625\n",
      "Epoch 43440 train loss 15266.0546875 val loss 59056.4609375\n",
      "Epoch 43450 train loss 15296.125 val loss 59073.4609375\n",
      "Epoch 43460 train loss 15265.5302734375 val loss 59059.65234375\n",
      "Epoch 43470 train loss 15304.3935546875 val loss 59056.81640625\n",
      "Epoch 43480 train loss 15263.4423828125 val loss 59104.88671875\n",
      "Epoch 43490 train loss 15279.26953125 val loss 59107.2734375\n",
      "Epoch 43500 train loss 15275.890625 val loss 59095.59375\n",
      "Epoch 43510 train loss 15272.8671875 val loss 59098.984375\n",
      "Epoch 43520 train loss 15273.5712890625 val loss 59106.39453125\n",
      "Epoch 43530 train loss 15267.138671875 val loss 59133.6953125\n",
      "Epoch 43540 train loss 15270.93359375 val loss 59141.96484375\n",
      "Epoch 43550 train loss 15269.751953125 val loss 59133.34765625\n",
      "Epoch 43560 train loss 15271.4423828125 val loss 59157.484375\n",
      "Epoch 43570 train loss 15274.4404296875 val loss 59211.1171875\n",
      "Epoch 43580 train loss 15258.40625 val loss 59149.34765625\n",
      "Epoch 43590 train loss 15287.6982421875 val loss 59175.3671875\n",
      "Epoch 43600 train loss 15287.0771484375 val loss 59210.625\n",
      "Epoch 43610 train loss 15258.2353515625 val loss 59179.2109375\n",
      "Epoch 43620 train loss 15267.1923828125 val loss 59191.7890625\n",
      "Epoch 43630 train loss 15263.919921875 val loss 59253.9765625\n",
      "Epoch 43640 train loss 15262.3037109375 val loss 59249.21484375\n",
      "Epoch 43650 train loss 15282.5087890625 val loss 59213.3359375\n",
      "Epoch 43660 train loss 15253.73828125 val loss 59262.59765625\n",
      "Epoch 43670 train loss 15274.3515625 val loss 59296.6796875\n",
      "Epoch 43680 train loss 15256.8857421875 val loss 59304.17578125\n",
      "Epoch 43690 train loss 15266.4833984375 val loss 59244.7578125\n",
      "Epoch 43700 train loss 15258.1259765625 val loss 59275.2265625\n",
      "Epoch 43710 train loss 15269.9287109375 val loss 59325.48828125\n",
      "Epoch 43720 train loss 15254.6484375 val loss 59319.625\n",
      "Epoch 43730 train loss 15264.076171875 val loss 59338.4296875\n",
      "Epoch 43740 train loss 15255.8603515625 val loss 59343.86328125\n",
      "Epoch 43750 train loss 15279.3388671875 val loss 59336.765625\n",
      "Epoch 43760 train loss 15251.931640625 val loss 59326.32421875\n",
      "Epoch 43770 train loss 15247.4501953125 val loss 59350.49609375\n",
      "Epoch 43780 train loss 15295.080078125 val loss 59377.5234375\n",
      "Epoch 43790 train loss 15253.962890625 val loss 59361.55859375\n",
      "Epoch 43800 train loss 15247.9814453125 val loss 59382.5546875\n",
      "Epoch 43810 train loss 15272.8125 val loss 59406.72265625\n",
      "Epoch 43820 train loss 15245.05859375 val loss 59384.0234375\n",
      "Epoch 43830 train loss 15274.5263671875 val loss 59415.21484375\n",
      "Epoch 43840 train loss 15246.32421875 val loss 59448.9453125\n",
      "Epoch 43850 train loss 15250.5693359375 val loss 59429.5859375\n",
      "Epoch 43860 train loss 15265.29296875 val loss 59407.0859375\n",
      "Epoch 43870 train loss 15240.3544921875 val loss 59427.80078125\n",
      "Epoch 43880 train loss 15258.6611328125 val loss 59453.06640625\n",
      "Epoch 43890 train loss 15271.2265625 val loss 59460.20703125\n",
      "Epoch 43900 train loss 15257.1845703125 val loss 59512.5859375\n",
      "Epoch 43910 train loss 15239.25390625 val loss 59493.4296875\n",
      "Epoch 43920 train loss 15245.1767578125 val loss 59496.69921875\n",
      "Epoch 43930 train loss 15252.0654296875 val loss 59493.19140625\n",
      "Epoch 43940 train loss 15235.3701171875 val loss 59505.46875\n",
      "Epoch 43950 train loss 15277.513671875 val loss 59563.96484375\n",
      "Epoch 43960 train loss 15237.6708984375 val loss 59541.6953125\n",
      "Epoch 43970 train loss 15249.6005859375 val loss 59536.19921875\n",
      "Epoch 43980 train loss 15235.0537109375 val loss 59600.7734375\n",
      "Epoch 43990 train loss 15255.5771484375 val loss 59611.2734375\n",
      "Epoch 44000 train loss 15239.326171875 val loss 59598.18359375\n",
      "Epoch 44010 train loss 15247.2197265625 val loss 59594.59765625\n",
      "Epoch 44020 train loss 15231.2236328125 val loss 59595.44140625\n",
      "Epoch 44030 train loss 15266.1591796875 val loss 59627.45703125\n",
      "Epoch 44040 train loss 15230.26953125 val loss 59608.8359375\n",
      "Epoch 44050 train loss 15237.607421875 val loss 59621.9296875\n",
      "Epoch 44060 train loss 15249.1845703125 val loss 59645.05078125\n",
      "Epoch 44070 train loss 15231.140625 val loss 59683.89453125\n",
      "Epoch 44080 train loss 15260.6162109375 val loss 59728.58203125\n",
      "Epoch 44090 train loss 15226.75390625 val loss 59686.63671875\n",
      "Epoch 44100 train loss 15243.9677734375 val loss 59662.640625\n",
      "Epoch 44110 train loss 15227.3466796875 val loss 59743.31640625\n",
      "Epoch 44120 train loss 15248.064453125 val loss 59730.82421875\n",
      "Epoch 44130 train loss 15225.880859375 val loss 59746.05859375\n",
      "Epoch 44140 train loss 15259.1162109375 val loss 59738.234375\n",
      "Epoch 44150 train loss 15279.9755859375 val loss 59803.015625\n",
      "Epoch 44160 train loss 15256.515625 val loss 59750.9296875\n",
      "Epoch 44170 train loss 15227.5712890625 val loss 59837.30078125\n",
      "Epoch 44180 train loss 15229.1640625 val loss 59793.22265625\n",
      "Epoch 44190 train loss 15233.029296875 val loss 59803.0703125\n",
      "Epoch 44200 train loss 15235.419921875 val loss 59782.1875\n",
      "Epoch 44210 train loss 15223.1640625 val loss 59807.73046875\n",
      "Epoch 44220 train loss 15233.4169921875 val loss 59831.890625\n",
      "Epoch 44230 train loss 15220.1689453125 val loss 59839.08984375\n",
      "Epoch 44240 train loss 15254.015625 val loss 59879.140625\n",
      "Epoch 44250 train loss 15219.9736328125 val loss 59863.921875\n",
      "Epoch 44260 train loss 15222.7607421875 val loss 59865.171875\n",
      "Epoch 44270 train loss 15241.708984375 val loss 59917.53515625\n",
      "Epoch 44280 train loss 15217.4580078125 val loss 59909.4921875\n",
      "Epoch 44290 train loss 15226.3525390625 val loss 59881.3671875\n",
      "Epoch 44300 train loss 15227.2880859375 val loss 59887.328125\n",
      "Epoch 44310 train loss 15243.6806640625 val loss 59961.6953125\n",
      "Epoch 44320 train loss 15213.2958984375 val loss 59913.70703125\n",
      "Epoch 44330 train loss 15233.326171875 val loss 59918.58984375\n",
      "Epoch 44340 train loss 15219.8388671875 val loss 59984.796875\n",
      "Epoch 44350 train loss 15218.826171875 val loss 59980.83984375\n",
      "Epoch 44360 train loss 15222.16796875 val loss 59983.92578125\n",
      "Epoch 44370 train loss 15217.4140625 val loss 59970.73046875\n",
      "Epoch 44380 train loss 15232.45703125 val loss 60010.23828125\n",
      "Epoch 44390 train loss 15209.8974609375 val loss 60003.8828125\n",
      "Epoch 44400 train loss 15207.59375 val loss 60001.38671875\n",
      "Epoch 44410 train loss 15244.560546875 val loss 60041.609375\n",
      "Epoch 44420 train loss 15212.126953125 val loss 60027.015625\n",
      "Epoch 44430 train loss 15209.3251953125 val loss 60038.3203125\n",
      "Epoch 44440 train loss 15225.728515625 val loss 60060.44921875\n",
      "Epoch 44450 train loss 15207.7353515625 val loss 60038.984375\n",
      "Epoch 44460 train loss 15224.3857421875 val loss 60038.64453125\n",
      "Epoch 44470 train loss 15210.361328125 val loss 60071.3671875\n",
      "Epoch 44480 train loss 15231.7587890625 val loss 60097.47265625\n",
      "Epoch 44490 train loss 15202.4951171875 val loss 60081.19140625\n",
      "Epoch 44500 train loss 15222.6552734375 val loss 60096.8984375\n",
      "Epoch 44510 train loss 15201.8642578125 val loss 60146.0859375\n",
      "Epoch 44520 train loss 15229.775390625 val loss 60152.796875\n",
      "Epoch 44530 train loss 15201.9482421875 val loss 60130.1328125\n",
      "Epoch 44540 train loss 15226.7919921875 val loss 60130.5546875\n",
      "Epoch 44550 train loss 15223.26171875 val loss 60207.27734375\n",
      "Epoch 44560 train loss 15199.099609375 val loss 60117.0390625\n",
      "Epoch 44570 train loss 15221.4462890625 val loss 60101.67578125\n",
      "Epoch 44580 train loss 15196.6142578125 val loss 60143.8671875\n",
      "Epoch 44590 train loss 15239.001953125 val loss 60164.84375\n",
      "Epoch 44600 train loss 15197.265625 val loss 60149.61328125\n",
      "Epoch 44610 train loss 15215.828125 val loss 60165.4375\n",
      "Epoch 44620 train loss 15194.4091796875 val loss 60183.734375\n",
      "Epoch 44630 train loss 15224.0576171875 val loss 60214.2421875\n",
      "Epoch 44640 train loss 15195.3349609375 val loss 60205.60546875\n",
      "Epoch 44650 train loss 15210.3671875 val loss 60224.765625\n",
      "Epoch 44660 train loss 15201.3017578125 val loss 60227.109375\n",
      "Epoch 44670 train loss 15196.732421875 val loss 60240.90234375\n",
      "Epoch 44680 train loss 15190.982421875 val loss 60278.96484375\n",
      "Epoch 44690 train loss 15248.1337890625 val loss 60265.0859375\n",
      "Epoch 44700 train loss 15205.7158203125 val loss 60346.28515625\n",
      "Epoch 44710 train loss 15188.47265625 val loss 60312.2421875\n",
      "Epoch 44720 train loss 15195.19921875 val loss 60280.93359375\n",
      "Epoch 44730 train loss 15216.005859375 val loss 60304.01953125\n",
      "Epoch 44740 train loss 15186.732421875 val loss 60291.24609375\n",
      "Epoch 44750 train loss 15196.3076171875 val loss 60324.21875\n",
      "Epoch 44760 train loss 15185.7001953125 val loss 60344.828125\n",
      "Epoch 44770 train loss 15203.0625 val loss 60354.28515625\n",
      "Epoch 44780 train loss 15197.8466796875 val loss 60325.12890625\n",
      "Epoch 44790 train loss 15182.380859375 val loss 60348.109375\n",
      "Epoch 44800 train loss 15205.201171875 val loss 60410.06640625\n",
      "Epoch 44810 train loss 15189.90234375 val loss 60399.21875\n",
      "Epoch 44820 train loss 15181.521484375 val loss 60395.03515625\n",
      "Epoch 44830 train loss 15209.98046875 val loss 60416.37890625\n",
      "Epoch 44840 train loss 15182.7783203125 val loss 60384.1171875\n",
      "Epoch 44850 train loss 15197.66796875 val loss 60398.484375\n",
      "Epoch 44860 train loss 15179.2626953125 val loss 60413.89453125\n",
      "Epoch 44870 train loss 15216.81640625 val loss 60436.3359375\n",
      "Epoch 44880 train loss 15180.859375 val loss 60429.26953125\n",
      "Epoch 44890 train loss 15202.5224609375 val loss 60448.21875\n",
      "Epoch 44900 train loss 15212.7138671875 val loss 60477.18359375\n",
      "Epoch 44910 train loss 15183.521484375 val loss 60482.421875\n",
      "Epoch 44920 train loss 15175.658203125 val loss 60479.63671875\n",
      "Epoch 44930 train loss 15189.4482421875 val loss 60497.28515625\n",
      "Epoch 44940 train loss 15177.1513671875 val loss 60505.1640625\n",
      "Epoch 44950 train loss 15173.912109375 val loss 60490.1796875\n",
      "Epoch 44960 train loss 15203.6337890625 val loss 60478.890625\n",
      "Epoch 44970 train loss 15218.767578125 val loss 60569.49609375\n",
      "Epoch 44980 train loss 15185.396484375 val loss 60514.1328125\n",
      "Epoch 44990 train loss 15170.685546875 val loss 60524.05859375\n",
      "Epoch 45000 train loss 15183.8310546875 val loss 60534.93359375\n",
      "Epoch 45010 train loss 15178.865234375 val loss 60528.23046875\n",
      "Epoch 45020 train loss 15177.4541015625 val loss 60534.91015625\n",
      "Epoch 45030 train loss 15195.6875 val loss 60536.9921875\n",
      "Epoch 45040 train loss 15168.9892578125 val loss 60569.2421875\n",
      "Epoch 45050 train loss 15182.6796875 val loss 60605.62890625\n",
      "Epoch 45060 train loss 15176.1220703125 val loss 60623.90625\n",
      "Epoch 45070 train loss 15178.2861328125 val loss 60610.45703125\n",
      "Epoch 45080 train loss 15170.5673828125 val loss 60608.6875\n",
      "Epoch 45090 train loss 15177.5634765625 val loss 60646.8515625\n",
      "Epoch 45100 train loss 15187.3583984375 val loss 60620.21875\n",
      "Epoch 45110 train loss 15172.140625 val loss 60608.80859375\n",
      "Epoch 45120 train loss 15168.9091796875 val loss 60639.16015625\n",
      "Epoch 45130 train loss 15209.2158203125 val loss 60670.98828125\n",
      "Epoch 45140 train loss 15187.927734375 val loss 60644.23046875\n",
      "Epoch 45150 train loss 15161.7685546875 val loss 60681.3984375\n",
      "Epoch 45160 train loss 15166.3984375 val loss 60699.28125\n",
      "Epoch 45170 train loss 15169.625 val loss 60693.9296875\n",
      "Epoch 45180 train loss 15182.70703125 val loss 60635.328125\n",
      "Epoch 45190 train loss 15160.6650390625 val loss 60688.0390625\n",
      "Epoch 45200 train loss 15162.3310546875 val loss 60691.47265625\n",
      "Epoch 45210 train loss 15191.8603515625 val loss 60676.32421875\n",
      "Epoch 45220 train loss 15161.830078125 val loss 60749.4140625\n",
      "Epoch 45230 train loss 15184.21875 val loss 60768.03125\n",
      "Epoch 45240 train loss 15181.16796875 val loss 60718.35546875\n",
      "Epoch 45250 train loss 15164.39453125 val loss 60752.1640625\n",
      "Epoch 45260 train loss 15156.8623046875 val loss 60762.42578125\n",
      "Epoch 45270 train loss 15192.8427734375 val loss 60775.98828125\n",
      "Epoch 45280 train loss 15156.5966796875 val loss 60777.66796875\n",
      "Epoch 45290 train loss 15165.27734375 val loss 60785.46875\n",
      "Epoch 45300 train loss 15155.771484375 val loss 60777.78515625\n",
      "Epoch 45310 train loss 15165.43359375 val loss 60781.17578125\n",
      "Epoch 45320 train loss 15194.658203125 val loss 60825.25390625\n",
      "Epoch 45330 train loss 15165.685546875 val loss 60806.171875\n",
      "Epoch 45340 train loss 15150.90625 val loss 60844.8359375\n",
      "Epoch 45350 train loss 15166.60546875 val loss 60850.203125\n",
      "Epoch 45360 train loss 15150.48046875 val loss 60858.62890625\n",
      "Epoch 45370 train loss 15153.16796875 val loss 60855.47265625\n",
      "Epoch 45380 train loss 15157.64453125 val loss 60848.25390625\n",
      "Epoch 45390 train loss 15190.0947265625 val loss 60841.55859375\n",
      "Epoch 45400 train loss 15163.248046875 val loss 60886.5390625\n",
      "Epoch 45410 train loss 15145.63671875 val loss 60896.4453125\n",
      "Epoch 45420 train loss 15177.98046875 val loss 60859.16015625\n",
      "Epoch 45430 train loss 15149.224609375 val loss 60949.26171875\n",
      "Epoch 45440 train loss 15151.734375 val loss 60899.00390625\n",
      "Epoch 45450 train loss 15169.853515625 val loss 60890.26171875\n",
      "Epoch 45460 train loss 15143.9130859375 val loss 60927.8203125\n",
      "Epoch 45470 train loss 15168.6142578125 val loss 60934.7421875\n",
      "Epoch 45480 train loss 15142.767578125 val loss 60926.30859375\n",
      "Epoch 45490 train loss 15147.2890625 val loss 60918.23046875\n",
      "Epoch 45500 train loss 15177.5771484375 val loss 60915.765625\n",
      "Epoch 45510 train loss 15153.6201171875 val loss 60977.078125\n",
      "Epoch 45520 train loss 15151.123046875 val loss 60988.078125\n",
      "Epoch 45530 train loss 15153.1201171875 val loss 60991.328125\n",
      "Epoch 45540 train loss 15140.236328125 val loss 60994.09375\n",
      "Epoch 45550 train loss 15144.19921875 val loss 60996.87890625\n",
      "Epoch 45560 train loss 15164.28515625 val loss 60994.91796875\n",
      "Epoch 45570 train loss 15154.490234375 val loss 61044.27734375\n",
      "Epoch 45580 train loss 15143.74609375 val loss 61032.26953125\n",
      "Epoch 45590 train loss 15137.3994140625 val loss 61036.02734375\n",
      "Epoch 45600 train loss 15165.58203125 val loss 61049.33984375\n",
      "Epoch 45610 train loss 15139.794921875 val loss 61029.69921875\n",
      "Epoch 45620 train loss 15144.5556640625 val loss 61003.828125\n",
      "Epoch 45630 train loss 15163.361328125 val loss 61025.2109375\n",
      "Epoch 45640 train loss 15141.923828125 val loss 61001.578125\n",
      "Epoch 45650 train loss 15135.8046875 val loss 61020.01953125\n",
      "Epoch 45660 train loss 15152.6806640625 val loss 61045.62109375\n",
      "Epoch 45670 train loss 15132.8193359375 val loss 61035.9609375\n",
      "Epoch 45680 train loss 15163.490234375 val loss 61024.74609375\n",
      "Epoch 45690 train loss 15139.1953125 val loss 61081.3359375\n",
      "Epoch 45700 train loss 15133.7578125 val loss 61042.29296875\n",
      "Epoch 45710 train loss 15173.484375 val loss 61028.19140625\n",
      "Epoch 45720 train loss 15162.451171875 val loss 61084.1015625\n",
      "Epoch 45730 train loss 15131.89453125 val loss 61079.765625\n",
      "Epoch 45740 train loss 15147.0107421875 val loss 61078.26171875\n",
      "Epoch 45750 train loss 15129.9443359375 val loss 61115.73828125\n",
      "Epoch 45760 train loss 15168.9375 val loss 61114.23828125\n",
      "Epoch 45770 train loss 15145.880859375 val loss 61107.3671875\n",
      "Epoch 45780 train loss 15126.9150390625 val loss 61149.265625\n",
      "Epoch 45790 train loss 15150.451171875 val loss 61149.7109375\n",
      "Epoch 45800 train loss 15134.373046875 val loss 61134.171875\n",
      "Epoch 45810 train loss 15133.0908203125 val loss 61156.51953125\n",
      "Epoch 45820 train loss 15154.1943359375 val loss 61181.01953125\n",
      "Epoch 45830 train loss 15137.5986328125 val loss 61183.109375\n",
      "Epoch 45840 train loss 15124.7919921875 val loss 61196.31640625\n",
      "Epoch 45850 train loss 15139.0087890625 val loss 61215.078125\n",
      "Epoch 45860 train loss 15123.4443359375 val loss 61215.43359375\n",
      "Epoch 45870 train loss 15140.8193359375 val loss 61197.86328125\n",
      "Epoch 45880 train loss 15123.2001953125 val loss 61205.265625\n",
      "Epoch 45890 train loss 15165.3427734375 val loss 61247.41015625\n",
      "Epoch 45900 train loss 15143.80078125 val loss 61214.5390625\n",
      "Epoch 45910 train loss 15121.6552734375 val loss 61249.61328125\n",
      "Epoch 45920 train loss 15143.5546875 val loss 61247.96875\n",
      "Epoch 45930 train loss 15122.5712890625 val loss 61223.9921875\n",
      "Epoch 45940 train loss 15135.580078125 val loss 61251.234375\n",
      "Epoch 45950 train loss 15135.3134765625 val loss 61301.296875\n",
      "Epoch 45960 train loss 15123.2880859375 val loss 61307.1875\n",
      "Epoch 45970 train loss 15119.296875 val loss 61264.00390625\n",
      "Epoch 45980 train loss 15126.3271484375 val loss 61279.01171875\n",
      "Epoch 45990 train loss 15130.369140625 val loss 61275.03125\n",
      "Epoch 46000 train loss 15119.46875 val loss 61267.2890625\n",
      "Epoch 46010 train loss 15119.2333984375 val loss 61280.79296875\n",
      "Epoch 46020 train loss 15129.775390625 val loss 61342.18359375\n",
      "Epoch 46030 train loss 15131.306640625 val loss 61323.06640625\n",
      "Epoch 46040 train loss 15113.91015625 val loss 61312.10546875\n",
      "Epoch 46050 train loss 15146.7099609375 val loss 61302.61328125\n",
      "Epoch 46060 train loss 15133.84765625 val loss 61365.34765625\n",
      "Epoch 46070 train loss 15110.6943359375 val loss 61357.0390625\n",
      "Epoch 46080 train loss 15137.955078125 val loss 61348.05078125\n",
      "Epoch 46090 train loss 15112.060546875 val loss 61348.34375\n",
      "Epoch 46100 train loss 15129.77734375 val loss 61386.3984375\n",
      "Epoch 46110 train loss 15111.54296875 val loss 61398.33984375\n",
      "Epoch 46120 train loss 15131.4384765625 val loss 61351.3203125\n",
      "Epoch 46130 train loss 15109.1787109375 val loss 61354.66796875\n",
      "Epoch 46140 train loss 15152.0908203125 val loss 61329.06640625\n",
      "Epoch 46150 train loss 15107.6865234375 val loss 61361.46484375\n",
      "Epoch 46160 train loss 15137.791015625 val loss 61398.62109375\n",
      "Epoch 46170 train loss 15105.693359375 val loss 61396.50390625\n",
      "Epoch 46180 train loss 15165.080078125 val loss 61358.390625\n",
      "Epoch 46190 train loss 15136.0859375 val loss 61420.96484375\n",
      "Epoch 46200 train loss 15144.1943359375 val loss 61408.046875\n",
      "Epoch 46210 train loss 15111.359375 val loss 61442.36328125\n",
      "Epoch 46220 train loss 15109.1953125 val loss 61418.359375\n",
      "Epoch 46230 train loss 15120.1552734375 val loss 61401.83203125\n",
      "Epoch 46240 train loss 15105.248046875 val loss 61452.24609375\n",
      "Epoch 46250 train loss 15147.0859375 val loss 61482.9921875\n",
      "Epoch 46260 train loss 15110.3740234375 val loss 61458.0546875\n",
      "Epoch 46270 train loss 15106.982421875 val loss 61451.68359375\n",
      "Epoch 46280 train loss 15129.107421875 val loss 61497.42578125\n",
      "Epoch 46290 train loss 15105.3427734375 val loss 61469.69140625\n",
      "Epoch 46300 train loss 15105.8515625 val loss 61488.18359375\n",
      "Epoch 46310 train loss 15099.15234375 val loss 61496.33203125\n",
      "Epoch 46320 train loss 15138.6640625 val loss 61510.71484375\n",
      "Epoch 46330 train loss 15111.62109375 val loss 61571.390625\n",
      "Epoch 46340 train loss 15099.1181640625 val loss 61501.859375\n",
      "Epoch 46350 train loss 15113.60546875 val loss 61478.234375\n",
      "Epoch 46360 train loss 15095.990234375 val loss 61507.37890625\n",
      "Epoch 46370 train loss 15145.4677734375 val loss 61469.078125\n",
      "Epoch 46380 train loss 15095.6552734375 val loss 61518.05859375\n",
      "Epoch 46390 train loss 15127.3505859375 val loss 61531.18359375\n",
      "Epoch 46400 train loss 15094.54296875 val loss 61539.453125\n",
      "Epoch 46410 train loss 15139.1865234375 val loss 61494.34765625\n",
      "Epoch 46420 train loss 15095.62109375 val loss 61563.8046875\n",
      "Epoch 46430 train loss 15107.076171875 val loss 61565.828125\n",
      "Epoch 46440 train loss 15120.244140625 val loss 61550.35546875\n",
      "Epoch 46450 train loss 15094.9873046875 val loss 61592.53515625\n",
      "Epoch 46460 train loss 15093.740234375 val loss 61600.38671875\n",
      "Epoch 46470 train loss 15101.1728515625 val loss 61564.06640625\n",
      "Epoch 46480 train loss 15115.76953125 val loss 61617.2890625\n",
      "Epoch 46490 train loss 15090.96875 val loss 61621.62109375\n",
      "Epoch 46500 train loss 15099.05078125 val loss 61616.29296875\n",
      "Epoch 46510 train loss 15111.37890625 val loss 61593.2578125\n",
      "Epoch 46520 train loss 15095.1826171875 val loss 61646.49609375\n",
      "Epoch 46530 train loss 15095.560546875 val loss 61675.11328125\n",
      "Epoch 46540 train loss 15096.4296875 val loss 61678.203125\n",
      "Epoch 46550 train loss 15112.62109375 val loss 61666.4921875\n",
      "Epoch 46560 train loss 15095.0224609375 val loss 61670.88671875\n",
      "Epoch 46570 train loss 15113.0595703125 val loss 61668.9609375\n",
      "Epoch 46580 train loss 15085.28515625 val loss 61709.9609375\n",
      "Epoch 46590 train loss 15105.4306640625 val loss 61753.93359375\n",
      "Epoch 46600 train loss 15105.7880859375 val loss 61681.08984375\n",
      "Epoch 46610 train loss 15084.017578125 val loss 61675.44921875\n",
      "Epoch 46620 train loss 15107.2373046875 val loss 61676.53515625\n",
      "Epoch 46630 train loss 15082.072265625 val loss 61704.04296875\n",
      "Epoch 46640 train loss 15113.013671875 val loss 61685.2421875\n",
      "Epoch 46650 train loss 15083.1728515625 val loss 61700.75390625\n",
      "Epoch 46660 train loss 15110.0537109375 val loss 61715.58984375\n",
      "Epoch 46670 train loss 15080.2275390625 val loss 61741.8203125\n",
      "Epoch 46680 train loss 15099.5625 val loss 61744.16015625\n",
      "Epoch 46690 train loss 15122.615234375 val loss 61747.46875\n",
      "Epoch 46700 train loss 15092.8828125 val loss 61780.30859375\n",
      "Epoch 46710 train loss 15079.9140625 val loss 61772.390625\n",
      "Epoch 46720 train loss 15084.2666015625 val loss 61778.8046875\n",
      "Epoch 46730 train loss 15084.115234375 val loss 61781.01171875\n",
      "Epoch 46740 train loss 15080.3935546875 val loss 61808.44140625\n",
      "Epoch 46750 train loss 15105.412109375 val loss 61832.3671875\n",
      "Epoch 46760 train loss 15076.9306640625 val loss 61840.41796875\n",
      "Epoch 46770 train loss 15087.572265625 val loss 61852.9296875\n",
      "Epoch 46780 train loss 15074.896484375 val loss 61857.96875\n",
      "Epoch 46790 train loss 15081.9755859375 val loss 61837.49609375\n",
      "Epoch 46800 train loss 15086.865234375 val loss 61850.2578125\n",
      "Epoch 46810 train loss 15101.8232421875 val loss 61864.19140625\n",
      "Epoch 46820 train loss 15073.072265625 val loss 61814.4375\n",
      "Epoch 46830 train loss 15108.3056640625 val loss 61807.07421875\n",
      "Epoch 46840 train loss 15084.783203125 val loss 61854.6640625\n",
      "Epoch 46850 train loss 15070.8349609375 val loss 61841.8515625\n",
      "Epoch 46860 train loss 15089.484375 val loss 61843.04296875\n",
      "Epoch 46870 train loss 15092.7890625 val loss 61871.53125\n",
      "Epoch 46880 train loss 15082.2578125 val loss 61840.328125\n",
      "Epoch 46890 train loss 15075.8779296875 val loss 61886.77734375\n",
      "Epoch 46900 train loss 15090.2744140625 val loss 61878.125\n",
      "Epoch 46910 train loss 15068.052734375 val loss 61915.5\n",
      "Epoch 46920 train loss 15103.171875 val loss 61950.5546875\n",
      "Epoch 46930 train loss 15068.1748046875 val loss 61934.3203125\n",
      "Epoch 46940 train loss 15081.490234375 val loss 61918.74609375\n",
      "Epoch 46950 train loss 15111.501953125 val loss 61964.7578125\n",
      "Epoch 46960 train loss 15082.1142578125 val loss 61943.640625\n",
      "Epoch 46970 train loss 15068.841796875 val loss 61969.2421875\n",
      "Epoch 46980 train loss 15065.671875 val loss 61962.484375\n",
      "Epoch 46990 train loss 15106.63671875 val loss 61991.0859375\n",
      "Epoch 47000 train loss 15081.59375 val loss 61981.453125\n",
      "Epoch 47010 train loss 15065.2578125 val loss 62005.19140625\n",
      "Epoch 47020 train loss 15077.119140625 val loss 61965.0703125\n",
      "Epoch 47030 train loss 15068.5546875 val loss 62005.11328125\n",
      "Epoch 47040 train loss 15061.560546875 val loss 62051.734375\n",
      "Epoch 47050 train loss 15095.978515625 val loss 62062.7890625\n",
      "Epoch 47060 train loss 15061.5771484375 val loss 62050.3359375\n",
      "Epoch 47070 train loss 15099.703125 val loss 62006.296875\n",
      "Epoch 47080 train loss 15062.359375 val loss 62043.60546875\n",
      "Epoch 47090 train loss 15107.1357421875 val loss 62040.3203125\n",
      "Epoch 47100 train loss 15069.37890625 val loss 62019.97265625\n",
      "Epoch 47110 train loss 15068.95703125 val loss 62054.75390625\n",
      "Epoch 47120 train loss 15061.0244140625 val loss 62063.06640625\n",
      "Epoch 47130 train loss 15105.841796875 val loss 62084.19921875\n",
      "Epoch 47140 train loss 15057.189453125 val loss 62065.43359375\n",
      "Epoch 47150 train loss 15105.0478515625 val loss 62037.6484375\n",
      "Epoch 47160 train loss 15057.5205078125 val loss 62107.1953125\n",
      "Epoch 47170 train loss 15083.126953125 val loss 62120.046875\n",
      "Epoch 47180 train loss 15061.81640625 val loss 62088.671875\n",
      "Epoch 47190 train loss 15055.392578125 val loss 62113.36328125\n",
      "Epoch 47200 train loss 15085.0107421875 val loss 62124.36328125\n",
      "Epoch 47210 train loss 15053.888671875 val loss 62110.40625\n",
      "Epoch 47220 train loss 15074.921875 val loss 62114.18359375\n",
      "Epoch 47230 train loss 15087.2880859375 val loss 62120.875\n",
      "Epoch 47240 train loss 15054.623046875 val loss 62158.9453125\n",
      "Epoch 47250 train loss 15061.53515625 val loss 62174.015625\n",
      "Epoch 47260 train loss 15085.6962890625 val loss 62194.4375\n",
      "Epoch 47270 train loss 15052.9482421875 val loss 62207.6875\n",
      "Epoch 47280 train loss 15055.7509765625 val loss 62195.61328125\n",
      "Epoch 47290 train loss 15064.98046875 val loss 62172.71484375\n",
      "Epoch 47300 train loss 15050.673828125 val loss 62202.39453125\n",
      "Epoch 47310 train loss 15107.4970703125 val loss 62259.640625\n",
      "Epoch 47320 train loss 15081.8203125 val loss 62193.33984375\n",
      "Epoch 47330 train loss 15047.0595703125 val loss 62216.73046875\n",
      "Epoch 47340 train loss 15082.0400390625 val loss 62247.21484375\n",
      "Epoch 47350 train loss 15049.4443359375 val loss 62199.98046875\n",
      "Epoch 47360 train loss 15051.4453125 val loss 62233.234375\n",
      "Epoch 47370 train loss 15044.9423828125 val loss 62245.46484375\n",
      "Epoch 47380 train loss 15074.2470703125 val loss 62263.609375\n",
      "Epoch 47390 train loss 15045.03125 val loss 62255.90234375\n",
      "Epoch 47400 train loss 15072.91796875 val loss 62259.2421875\n",
      "Epoch 47410 train loss 15046.20703125 val loss 62270.5390625\n",
      "Epoch 47420 train loss 15082.328125 val loss 62273.640625\n",
      "Epoch 47430 train loss 15047.837890625 val loss 62323.796875\n",
      "Epoch 47440 train loss 15047.0166015625 val loss 62304.58203125\n",
      "Epoch 47450 train loss 15086.2138671875 val loss 62301.546875\n",
      "Epoch 47460 train loss 15040.603515625 val loss 62342.17578125\n",
      "Epoch 47470 train loss 15059.1513671875 val loss 62347.20703125\n",
      "Epoch 47480 train loss 15076.7802734375 val loss 62331.23046875\n",
      "Epoch 47490 train loss 15040.4541015625 val loss 62365.69140625\n",
      "Epoch 47500 train loss 15056.8134765625 val loss 62407.046875\n",
      "Epoch 47510 train loss 15046.2900390625 val loss 62385.83203125\n",
      "Epoch 47520 train loss 15050.47265625 val loss 62428.80859375\n",
      "Epoch 47530 train loss 15039.83984375 val loss 62388.546875\n",
      "Epoch 47540 train loss 15040.986328125 val loss 62426.17578125\n",
      "Epoch 47550 train loss 15051.2392578125 val loss 62419.453125\n",
      "Epoch 47560 train loss 15067.580078125 val loss 62402.12109375\n",
      "Epoch 47570 train loss 15034.796875 val loss 62432.34375\n",
      "Epoch 47580 train loss 15077.3671875 val loss 62440.4765625\n",
      "Epoch 47590 train loss 15081.783203125 val loss 62424.796875\n",
      "Epoch 47600 train loss 15052.0771484375 val loss 62446.16796875\n",
      "Epoch 47610 train loss 15032.6171875 val loss 62416.390625\n",
      "Epoch 47620 train loss 15040.1025390625 val loss 62427.52734375\n",
      "Epoch 47630 train loss 15045.1484375 val loss 62430.64453125\n",
      "Epoch 47640 train loss 15030.9267578125 val loss 62448.66015625\n",
      "Epoch 47650 train loss 15049.6640625 val loss 62449.68359375\n",
      "Epoch 47660 train loss 15044.8798828125 val loss 62444.40234375\n",
      "Epoch 47670 train loss 15032.0908203125 val loss 62468.7890625\n",
      "Epoch 47680 train loss 15067.484375 val loss 62481.453125\n",
      "Epoch 47690 train loss 15032.1845703125 val loss 62489.0859375\n",
      "Epoch 47700 train loss 15034.9892578125 val loss 62491.11328125\n",
      "Epoch 47710 train loss 15032.9384765625 val loss 62511.5234375\n",
      "Epoch 47720 train loss 15044.3740234375 val loss 62518.4765625\n",
      "Epoch 47730 train loss 15049.806640625 val loss 62482.0703125\n",
      "Epoch 47740 train loss 15038.6748046875 val loss 62514.2578125\n",
      "Epoch 47750 train loss 15035.8671875 val loss 62513.81640625\n",
      "Epoch 47760 train loss 15047.34375 val loss 62522.1328125\n",
      "Epoch 47770 train loss 15024.177734375 val loss 62560.9375\n",
      "Epoch 47780 train loss 15068.8720703125 val loss 62592.8359375\n",
      "Epoch 47790 train loss 15027.927734375 val loss 62554.1015625\n",
      "Epoch 47800 train loss 15026.66796875 val loss 62557.84375\n",
      "Epoch 47810 train loss 15062.720703125 val loss 62613.859375\n",
      "Epoch 47820 train loss 15030.4453125 val loss 62613.66015625\n",
      "Epoch 47830 train loss 15028.3505859375 val loss 62622.17578125\n",
      "Epoch 47840 train loss 15060.802734375 val loss 62599.96875\n",
      "Epoch 47850 train loss 15030.1171875 val loss 62558.40234375\n",
      "Epoch 47860 train loss 15033.8291015625 val loss 62607.9375\n",
      "Epoch 47870 train loss 15024.0947265625 val loss 62591.25\n",
      "Epoch 47880 train loss 15044.70703125 val loss 62576.81640625\n",
      "Epoch 47890 train loss 15022.845703125 val loss 62614.42578125\n",
      "Epoch 47900 train loss 15035.80078125 val loss 62624.765625\n",
      "Epoch 47910 train loss 15017.623046875 val loss 62636.62109375\n",
      "Epoch 47920 train loss 15066.3564453125 val loss 62628.30078125\n",
      "Epoch 47930 train loss 15047.341796875 val loss 62683.9453125\n",
      "Epoch 47940 train loss 15016.095703125 val loss 62659.89453125\n",
      "Epoch 47950 train loss 15044.8349609375 val loss 62641.984375\n",
      "Epoch 47960 train loss 15049.3125 val loss 62723.55078125\n",
      "Epoch 47970 train loss 15015.400390625 val loss 62691.58203125\n",
      "Epoch 47980 train loss 15026.1572265625 val loss 62690.44140625\n",
      "Epoch 47990 train loss 15014.84765625 val loss 62705.45703125\n",
      "Epoch 48000 train loss 15038.44140625 val loss 62768.9609375\n",
      "Epoch 48010 train loss 15023.654296875 val loss 62725.22265625\n",
      "Epoch 48020 train loss 15015.41015625 val loss 62742.984375\n",
      "Epoch 48030 train loss 15042.3740234375 val loss 62764.1328125\n",
      "Epoch 48040 train loss 15012.08984375 val loss 62750.28515625\n",
      "Epoch 48050 train loss 15026.1845703125 val loss 62784.75390625\n",
      "Epoch 48060 train loss 15014.1455078125 val loss 62777.51171875\n",
      "Epoch 48070 train loss 15045.6884765625 val loss 62773.19921875\n",
      "Epoch 48080 train loss 15010.919921875 val loss 62808.46484375\n",
      "Epoch 48090 train loss 15055.755859375 val loss 62748.359375\n",
      "Epoch 48100 train loss 15010.267578125 val loss 62807.15234375\n",
      "Epoch 48110 train loss 15021.7236328125 val loss 62793.953125\n",
      "Epoch 48120 train loss 15023.947265625 val loss 62805.53125\n",
      "Epoch 48130 train loss 15008.880859375 val loss 62783.8046875\n",
      "Epoch 48140 train loss 15012.18359375 val loss 62791.26953125\n",
      "Epoch 48150 train loss 15050.822265625 val loss 62820.9375\n",
      "Epoch 48160 train loss 15014.8876953125 val loss 62823.359375\n",
      "Epoch 48170 train loss 15005.98046875 val loss 62840.4921875\n",
      "Epoch 48180 train loss 15035.84765625 val loss 62846.203125\n",
      "Epoch 48190 train loss 15016.9873046875 val loss 62853.3984375\n",
      "Epoch 48200 train loss 15007.318359375 val loss 62878.79296875\n",
      "Epoch 48210 train loss 15026.7119140625 val loss 62869.59765625\n",
      "Epoch 48220 train loss 15005.771484375 val loss 62877.66015625\n",
      "Epoch 48230 train loss 15010.9677734375 val loss 62930.54296875\n",
      "Epoch 48240 train loss 15009.46484375 val loss 62898.8984375\n",
      "Epoch 48250 train loss 15012.072265625 val loss 62895.83203125\n",
      "Epoch 48260 train loss 15007.2470703125 val loss 62935.6015625\n",
      "Epoch 48270 train loss 15035.1982421875 val loss 62942.078125\n",
      "Epoch 48280 train loss 14999.84765625 val loss 62956.11328125\n",
      "Epoch 48290 train loss 15023.4541015625 val loss 62993.703125\n",
      "Epoch 48300 train loss 14999.701171875 val loss 62979.33984375\n",
      "Epoch 48310 train loss 15045.7490234375 val loss 62928.55078125\n",
      "Epoch 48320 train loss 15001.53515625 val loss 63023.71484375\n",
      "Epoch 48330 train loss 15001.0576171875 val loss 62993.9609375\n",
      "Epoch 48340 train loss 15026.7216796875 val loss 62978.86328125\n",
      "Epoch 48350 train loss 14998.6845703125 val loss 63025.953125\n",
      "Epoch 48360 train loss 15019.525390625 val loss 63046.95703125\n",
      "Epoch 48370 train loss 14997.1376953125 val loss 63049.5\n",
      "Epoch 48380 train loss 15030.357421875 val loss 63023.88671875\n",
      "Epoch 48390 train loss 15018.708984375 val loss 62991.82421875\n",
      "Epoch 48400 train loss 14997.5087890625 val loss 63032.95703125\n",
      "Epoch 48410 train loss 14994.2509765625 val loss 63030.03515625\n",
      "Epoch 48420 train loss 15025.373046875 val loss 63029.296875\n",
      "Epoch 48430 train loss 15007.048828125 val loss 63096.1875\n",
      "Epoch 48440 train loss 15003.50390625 val loss 63080.34765625\n",
      "Epoch 48450 train loss 15008.849609375 val loss 63096.2109375\n",
      "Epoch 48460 train loss 14993.9560546875 val loss 63082.15234375\n",
      "Epoch 48470 train loss 15011.791015625 val loss 63082.5234375\n",
      "Epoch 48480 train loss 15030.6572265625 val loss 63096.79296875\n",
      "Epoch 48490 train loss 15000.5263671875 val loss 63077.92578125\n",
      "Epoch 48500 train loss 14995.603515625 val loss 63108.953125\n",
      "Epoch 48510 train loss 14987.5654296875 val loss 63129.984375\n",
      "Epoch 48520 train loss 15005.431640625 val loss 63171.91796875\n",
      "Epoch 48530 train loss 14996.50390625 val loss 63166.56640625\n",
      "Epoch 48540 train loss 15011.5478515625 val loss 63194.05078125\n",
      "Epoch 48550 train loss 14986.3056640625 val loss 63181.85546875\n",
      "Epoch 48560 train loss 15012.1953125 val loss 63191.6171875\n",
      "Epoch 48570 train loss 14995.4970703125 val loss 63246.24609375\n",
      "Epoch 48580 train loss 14988.322265625 val loss 63218.49609375\n",
      "Epoch 48590 train loss 15014.5546875 val loss 63196.73046875\n",
      "Epoch 48600 train loss 14992.5107421875 val loss 63235.640625\n",
      "Epoch 48610 train loss 14987.6533203125 val loss 63238.03125\n",
      "Epoch 48620 train loss 14997.708984375 val loss 63220.90625\n",
      "Epoch 48630 train loss 14999.8310546875 val loss 63215.19140625\n",
      "Epoch 48640 train loss 14991.5615234375 val loss 63228.5625\n",
      "Epoch 48650 train loss 14982.4912109375 val loss 63282.71484375\n",
      "Epoch 48660 train loss 15022.962890625 val loss 63256.11328125\n",
      "Epoch 48670 train loss 14981.109375 val loss 63230.14453125\n",
      "Epoch 48680 train loss 15012.669921875 val loss 63255.42578125\n",
      "Epoch 48690 train loss 14991.0244140625 val loss 63291.2109375\n",
      "Epoch 48700 train loss 14985.212890625 val loss 63278.58984375\n",
      "Epoch 48710 train loss 14996.1513671875 val loss 63277.33203125\n",
      "Epoch 48720 train loss 14980.923828125 val loss 63296.08984375\n",
      "Epoch 48730 train loss 15026.6015625 val loss 63321.89453125\n",
      "Epoch 48740 train loss 14981.3271484375 val loss 63277.61328125\n",
      "Epoch 48750 train loss 14982.25 val loss 63314.25\n",
      "Epoch 48760 train loss 14994.2685546875 val loss 63375.26171875\n",
      "Epoch 48770 train loss 14979.96484375 val loss 63362.734375\n",
      "Epoch 48780 train loss 14978.431640625 val loss 63342.53515625\n",
      "Epoch 48790 train loss 14997.892578125 val loss 63353.23046875\n",
      "Epoch 48800 train loss 14988.220703125 val loss 63400.66015625\n",
      "Epoch 48810 train loss 14987.3916015625 val loss 63393.52734375\n",
      "Epoch 48820 train loss 14982.3603515625 val loss 63367.203125\n",
      "Epoch 48830 train loss 14993.20703125 val loss 63351.6484375\n",
      "Epoch 48840 train loss 14973.4658203125 val loss 63359.0\n",
      "Epoch 48850 train loss 14981.6728515625 val loss 63382.0078125\n",
      "Epoch 48860 train loss 14977.2890625 val loss 63367.0625\n",
      "Epoch 48870 train loss 15025.56640625 val loss 63309.5546875\n",
      "Epoch 48880 train loss 14980.78515625 val loss 63371.95703125\n",
      "Epoch 48890 train loss 14969.7841796875 val loss 63376.19140625\n",
      "Epoch 48900 train loss 15019.25390625 val loss 63346.7421875\n",
      "Epoch 48910 train loss 14985.50390625 val loss 63391.93359375\n",
      "Epoch 48920 train loss 14968.35546875 val loss 63381.03515625\n",
      "Epoch 48930 train loss 14966.0439453125 val loss 63375.03515625\n",
      "Epoch 48940 train loss 14989.2509765625 val loss 63376.20703125\n",
      "Epoch 48950 train loss 14966.8984375 val loss 63432.1796875\n",
      "Epoch 48960 train loss 14970.533203125 val loss 63410.12109375\n",
      "Epoch 48970 train loss 14976.5576171875 val loss 63404.6484375\n",
      "Epoch 48980 train loss 14971.1806640625 val loss 63434.9921875\n",
      "Epoch 48990 train loss 14987.7568359375 val loss 63447.66796875\n",
      "Epoch 49000 train loss 14965.1806640625 val loss 63417.53125\n",
      "Epoch 49010 train loss 14959.869140625 val loss 63440.51953125\n",
      "Epoch 49020 train loss 14995.8427734375 val loss 63484.703125\n",
      "Epoch 49030 train loss 14982.21484375 val loss 63485.9296875\n",
      "Epoch 49040 train loss 14960.0107421875 val loss 63478.88671875\n",
      "Epoch 49050 train loss 14968.34765625 val loss 63479.1640625\n",
      "Epoch 49060 train loss 14958.64453125 val loss 63520.88671875\n",
      "Epoch 49070 train loss 14976.09765625 val loss 63505.66015625\n",
      "Epoch 49080 train loss 14991.892578125 val loss 63477.1796875\n",
      "Epoch 49090 train loss 14961.3837890625 val loss 63507.39453125\n",
      "Epoch 49100 train loss 14963.806640625 val loss 63516.12109375\n",
      "Epoch 49110 train loss 14983.8056640625 val loss 63478.86328125\n",
      "Epoch 49120 train loss 14952.95703125 val loss 63474.37890625\n",
      "Epoch 49130 train loss 14987.3515625 val loss 63527.35546875\n",
      "Epoch 49140 train loss 14953.3271484375 val loss 63533.57421875\n",
      "Epoch 49150 train loss 14994.8759765625 val loss 63531.01171875\n",
      "Epoch 49160 train loss 14974.369140625 val loss 63517.59375\n",
      "Epoch 49170 train loss 14956.8173828125 val loss 63552.98046875\n",
      "Epoch 49180 train loss 14950.259765625 val loss 63558.6484375\n",
      "Epoch 49190 train loss 15001.478515625 val loss 63604.0234375\n",
      "Epoch 49200 train loss 14960.3017578125 val loss 63603.65625\n",
      "Epoch 49210 train loss 14950.6044921875 val loss 63584.9921875\n",
      "Epoch 49220 train loss 14978.7744140625 val loss 63611.9609375\n",
      "Epoch 49230 train loss 14969.8095703125 val loss 63597.48046875\n",
      "Epoch 49240 train loss 14949.3037109375 val loss 63608.0078125\n",
      "Epoch 49250 train loss 14973.4052734375 val loss 63662.015625\n",
      "Epoch 49260 train loss 14946.892578125 val loss 63674.0546875\n",
      "Epoch 49270 train loss 14967.552734375 val loss 63671.9375\n",
      "Epoch 49280 train loss 14952.2158203125 val loss 63692.953125\n",
      "Epoch 49290 train loss 14982.96875 val loss 63630.17578125\n",
      "Epoch 49300 train loss 14945.2705078125 val loss 63681.953125\n",
      "Epoch 49310 train loss 14994.6884765625 val loss 63690.93359375\n",
      "Epoch 49320 train loss 14946.408203125 val loss 63736.01171875\n",
      "Epoch 49330 train loss 14961.8896484375 val loss 63691.65625\n",
      "Epoch 49340 train loss 14948.2626953125 val loss 63678.48046875\n",
      "Epoch 49350 train loss 14979.2890625 val loss 63703.953125\n",
      "Epoch 49360 train loss 14988.626953125 val loss 63660.76171875\n",
      "Epoch 49370 train loss 14940.37890625 val loss 63702.984375\n",
      "Epoch 49380 train loss 14957.384765625 val loss 63718.515625\n",
      "Epoch 49390 train loss 14953.708984375 val loss 63725.171875\n",
      "Epoch 49400 train loss 14939.169921875 val loss 63748.453125\n",
      "Epoch 49410 train loss 14972.59765625 val loss 63746.98046875\n",
      "Epoch 49420 train loss 14942.2587890625 val loss 63761.93359375\n",
      "Epoch 49430 train loss 14956.193359375 val loss 63761.4921875\n",
      "Epoch 49440 train loss 14953.9638671875 val loss 63801.0625\n",
      "Epoch 49450 train loss 14974.310546875 val loss 63782.5\n",
      "Epoch 49460 train loss 14936.8720703125 val loss 63804.90625\n",
      "Epoch 49470 train loss 14957.0712890625 val loss 63797.40625\n",
      "Epoch 49480 train loss 14934.3466796875 val loss 63810.76953125\n",
      "Epoch 49490 train loss 14976.5869140625 val loss 63779.0546875\n",
      "Epoch 49500 train loss 14935.7099609375 val loss 63818.37109375\n",
      "Epoch 49510 train loss 14949.5615234375 val loss 63868.90234375\n",
      "Epoch 49520 train loss 14942.552734375 val loss 63834.75\n",
      "Epoch 49530 train loss 14957.5068359375 val loss 63883.83203125\n",
      "Epoch 49540 train loss 14935.5458984375 val loss 63862.88671875\n",
      "Epoch 49550 train loss 14961.3310546875 val loss 63852.6796875\n",
      "Epoch 49560 train loss 14951.4345703125 val loss 63885.4453125\n",
      "Epoch 49570 train loss 14936.9736328125 val loss 63857.109375\n",
      "Epoch 49580 train loss 14947.546875 val loss 63875.66015625\n",
      "Epoch 49590 train loss 14945.759765625 val loss 63883.9140625\n",
      "Epoch 49600 train loss 14940.9384765625 val loss 63918.8515625\n",
      "Epoch 49610 train loss 14949.990234375 val loss 63878.29296875\n",
      "Epoch 49620 train loss 14942.318359375 val loss 63873.69921875\n",
      "Epoch 49630 train loss 14930.2275390625 val loss 63891.6875\n",
      "Epoch 49640 train loss 14979.2158203125 val loss 63894.5546875\n",
      "Epoch 49650 train loss 14965.9814453125 val loss 63959.828125\n",
      "Epoch 49660 train loss 14943.6474609375 val loss 63943.65625\n",
      "Epoch 49670 train loss 14929.271484375 val loss 63965.9765625\n",
      "Epoch 49680 train loss 14941.3837890625 val loss 63967.9921875\n",
      "Epoch 49690 train loss 14927.1748046875 val loss 63961.796875\n",
      "Epoch 49700 train loss 14925.2587890625 val loss 63962.375\n",
      "Epoch 49710 train loss 14962.8671875 val loss 64007.16796875\n",
      "Epoch 49720 train loss 14945.0712890625 val loss 63968.2890625\n",
      "Epoch 49730 train loss 14928.1708984375 val loss 64020.03125\n",
      "Epoch 49740 train loss 14937.587890625 val loss 64062.26953125\n",
      "Epoch 49750 train loss 14929.6005859375 val loss 64043.91796875\n",
      "Epoch 49760 train loss 14940.0576171875 val loss 64039.5625\n",
      "Epoch 49770 train loss 14920.8427734375 val loss 64088.69140625\n",
      "Epoch 49780 train loss 14942.9951171875 val loss 64071.3359375\n",
      "Epoch 49790 train loss 14928.994140625 val loss 64074.18359375\n",
      "Epoch 49800 train loss 14925.2685546875 val loss 64077.171875\n",
      "Epoch 49810 train loss 14951.2041015625 val loss 64105.515625\n",
      "Epoch 49820 train loss 14918.3623046875 val loss 64104.671875\n",
      "Epoch 49830 train loss 14943.80078125 val loss 64106.48828125\n",
      "Epoch 49840 train loss 14919.7177734375 val loss 64075.2890625\n",
      "Epoch 49850 train loss 14930.5693359375 val loss 64070.8984375\n",
      "Epoch 49860 train loss 14916.259765625 val loss 64099.3984375\n",
      "Epoch 49870 train loss 14940.38671875 val loss 64135.15234375\n",
      "Epoch 49880 train loss 14927.7265625 val loss 64129.07421875\n",
      "Epoch 49890 train loss 14928.9052734375 val loss 64101.84765625\n",
      "Epoch 49900 train loss 14916.9072265625 val loss 64137.1015625\n",
      "Epoch 49910 train loss 14943.349609375 val loss 64141.421875\n",
      "Epoch 49920 train loss 14913.9921875 val loss 64173.4140625\n",
      "Epoch 49930 train loss 14914.5849609375 val loss 64164.88671875\n",
      "Epoch 49940 train loss 14955.958984375 val loss 64224.2734375\n",
      "Epoch 49950 train loss 14972.392578125 val loss 64166.34375\n",
      "Epoch 49960 train loss 14944.50390625 val loss 64242.84765625\n",
      "Epoch 49970 train loss 14914.904296875 val loss 64209.91796875\n",
      "Epoch 49980 train loss 14931.541015625 val loss 64196.5546875\n",
      "Epoch 49990 train loss 14920.140625 val loss 64227.21484375\n"
     ]
    }
   ],
   "source": [
    "epochs = 50000\n",
    "final_losses = []\n",
    "final_val_losses = []\n",
    "for i in range(epochs):\n",
    "    y_pred = model(train_categorical, train_cont)\n",
    "    loss = torch.sqrt(loss_function(y_pred, y_train))\n",
    "    final_losses.append(loss.item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_test_pred = model(test_categorical, test_cont)\n",
    "        val_loss = torch.sqrt(loss_function(y_test_pred, y_test))\n",
    "        final_val_losses.append(val_loss.item())\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Epoch {i} train loss {loss.item()} val loss {val_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plot final_loss and final_val_loss\n",
    "\n",
    "plt.plot(range(epochs), final_losses, label='train loss')\n",
    "plt.plot(range(epochs), final_val_losses, label='val loss')\n",
    "plt.ylabel('RMSE Loss')\n",
    "plt.xlabel('epoch');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>20</td>\n",
       "      <td>RH</td>\n",
       "      <td>80.0</td>\n",
       "      <td>11622</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>1961</td>\n",
       "      <td>896</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>81.0</td>\n",
       "      <td>14267</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>1958</td>\n",
       "      <td>1329</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>74.0</td>\n",
       "      <td>13830</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>1997</td>\n",
       "      <td>928</td>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>78.0</td>\n",
       "      <td>9978</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>1998</td>\n",
       "      <td>926</td>\n",
       "      <td>678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>120</td>\n",
       "      <td>RL</td>\n",
       "      <td>43.0</td>\n",
       "      <td>5005</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>1992</td>\n",
       "      <td>1280</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2915</td>\n",
       "      <td>160</td>\n",
       "      <td>RM</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1936</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>1970</td>\n",
       "      <td>546</td>\n",
       "      <td>546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>2916</td>\n",
       "      <td>160</td>\n",
       "      <td>RM</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1894</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>1970</td>\n",
       "      <td>546</td>\n",
       "      <td>546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2917</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>160.0</td>\n",
       "      <td>20000</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>1960</td>\n",
       "      <td>1224</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2918</td>\n",
       "      <td>85</td>\n",
       "      <td>RL</td>\n",
       "      <td>62.0</td>\n",
       "      <td>10441</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>1992</td>\n",
       "      <td>970</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2919</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>74.0</td>\n",
       "      <td>9627</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>1993</td>\n",
       "      <td>996</td>\n",
       "      <td>1004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1229 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id  MSSubClass MSZoning  LotFrontage  LotArea Street LotShape  \\\n",
       "0     1461          20       RH         80.0    11622   Pave      Reg   \n",
       "1     1462          20       RL         81.0    14267   Pave      IR1   \n",
       "2     1463          60       RL         74.0    13830   Pave      IR1   \n",
       "3     1464          60       RL         78.0     9978   Pave      IR1   \n",
       "4     1465         120       RL         43.0     5005   Pave      IR1   \n",
       "...    ...         ...      ...          ...      ...    ...      ...   \n",
       "1454  2915         160       RM         21.0     1936   Pave      Reg   \n",
       "1455  2916         160       RM         21.0     1894   Pave      Reg   \n",
       "1456  2917          20       RL        160.0    20000   Pave      Reg   \n",
       "1457  2918          85       RL         62.0    10441   Pave      Reg   \n",
       "1458  2919          60       RL         74.0     9627   Pave      Reg   \n",
       "\n",
       "      YearBuilt  1stFlrSF  2ndFlrSF  \n",
       "0          1961       896         0  \n",
       "1          1958      1329         0  \n",
       "2          1997       928       701  \n",
       "3          1998       926       678  \n",
       "4          1992      1280         0  \n",
       "...         ...       ...       ...  \n",
       "1454       1970       546       546  \n",
       "1455       1970       546       546  \n",
       "1456       1960      1224         0  \n",
       "1457       1992       970         0  \n",
       "1458       1993       996      1004  \n",
       "\n",
       "[1229 rows x 10 columns]"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('house_price_data/test.csv', usecols=[\"Id\", \"MSSubClass\", \"MSZoning\", \"LotFrontage\", \"LotArea\",\n",
    "                                         \"Street\", \"YearBuilt\", \"LotShape\", \"1stFlrSF\", \"2ndFlrSF\"]).dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Total Years'] = datetime.datetime.now().year - df['YearBuilt']\n",
    "df = df.drop(['YearBuilt'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>Total Years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>80.0</td>\n",
       "      <td>11622</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>896</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>81.0</td>\n",
       "      <td>14267</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1329</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>74.0</td>\n",
       "      <td>13830</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>928</td>\n",
       "      <td>701</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>78.0</td>\n",
       "      <td>9978</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>926</td>\n",
       "      <td>678</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>43.0</td>\n",
       "      <td>5005</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1280</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2915</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1936</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>546</td>\n",
       "      <td>546</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>2916</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1894</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>546</td>\n",
       "      <td>546</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2917</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>160.0</td>\n",
       "      <td>20000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1224</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2918</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>62.0</td>\n",
       "      <td>10441</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>970</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2919</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>74.0</td>\n",
       "      <td>9627</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>996</td>\n",
       "      <td>1004</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1229 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id  MSSubClass  MSZoning  LotFrontage  LotArea  Street  LotShape  \\\n",
       "0     1461           0         2         80.0    11622       1         3   \n",
       "1     1462           0         3         81.0    14267       1         0   \n",
       "2     1463           5         3         74.0    13830       1         0   \n",
       "3     1464           5         3         78.0     9978       1         0   \n",
       "4     1465          11         3         43.0     5005       1         0   \n",
       "...    ...         ...       ...          ...      ...     ...       ...   \n",
       "1454  2915          12         4         21.0     1936       1         3   \n",
       "1455  2916          12         4         21.0     1894       1         3   \n",
       "1456  2917           0         3        160.0    20000       1         3   \n",
       "1457  2918           9         3         62.0    10441       1         3   \n",
       "1458  2919           5         3         74.0     9627       1         3   \n",
       "\n",
       "      1stFlrSF  2ndFlrSF  Total Years  \n",
       "0          896         0           63  \n",
       "1         1329         0           66  \n",
       "2          928       701           27  \n",
       "3          926       678           26  \n",
       "4         1280         0           32  \n",
       "...        ...       ...          ...  \n",
       "1454       546       546           54  \n",
       "1455       546       546           54  \n",
       "1456      1224         0           64  \n",
       "1457       970         0           32  \n",
       "1458       996      1004           31  \n",
       "\n",
       "[1229 rows x 10 columns]"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for f in cat_features:\n",
    "    lbl_encoders[f] = LabelEncoder()\n",
    "    df[f] = lbl_encoders[f].fit_transform(df[f])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1229, 4])"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_cate_tensor = torch.tensor(np.stack([df[f] for f in cat_features],1,), dtype=torch.int)\n",
    "pred_cate_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1229, 5])"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_cont_tensor = torch.tensor(np.stack([df[f] for f in cont_features],1), dtype=torch.float)\n",
    "pred_cont_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1229, 1])"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_price = model(pred_cate_tensor, pred_cont_tensor)\n",
    "pred_price.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1229,), torch.Size([1229, 1]))"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = df['Id']\n",
    "ids.values.shape,pred_price.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame({\"Id\":ids.values, \"SalePrice\": pred_price.squeeze().detach().numpy()})\n",
    "result.to_csv('house_price_predictions.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedForwardModel(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Embedding(15, 8)\n",
       "    (1): Embedding(5, 3)\n",
       "    (2): Embedding(2, 1)\n",
       "    (3): Embedding(4, 2)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0.4, inplace=False)\n",
       "  (bn_cont): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=19, out_features=100, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.4, inplace=False)\n",
       "    (4): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.4, inplace=False)\n",
       "    (8): Linear(in_features=50, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
